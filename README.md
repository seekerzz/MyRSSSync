# 五里墩茶社
---
| Title | Date | Summary |
| --- | --- | --- |
# 小工蚁创始人
---
| Title | Date | Summary |
| --- | --- | --- |
# AIGCLINK
---
| Title | Date | Summary |
| --- | --- | --- |
# 技术爬爬虾
---
| Title | Date | Summary |
| --- | --- | --- |
# 秋芝2046
---
| Title | Date | Summary |
| --- | --- | --- |
# GitHub All Languages Daily Trending
---
| Title | Summary |
| --- | --- |
| [openai/codex](https://github.com/openai/codex) | 轻量级代码代理，运行在终端上，可通过npm或Homebrew全局安装，并提供GitHub最新发布中的特定平台二进制文件下载。支持通过ChatGPT账号与之集成使用，同时提供官方文档、贡献指南和许可证信息。 |
| [VectifyAI/PageIndex](https://github.com/VectifyAI/PageIndex) | 根据您的要求，我为您翻译了此页面的内容。这是一个关于使用 PageIndex 进行文本处理、索引和检索的项目介绍。<br/><br/>**主要功能点概览：**<br/><br/>1. **文档索引与检索（Document Indexing & Retrieval）**<br/>   - 提供了基于语义和结构的方式对文档进行建模，以创建高效索引。<br/>   - 支持多种语言，并且能够处理复杂、多模态的数据，包括文本、图像等。<br/><br/>2. **性能优化**<br/>   - 实现快速的全文检索速度，减少查询响应时间。<br/>   - 支持在线学习能力，根据新的文档数据自动更新索引结构。<br/><br/>3. **案例研究：金融领域应用**<br/>   - Mafin 2.5 是一个基于推理的关系增强型系统，在财务文档分析中使用 PageIndex 实现了高精度处理。<br/><br/>4. **资源库与支持**<br/>   - 提供一系列教程、实践和代码示例，帮助用户快速上手。<br/>   - 包括与主流平台（如 Discord, Twitter 和 LinkedIn）的集成选项。<br/><br/>5. **社区互动与反馈**<br/>   - 通过各种渠道（如官方社交媒体账号、Discord 频道等）与开发者和潜在用户提供交流机会。<br/><br/>**核心优势：**<br/><br/>- **高效索引构建**，可以快速对大规模数据进行索引。<br/>- **精确检索能力**，确保高准确度的文档搜索结果。<br/>- **适应多种语言与多模态输入**，提供广泛的适用性。<br/>- **持续改进与优化**，通过在线学习提升系统性能。<br/><br/>如果您需要进一步的信息或具体的代码示例，请告知。 |
| [OpenBMB/UltraRAG](https://github.com/OpenBMB/UltraRAG) | UltraRAG项目是一个全面的文档处理工具，包括文档理解、知识检索和生成等关键功能。它提供了从文本理解到知识集成、多模态信息检索和自动生成多种类型文档的一站式解决方案。以下是其主要特性：<br/><br/>1. **多语言支持**：UltraRAG支持多种语言，使得不同背景的用户能够适应不同的使用环境。<br/><br/>2. **API接口**：<br/>   - **Retrieval API**: 基于语义搜索或关键词匹配来检索文档中相关的信息。<br/>   - **Generation API**: 生成与输入相关联的文档、报告等。<br/>   - **Translation API**: 实现跨语言的信息翻译和理解。<br/><br/>3. **可扩展性**：用户可以将UltraRAG集成到自定义系统或应用中，通过API调用实现个性化的功能和服务。<br/><br/>4. **社区与支持**：<br/>   - GitHub上的问题跟踪和代码提交用于技术支持与项目改进。<br/>   - 社交媒体平台如微信、飞书及Discord提供了互动交流的渠道，用户可直接向开发团队咨询或分享反馈。<br/><br/>5. **贡献机制**：鼓励开发者通过fork仓库、提出issue（问题）和创建pull requests（拉取请求）等方式参与项目的改进与扩展。<br/><br/>6. **多场景应用**：<br/>   - **Deep Research Pipeline**: 结合深度研究模型，自动完成多次检索集成生成大量文本报告。<br/>   - **Web UI**: 提供直观的用户界面，用于实时查看检索结果、参数调整和生成文档。<br/><br/>UltraRAG致力于通过高效、全面的功能为用户提供强大的文档处理能力，无论是在学术研究、商业分析还是日常办公中都能发挥重要作用。对于使用过程中遇到的问题或有改进意见，欢迎社区成员参与讨论与贡献。 |
| [microsoft/VibeVoice](https://github.com/microsoft/VibeVoice) | VibeVoice是一个由微软开发的文本转语音（TTS）模型系列，旨在生成高质量、流畅和自然的声音。VibeVoice分为三类主要模型：<br/><br/>1. VibeVoice2: 一个大型的参数量为760亿的语言模型，它在多项语言理解和生成任务上达到了SOTA水平。<br/>2. VibeVoice Streaming: 提供轻量级、实时的文本转语音功能，支持流式文本输入和长时间段的语音生成。该模型大小约为0.5B，并具备快速响应能力（大约300毫秒的第一可听见延迟）。<br/>3. VibeVoice 1.5b：一个介于VibeVoice2和VibeVoice Streaming之间的中等参数量版本，专为部署和开发目的设计。<br/><br/>VibeVoice在多种领域展现出了其优势，包括语言理解、自然语言生成、文本转语音以及提供高质量的音频体验。然而，它与任何基于AI的模型一样，存在一定的风险和限制：<br/><br/>- **偏见和偏差**：模型可能会继承其基础架构的偏见或错误。<br/>- **Deepfakes和误导信息**：高保真的合成声音可能被滥用以创建虚假音频内容，用于欺诈、假冒或者传播误导性信息。使用此类生成的内容时需谨慎，并确保准确性和合法性。<br/><br/>VibeVoice旨在为研究和开发领域提供工具，建议在商业或实际应用中进行充分的测试和调整后再部署。用户应按照相关法律和规定在适当的情况下使用这些模型。 |
| [qarmin/czkawka](https://github.com/qarmin/czkawka) | 这篇文章详细介绍了Czkawka这一用于查找重复文件的软件项目，包括它的功能、使用方法、支持的平台和库、与第三方项目的集成、官方维护的项目以及许可证信息。以下是主要内容摘要：<br/><br/>1. **核心功能**：Czkawka提供了一个命令行工具（`czkawka-cli`）和图形界面版本（`czkawka-gtk`），用于检测并管理文件重复项。<br/><br/>2. **库集成**：它包括一个名为`czkawka_core`的Rust库，用于实现通用功能，可被其他项目复用。例如：<br/>   - Czkawka Tauri是一个基于Tauri框架的GUI前端。<br/>   - `page-dewarp`使用立方片模型对文档图像进行去畸变处理的库。<br/>   - PyCzkawka提供了Python绑定。<br/><br/>3. **官方支持和维护**：主要关注此仓库、预构建二进制文件、Crates.io上的项目以及Flathub中的应用。强调了通过捐赠来支持进一步开发的需求。<br/><br/>4. **许可证信息**：<br/>   - 代码和文档遵循MIT许可协议。<br/>   - 图像遵循CC BY 4.0许可。<br/>   - Czkawka GTK GUI 和 CLI 版本的许可为MIT，Krokiet使用GPL-3.0-only许可。<br/><br/>5. **感谢声明**：提及了对FSlint、翻译者、系统提供者、视频制作者和文章撰写者的贡献表示感谢，并强调官方支持仅限于指定仓库、二进制包、Crates.io 和 Flathub上的项目。不推荐使用非官方来源的包，需确保安全。<br/><br/>6. **资金捐赠**：鼓励用户通过赞助来支持项目的持续发展。<br/><br/>总结而言，Czkawka是一个功能强大的文件重复查找工具，通过其核心库被多个项目复用，并有来自社区的不同集成和扩展。该项目强调了许可证透明度与官方支持项目明确划分的特点，并提供了一个渠道供用户贡献反馈和支持开发工作。 |
| [Psiphon-Inc/conduit](https://github.com/Psiphon-Inc/conduit) | Conduit是一款运行于Psiphon隧道核心的移动应用，适用于Android、iOS和Mac（通过Catalyst）。此项目使用Git LFS管理大型文件，并提供翻译说明。更多信息访问官网或查看翻译指南。 |
| [remotion-dev/remotion](https://github.com/remotion-dev/remotion) | Remotion 是一个利用 React 创建视频的框架，旨在充分利用 web 技术（如 CSS、Canvas、SVG 和 WebGL），通过编程方式使用变量、函数和算法创建新效果，并借助 React 的可重用组件、强大组合、快速刷新和包生态系统。本文档提供了入门指南、文档、API 参考、许可证信息和贡献准则，展示了使用 Remotion 创作的视频作品，并介绍了如何开始使用该框架。 |
| [Blaizzy/mlx-audio](https://github.com/Blaizzy/mlx-audio) | MLX Audio是为Apple Silicon设备设计的高性能音频处理库，提供了文本到语音（TTS）、语音识别（STT）和语音合成等特性。以下是MLX Audio的关键点及中文概述：<br/><br/>1. **核心功能**：<br/>   - **文本转语音（TTS）**: 支持将文本转换为语音。<br/>   - **语音识别（STT）**: 能够识别并解析语音输入到文本形式。<br/>   - **语音合成**：用于生成流畅自然的语音输出。<br/><br/>2. **技术亮点**：<br/>   - **基于MLX框架**：利用Apple专有的MLX库，实现高效的音频处理和模型推理。<br/>   - **针对M1/M2等芯片优化**：特别针对Arm架构的苹果设备（如MacBook M1、iPhone 13系列）进行了性能调优。<br/><br/>3. **开发环境与依赖**：<br/>   - **语言支持**：提供Python接口，也考虑了Swift/iOS平台。<br/>   - **系统要求**：需要运行在具有Apple芯片的设备上，并安装了相应的软件库（如ffmpeg用于音频编码）。<br/><br/>4. **功能增强与改进**：<br/>   - **模型转换**：支持将现有模型量化和优化以减小大小、提高性能，包括4位、6位或8位量化选项。<br/>   - **版本更新**：持续的代码维护和错误修复，确保了用户能够获得最新的功能和稳定性。<br/><br/>5. **目标与用例**：<br/>   - **开发人员与工程师**：提供了一个用于音频处理项目的全面工具集。<br/>   - **苹果设备用户**：特别是Macbook M1/M2系列以及iPhone 13等产品的开发者和爱好者，能利用其进行创新应用开发或个人项目。<br/><br/>6. **社区与贡献**：<br/>   - 提供了详细的文档、教程和示例代码，促进了知识共享与学习。<br/>   - 鼓励用户报告问题并参与开源开发，形成了活跃的开发者社区。<br/><br/>MLX Audio通过提供全面的音频处理解决方案，为苹果生态系统内的各种应用提供了强大的基础，适用于从智能家居到教育娱乐等多个领域。它不仅是技术实现的一次飞跃，也是开发者创新的平台。 |
| [supermemoryai/supermemory](https://github.com/supermemoryai/supermemory) | Supermemory 是一个集成AI的智能记忆系统，旨在帮助用户存储、检索和利用信息。以下是对其关键功能和技术点的中文总结：<br/><br/>**核心功能**：<br/>1. **记忆管理与检索**：用户可以添加记忆、文本或文件至系统中，并通过聊天方式获取所需的信息。<br/>2. **AI工具集成**：支持连接如ChatGPT和Claude等人工智能工具，提供更丰富的交互体验。<br/>3. **浏览器扩展**：Chrome/Edge浏览器插件允许用户直接从网页保存内容，方便快捷地与社交媒体（如Twitter）互动并导入信息。<br/>4. **跨平台服务整合**：用户可链接Notion、Google Drive和OneDrive等云存储服务，实现数据的跨应用共享。<br/><br/>**技术亮点**：<br/>- 利用 AI 优化记忆检索效率和准确性；<br/>- 实现与第三方工具（如AI助手）的无缝连接，增强智能化体验。<br/>- 浏览器扩展增强了用户在网页上的交互性和便利性；<br/>- 跨平台服务集成提高了数据管理的灵活性。<br/><br/>**参与方式**：<br/>1. **代码贡献**：修复错误、增加新功能或优化界面设计。<br/>2. **性能提升**：通过改进算法和优化代码提高整体系统效率。<br/>3. **文档完善**：更新用户指南和帮助文档，增强用户体验。<br/><br/>**社区与支持**：<br/>- 通过邮件、Discord服务器和官方文档提供技术支持和服务咨询。<br/><br/>**贡献者指南**：<br/>查阅 GitHub 页面获取详细指导、开发设置、编码标准以及完整的贡献流程信息。<br/><br/>**持续更新与计划**：<br/>密切关注项目页面了解最新改进和未来规划，以及查看“好入手”或“需要帮助”的问题来参与贡献。<br/><br/>通过以上介绍，Supermemory 不仅提供了一个高效的记忆管理工具，还旨在为用户构建一个集成 AI 的智能生态系统。 |
# eess.AS updates on arXiv.org
---
| Title | Summary |
| --- | --- |
| [ES4R: Speech Encoding Based on Prepositive Affective Modeling for Empathetic Response Generation](https://arxiv.org/abs/2601.16225) | 贡献点:<br/><br/>1. **提出ES4R框架**：提出了一种名为ES4R（Empathetic Speech for Response Generation）的框架，用于基于语音的同理心回应生成。<br/><br/>2. **明确的情感语境建模**：创新地在演讲编码之前明确建模了结构化情感上下文，而不是依赖于解码器的隐式学习或显式情绪监督。<br/><br/>3. **双层注意力机制引入**：引入了一种两层注意力机制来捕获会话级的情绪状态和对话级的情感动态。这增强了对不同层次情感信息的理解与表达。<br/><br/>4. **跨模态关注融合**：通过语音指导的跨模态关注方式，将情感表示与文本语义整合在一起，生成具有同理心的回应。<br/><br/>5. **能量驱动的选择策略和风格融合**：使用能量驱动的选择策略和风格融合方法来实现基于语音的情感响应合成。确保了在自动生成的语音输出中表现出同理心特性。<br/><br/>6. **全面性能提升与跨模型泛化性**：ES4R框架在整个自动评估和人工评估中均显著超越强大的基线，并展现出对不同大型语言模型（LLM）基础的鲁棒性和适应性。 |
| [Zero-Shot Speech LLMs for Multi-Aspect Evaluation of L2 Speech: Challenges and Opportunities](https://arxiv.org/abs/2601.16230) | ### 贡献点:<br/><br/>1. **研究目标明确性**: 论文聚焦于评估学习者L2英语发音的准确性，强调了自动化评分在提供个性化反馈和公平评价个体进步中的重要性。<br/><br/>2. **模型选择与应用**: 使用Qwen2-Audio-7B-Instruct这一经过指令调优的语言模型，对5,000个Speechocean762发音样本进行了评估。这种方法利用大型语言模型（LLM）在语音评估领域展现了可能性。<br/><br/>3. **评估结果与分析**:<br/>   - 模型生成了符合评分标准的分数，涵盖了准确性、流畅度、语调和完整性四个方面，显示了与人类评级之间的高度一致性。<br/>   - 特别是对于高质量发音，模型表现尤为出色。<br/>   - 对于低质量发音，模型有倾向性预测较高的评分，并在错误检测精度上有所欠缺。<br/><br/>4. **潜在应用与未来方向**:<br/>   - 这项研究展示了语音LLM在规模化发音评估方面的强大潜力。<br/>   - 提出通过改进提示设计、校准和音素整合来提升模型性能的建议，以促进计算机辅助发音训练的发展。这表明了进一步优化和提升自动化评分系统的方法论。<br/><br/>### 总结：该论文探讨了一种使用指令调优的语言模型在评估L2英语发音方面的可能性与挑战，并提出了未来改进的方向，旨在通过技术手段提高语言学习中的自动评分效率和准确性。 |
| [Test-Time Adaptation for Speech Emotion Recognition](https://arxiv.org/abs/2601.16240) | ### 贡献点：<br/><br/>1. **识别领域转变的挑战**：该研究强调了语音情绪识别（SER）系统在实践中遭遇的主要挑战，包括说话者变化、行为与自然情绪的区别以及跨语料库变异性。<br/><br/>2. **适应性方法的调查**：文章探讨并对比了针对SER中特有的领域偏移进行测试时适应（TTA）的11种方法的有效性。这一研究填补了在视觉分类和语音识别领域的现有适配方法与情感表达领域特定挑战之间的空白。<br/><br/>3. **新型适配技术评估**：首次系统地评估并比较了面向SER任务的TAT方法，重点关注无监督目标数据下的模型调整策略。<br/><br/>4. **后传播自由TTA方法的优势**：研究发现，无需反向传播的TTA方法在SER领域具有显著优势，这可能归因于它们能够更灵活地适应情感表达的复杂性和多义性。<br/><br/>5. **失败的方法类型和原因分析**：文章识别并解释了熵最小化和伪标签法等方法为何表现不佳。其根本原因是这些方法假定存在一个明确且自信的真实标签，但在情绪表达领域中这一假设并不成立，情绪表达具有内在的模糊性和复杂性。<br/><br/>6. **方法适用性的限制与依赖性**：研究指出没有单一的TTA方法适用于所有情况，并强调了方法有效性高度取决于特定领域的分布变化和具体任务的特点。这表明在应用TAT时需要根据具体情况选择或设计更适合的方法。 |
| [EdgeSpot: Efficient and High-Performance Few-Shot Model for Keyword Spotting](https://arxiv.org/abs/2601.16316) | 贡献点:<br/><br/>1. **创新模型EdgeSpot**: 开发了一种适用于边缘设备的高效少样本语音识别模型，称为EdgeSpot。该模型结合了优化后的BC-ResNet音频基础结构、可训练的通道能量归一化前端和轻量级时域自注意力机制。<br/><br/>2. **知识蒸馏技术的应用**: 使用基于自我监督的学习方法进行训练阶段的知识蒸馏，通过采用优化过的Sub-center ArcFace损失函数来提升模型性能。这表明了在边缘设备上利用知识蒸馏能够显著提高语音识别的准确率。<br/><br/>3. **改进的准确性与计算效率**: EdgeSpot模型在整个固定错误警报率（False Alarm Rate, FAR）下，始终表现出比传统的BC-ResNet基线更高的准确性。具体地，EdgeSpot的最大变体EdgeSpot-4在1% FARP下的10射频准确度从73.7%提升至82.0%，而其计算成本仅为29.4M MACs，并且参数量只有128k。<br/><br/>4. **高效资源利用**: EdgeSpot通过优化模型结构和参数，达到了高精度的同时，显著降低了计算需求和系统负担。这使得该模型在资源受限的边缘设备上具有较高的应用价值。 |
| [TidyVoice: A Curated Multilingual Dataset for Speaker Verification Derived from Common Voice](https://arxiv.org/abs/2601.16358) | 贡献点:<br/><br/>1. **提出TidyVoice数据集** - 创造了一个名为TidyVoice的数据集，该数据集来源于Mozilla的Common Voice语料库，并通过处理其内部发言者异质性问题来生成。这个数据集对于多语言说话人识别系统的发展至关重要。<br/><br/>2. **解决多语言数据稀缺问题** - 解决了缺乏大型、公开可用且具有多语言功能的数据集的问题，特别是对于在反欺骗等应用中至关重要的阅读语音风格的领域。<br/><br/>3. **Tidy-M和Tidy-X条件定义** - 提出了两个不同的条件：Tidy-M用于包含81种语言中的单语发言者的目标和非目标试听（训练和测试数据来自超过21,200个单语发言人），而Tidy-X则包含多语言发言者的相同语言和跨语言试听。<br/><br/>4. **ResNet模型应用** - 应用两种ResNet模型架构，通过在全面的Tidy-M分割上进行微调，实现了35%的EER（错误率）表现。这表明了对TidyVoice数据集的有效利用。<br/><br/>5. **增强模型泛化能力** - 微调不仅提高了模型在TidyVoice上的性能，还增强了其在未见过的CANDOR语料库中的对话采访数据上的表现。<br/><br/>6. **公开资源提供** - 完整的数据集、评估试听以及使用这些模型进行实验的结果均对公众开放，为社区提供了一个新的研究资源。 |
| [FlowSE-GRPO: Training Flow Matching Speech Enhancement via Online Reinforcement Learning](https://arxiv.org/abs/2601.16483) | ### 贡献点:<br/><br/>1. **引入在线强化学习（RL）方法于生成性语音增强领域:** 该论文首次成功地将在线Group Relative Policy Optimization (GRPO)算法整合至流匹配的语音增强框架中，实现了与感知和任务导向指标的有效后训练对齐，并仅通过少量更新步骤就能达到效率。<br/><br/>2. **算法适应性:** 与其他工作专注于离线方法或大型语言模型上的在线GRPO不同，该论文针对连续、时间序列性质的语言和生成模型流匹配动态进行了算法的调整和优化。<br/><br/>3. **单一奖励目标下的快速指标提升与潜在问题:** 研究显示，通过优化单一奖励目标可以迅速提升评估指标，但可能会导致过度优化（reward hacking），虽然评分升高，但实际上降低了音频保真度。这揭示了单一奖励优化策略的局限性及潜在风险。<br/><br/>4. **多指标奖励优化策略:** 为了减少这种过拟合现象并提高整体性能，论文提出了一种多指标奖励优化策略，旨在平衡互斥的目标之间的冲突，显著减少了模型对特定评估指标的过度依赖和适应问题，并提升了性能的整体表现。<br/><br/>5. **实证验证与应用指导:** 实验结果验证了在线GRPO在语音增强领域的有效性，并为基于强化学习的生成性音频模型后训练提供了实用的操作指南。这不仅证明了该方法在实际应用中的可行性，也为未来研究提供了具体实施路径和建议。<br/><br/>综上所述，该论文对生成性语音增强领域做出了重要贡献，通过创新地将在线RL技术应用于流匹配框架，并解决了一般化、优化策略及多指标平衡等挑战，为后续相关研究提供了新的方法论和技术路线。 |
| [SoundBreak: A Systematic Study of Audio-Only Adversarial Attacks on Trimodal Models](https://arxiv.org/abs/2601.16231) | ### 贡献点：<br/><br/>1. **多模态模型的鲁棒性研究**：论文探讨了整合音频、视觉与语言的多模态基础模型在推理和生成任务上的性能，同时强调了它们对对抗操纵的稳健性仍然缺乏深入理解。<br/><br/>2. **实际且未被充分探索的威胁模型**：提出并分析了一个现实且鲜有研究的攻击模型——针对三模态（音频-视频-语言）模型的无目标、仅基于音频的对抗攻击。<br/><br/>3. **多模态处理阶段的目标式攻击**：对六种互补的攻击目标进行了详细分析，这些目标分别针对多模态处理的不同阶段，如音频编码器表示、跨模态注意、隐藏状态和输出似然性等。<br/><br/>4. **广泛的模型和基准测试**：在三个最先进的模型上以及多个评估标准中展示，仅基于音频的扰动能够引发严重的多模态失败，并达到高达96%的成功攻击率。<br/><br/>5. **低感知失真与优化的受益关系**：证明即使在极低感知失真（LPIPS <= 0.08, SI-SNR >= 0）和更高优化水平下，攻击也能成功，并且相比数据规模的增加，优化过程更能提升成功率。<br/><br/>6. **模型间和编码器间的可转移性限制**：指出不同模型和编码器之间攻击效果的传递能力有限，强调了特定系统的响应，如语音识别系统（如Whisper），主要对扰动幅度有反应，在严重失真下达到>97%的成功率。<br/><br/>7. **揭示多模态系统的单模态攻击面并提出防御建议**：通过上述发现，论文揭示了在多模态系统中被忽视的单一模态攻击途径，并激发了增强跨模态一致性来防御此类攻击的需求。 |
| [Contrastive Knowledge Distillation for Embedding Refinement in Personalized Speech Enhancement](https://arxiv.org/abs/2601.16235) | ### 贡献点：<br/><br/>1. **提出了一种实时的个性化语音增强（PSE）方法**，通过在推理时动态优化演讲者嵌入（speaker embedding），以提高性能并保持低计算负载。<br/><br/>2. **引入了新型对比知识蒸馏技术**，用于训练一个参数数量为150,000的小型演讲者编码器。该方法从复杂的嵌入中学习，显著提升个性化语音增强的效果。<br/><br/>3. **优化演讲者识别和去噪性能**：通过动态优化演讲者嵌入，在个性化语音增强系统中实现实时的细微调整，从而改进了目标声音提取的能力，特别是在存在干扰声的情况下。<br/><br/>4. **减小模型大小的同时提高性能**：使用小型编码器取代传统重载模型，降低了计算成本和资源消耗，同时保持或提高了个性化语音增强的结果质量。 |
| [The CMU-AIST submission for the ICME 2025 Audio Encoder Challenge](https://arxiv.org/abs/2601.16273) | 贡献点如下：<br/><br/>1. **提出系统结构**：基于“BEATs”模型，构建了一个用于音频编码挑战的提交系统。该系统利用了源自多个语音、音乐和声音语料库的74,000小时数据来扩展和增强BEATs模型。<br/><br/>2. **大规模模型开发**：将BEATs架构扩展并调整至30亿参数量级，这是一个显著的技术进步，提供了更高的处理能力和表示复杂性的能力。<br/><br/>3. **训练策略研究**：实验了以语音为中心的预训练混合物以及平衡后的预训练混合物，以探索不同领域数据对最终性能的影响。<br/><br/>4. **系统组成**：提交的系统包含了一个Dasheng 1.2亿参数模型和两个根据上述预训练数据混合优化的自定义规模增强的BEATs模型组成的集合。这种设计旨在结合每个构成模型的最佳能力。<br/><br/>5. **提升性能**：提出了一种简单的集合技术，该技术保留了构成模型的最优点，并且超越了基线和Dasheng 1.2B系统。<br/><br/>6. **开放科学贡献**：公开发布了训练后的检查点，以促进科学界的共享与重复验证，通过HuggingFace平台（https://huggingface.co/shikhar7ssu/OpenBEATs-ICME-SOUND 和 https://huggingface.co/shikhar7ssu/OpenBEATs-ICME）提供访问。 |
| [Auditory Attention Decoding without Spatial Information: A Diotic EEG Study](https://arxiv.org/abs/2601.16442) | ###贡献点:<br/><br/>1. **新领域拓展**: 提出了一种在全双工环境（diotic environments）下的听觉注意力解码方法(Auditory Attention Decoding, AAD)，该环境意味着左右两耳接收相同的声音混合，这与以往研究使用的空间依赖性不同，为更真实的“鸡尾酒会”场景提供了可能。<br/><br/>2. **技术融合**: 采用独立的编码器将脑电图(EEG)和语音信号映射到共享的潜在空间，通过这一方法结合了两种不同类型的生物医学信号处理技术。<br/><br/>3. **模型架构**: 提出了一种由BrainNetwork架构与两层一维卷积神经网络(CNNs)构成的模型架构。其中，BrainNetwork用于EEG的编码，而wav2vec 2.0被用作提取语音特征的方法。<br/><br/>4. **注意力识别方式**: 实现了通过计算EEG和语音表示之间的余弦相似度来确定关注的语音流，这种方法提供了一种新颖且有效的注意力识别机制。<br/><br/>5. **实际应用价值**: 这项技术对于实现智能助听器和客观听力测试系统具有重要意义，能够解决“鸡尾酒会问题”等挑战性的场景。<br/><br/>6. **性能提升**: 在全双工EEG数据集上的实验结果表明，该方法的准确率达到了72.70%，显著高于目前基于方向的AAD方法（22.58%的提升），这标志着在解决复杂听觉环境下的注意力解码问题上取得了进步。<br/><br/>###总结：<br/>这项研究通过提出一种面向全双工环境的听觉注意力解码框架，融合了脑电图和语音信号处理技术，采用创新的数据映射与模型架构，并实现了对关注语音流的有效识别。其显著提升的准确率标志着在解决实际聆听挑战（如“鸡尾酒会”问题）上取得了重要进展，为智能助听器和客观听力测试系统的发展提供了有力支持。 |
| [Do Models Hear Like Us? Probing the Representational Alignment of Audio LLMs and Naturalistic EEG](https://arxiv.org/abs/2601.16540) | ### 贡献点:<br/><br/>1. **系统性研究音频大型语言模型与人脑电生理活动的关联**: 通过比较12个开源音频大语言模型(Audio Large Language Models, ALLMs)和两个数据集中的事件相关脑电图(EEG)信号，该论文探索了自然听力场景下ALLM内部表示是否与人类神经动力学相一致。<br/><br/>2. **多层次表征相似性分析**: 应用了8种相似度指标（如Spearman-based代表相似性分析RSA）来描述句子内表示的几何结构。这种方法帮助研究人员深入理解了模型对不同句法和语义元素的响应方式。<br/><br/>3. **发现ALLMs的深度依赖性质**: 分析揭示了一种基于层次性的分割现象，即相似度指标的不同导致了模型排名的巨大差异，并且观察到在250-500毫秒的时间窗口内深度相关的峰状模式，与N400事件相关神经动力学特征一致。<br/><br/>4. **影响ALLMs几何相似性的情绪分离**: 研究发现负情绪（通过提出三模态邻域一致性（TNC）标准识别）可以降低几何相似度的同时增强基于协方差的依赖关系。这一发现表明情感对ALLM表征能力的影响是显著且可量化的。<br/><br/>5. **提供关于ALLMs表征机制的新神经生物学见解**: 这些发现提供了关于音频大型语言模型在处理和理解语音时内在工作机制的新视角，揭示了它们如何整合听觉感知与语言理解的过程，并提供了对当前模型性能的理解。 |
| [CORD: Bridging the Audio-Text Reasoning Gap via Weighted On-policy Cross-modal Distillation](https://arxiv.org/abs/2601.16547) | 贡献点:<br/><br/>1. **提出Cord框架**: 引入了CORD(跨模态自去噪)框架，用于解决大型音频语言模型(LALMs)在知识和推理能力方面的局限性。该框架通过在线上跨模态自引导方式有效地连接了特征表示空间中的听觉与语义差距。<br/><br/>2. **统一的多模态对齐**: CORD实现了音频条件下的推理解释与其文本条件对应的内部一致，这一过程在一个统一模型中完成。这意味着将文本作为内部教师，进行全音频过程中的多层次对齐。<br/><br/>3. **策略性多粒度对齐**: 在单词层面，CORD使用了基于政策的逆KL散度与重要性感知加权方法，优先处理早期和语义关键的单词；在序列层面，则引入了一种基于评判者的全局奖励机制，通过分组相对策略优化来最大化完整的推理路径。<br/><br/>4. **增强音频条件下的推理能力**: 实验结果表明，CORD能够一致地提升音频条件下的推理解释能力，并且仅需80,000个合成训练样本就能显著缩小音频和文本之间的性能差距，验证了其策略性、多层级跨模态对齐方法的有效性和数据效率。<br/><br/>5. **验证Cord的高效与实用性**: 通过多个基准测试表明，CORD不仅能够提升LALMs在处理音频信息时的表现，而且证明了其框架在数据使用方面具有高效率和实用性。 |
| [Omni-directional attention mechanism based on Mamba for speech separation](https://arxiv.org/abs/2601.16603) | 贡献点如下：<br/><br/>1. **新型模型的提出** - Mamba作为一种选择性状态空间模型（SSM），作为一个高效替代Transformer的方法，被用于语音建模，实现了对长序列处理的同时保持线性复杂度。这一创新为使用Mamba进行高效的长序列处理提供了可能。<br/><br/>2. **多维度注意力机制** - 提出了一种基于单向Mamba的高效全向注意力（OA）机制，该机制能够从声谱图的十个不同方向建模全局依赖关系，突破了传统方法仅在单一维度上的局部一维建模局限，有效地增强了模型捕获跨二维频谱全局依赖的能力。<br/><br/>3. **应用与评估** - 将这一全向注意力机制集成到两个基线分离模型中，并在三个公共数据集上进行了评估。通过实验验证了该方法的有效性，在保持线性复杂度的同时，实现了对现有最佳系统（SOTA）的显著性能提升。<br/><br/>4. **综合性能优势** - 实验结果显示，相较于现有的基准模型和SOTA系统，这一全向注意力机制在语音分离任务中表现出更优的性能，并且仍然保持了Mamba的高效线性复杂度特性。 |
| [I Guess That's Why They Call it the Blues: Causal Analysis for Audio Classifiers](https://arxiv.org/abs/2601.16675) | ### 贡献点:<br/><br/>1. **方法创新**: 本论文提出了一个新的方法，通过因果推理来发现特定音频分类任务所需的、充分且必要的频率空间特征。这种方法为理解音频分类器如何工作以及它们可能受到的操纵提供了深入洞察。<br/><br/>2. **工具开发**: 开发了一款名为FreqReX的工具实施这一算法，并在多个标准基准数据集上进行了实验，以验证方法的有效性与实用性。<br/><br/>3. **显著改变能力**: 实验结果表明，通过微小地调整输入频率（仅对大约240,000个频率中的一个），可以显著改变模型输出分类的准确性。具体而言，在240,000频率变化中，有58%会导致分类发生改变。<br/><br/>4. **不可感知性**: 频率的微小调整可能无法被人类耳朵察觉，这表明音频分类器对细微频率差异非常敏感。<br/><br/>5. **分析与操纵能力**: 这些结果证明了因果分析在理解音频分类器的决策过程以及成功操控其输出方面的实用价值。这一发现不仅揭示了音频分类器潜在的脆弱性和可欺骗性，也为未来开发更安全、可靠的音频识别系统提供了指导。<br/><br/>6. **理论与实践结合**: 结合理论方法（因果推理）和实际应用（通过工具实现），为理解并改善音频分类系统的性能提供了一个综合框架。 |
| [E2E-AEC: Implementing an end-to-end neural network learning approach for acoustic echo cancellation](https://arxiv.org/abs/2601.16774) | ### 贡献点:<br/><br/>1. **新型神经网络端到端回声消除方法**: 提出了一种基于深度学习的、无需依赖传统线性回声消除技术(即线性回声消除, LAEC)和时间延迟估计的端到端回声消除方法。此方法具备流式推理能力。<br/><br/>2. **渐进学习策略**: 引入并优化了渐进学习策略，通过逐步提高回声抑制效果来改进算法性能。<br/><br/>3. **知识迁移机制**: 利用预先训练的LAEC模型进行初始化，并通过LAEC训练过程中获得的经验来进行知识传递。<br/><br/>4. **注意力机制优化**: 通过在注意力权重上应用损失函数优化注意力机制，以实现参考信号和麦克风信号之间精确的时间对齐。<br/><br/>5. **语音活动检测集成**: 集成了语音活动检测功能，用于增强语音质量并改善回声消除效果，在远端无语音时通过掩蔽网络输出来提升性能。<br/><br/>6. **实验验证有效性**: 通过在公开数据集上的实验验证了方法的有效性。 |
| [A Novel Transfer Learning Approach for Mental Stability Classification from Voice Signal](https://arxiv.org/abs/2601.16793) | ###贡献点:<br/><br/>1. **提出新型转换学习方法与数据增强技术**: 该研究通过结合转换学习和数据增强技术，为使用人类语音信号进行精神稳定分类提供了一种新颖的方法。这解决了有限数据可用性带来的挑战。<br/><br/>2. **利用卷积神经网络(CNN)分析声谱图图像**: 研究中采用了CNN来对从语音录音生成的声谱图图像进行分析，这是处理声音信号的一种有效方法。<br/><br/>3. **评估三种CNN架构**: 通过比较VGG16、InceptionV3和DenseNet121这三种CNN模型在三个实验阶段的表现：仅使用未增强数据训练、使用增强数据训练以及采用转换学习策略。这种方法涉及预训练模型于增强的数据集上，然后针对非增强的数据集进行微调。<br/><br/>4. **转换学习策略的实施**: 该策略包括先对增强的数据集进行模型预训练，然后再在原始数据集上进行微调，并严格遵守数据分离规则以防止数据泄露。<br/><br/>5. **显著提升分类性能**：与基线方法相比，采用提出的转换学习策略后，在分类性能方面有了显著提高。DenseNet121在这种设置下达到了最高的准确率94%和AUC得分99%，这表明结合数据增强和转换学习可以有效增强基于CNN的语音声谱图进行精神稳定分类的能力。<br/><br/>6. **提供非侵入性心理健康诊断工具**：研究结果强调了联合使用数据增强与转换学习的重要性，特别是对于通过声音信号评估精神健康状态。这为心理健康诊断提供了有前景的、非侵入性的工具。 |
| [Lightweight Implicit Neural Network for Binaural Audio Synthesis](https://arxiv.org/abs/2509.14069) | ### 贡献点：<br/><br/>1. **提出轻量级隐式神经网络（Lite-INN）**：针对高保真二声道音频合成领域中的计算资源密集问题，论文作者引入了一种名为“Lite-INN”的新型双阶段框架。该方法旨在通过减轻计算负担来提高边缘设备上应用的便利性。<br/><br/>2. **时间域扭曲与分阶段处理**：Lite-INN 首先利用时间域扭曲生成初步估计值，这一阶段后由“隐式二声道校正器（IBC）”模块进行细化。该流程分为两个阶段，提高了音频合成的质量和效率。<br/><br/>3. **隐式神经网络中的幅度和相位修正**：通过将幅度和相位纠正直接作为预测输出，Lite-INN 中的 IBC 模块实现了非常紧凑的模型架构设计。这一特性有助于显著减少参数数量和计算操作（MACs）。<br/><br/>4. **与最优基线模型相媲美的感知质量**：实验结果显示，虽然 Lite-INN 在参数量上减少了72.7%，但在MACs方面要求的操作次数也大大降低，同时仍能实现与最佳性能基准模型相当的听觉感知质量。<br/><br/>5. **解决合成质量和计算效率之间的权衡问题**：论文强调了Lite-INN的有效性，成功解决了高保真度音频边缘设备空间应用中的合成质量与计算效率之间的平衡问题，为该领域提供了新的解决方案。 |
| [A Lightweight Fourier-based Network for Binaural Speech Enhancement with Spatial Cue Preservation](https://arxiv.org/abs/2509.14076) | 贡献点如下：<br/><br/>1. **提出GAF-Net（Global Adaptive Fourier Network）**：一种旨在在性能和计算效率之间建立平衡的轻量级深度复数网络，以解决单声道语音增强中普遍存在的性能与计算成本之间的严重折衷问题。<br/><br/>2. **架构设计**：<br/>   - **第一部分**：采用短时傅里叶变换（Short-Time Fourier Transform）和gammatone特征的双特征编码器，增强了声学表示的鲁棒性。<br/>   - **第二部分**：一个通道独立的全局自适应傅里叶调制模块，高效捕捉长期时间依赖性，并保持空间线索。<br/>   - **第三部分**：实现动态门控机制以减少处理异常。<br/><br/>3. **性能表现**：<br/>   - GAF-Net在信度差（ILD和IPD误差）和客观可理解性（MBSTOI）等关键指标上表现出与先进方法相匹敌的竞争性性能，同时具有更少的参数和计算成本。<br/><br/>4. **实用价值**：这些结果证明GAF-Net为资源受限设备上实现高质量二声道处理提供了一种可行的方法。 |
| [Frame-Stacked Local Transformers For Efficient Multi-Codebook Speech Generation](https://arxiv.org/abs/2509.19592) | ### 贡献点:<br/><br/>1. **提出基于大型语言模型的语音生成方法**:<br/>   - 该研究聚焦于使用大型语言模型(Large Language Models, LLMs)进行语音生成，其中涉及到了对离散声码器的操作。声码器与文本令牌在结构上存在根本差异，因为它具有多代码库体系。<br/>   <br/>2. **面对依赖性挑战的预测方法**:<br/>   - 由于声码器在每一时间步需要联合预测N个代码本条目，这引入了依赖关系，对简单的并行预测方法构成挑战。这些模型通常面临着如何平衡解码效率与预测精度之间的权衡。<br/><br/>3. **探讨层级策略与局部变换器（Local Transformer, LT）**:<br/>   - 研究通过使用两种不同的LT架构来解决上述问题：一种是递归生成声码本的自回归变换器，另一种则是基于MaskGIT的迭代掩码预测转换器。这两种方法都支持帧堆叠，能够同时预测多个帧，并由LT解码其代码本。<br/><br/>4. **评估并行和迭代采样策略**:<br/>   - 通过广泛的分析，研究对不同吞吐量和质量范围内的平行与迭代采样策略进行了比较和权衡，旨在提供关于速度和感知质量之间平衡的见解。<br/><br/>5. **提出基于部署优先级的选择指南**:<br/>   - 根据计算效率和合成准确性等部署需求，研究为选择解码策略提供了实用指导原则。这有助于研究人员和开发者在实际应用中做出更合适的技术决策。 |
| [Enhanced Generative Machine Listener](https://arxiv.org/abs/2509.21463) | 贡献点:<br/><br/>1. **提出GMLv2模型**: 该论文介绍了一种基于参考的音频质量预测模型，专门用于预测由MUSHRA评分测量的主观音频质量。<br/><br/>2. **采用Beta分布损失函数**: GMLv2使用Beta分布作为损失函数来拟合听众的评级，增强了模型对听众反馈的适应性和准确性。<br/><br/>3. **融合神经音频编码（NAC）主观数据集**: 通过整合额外的NAC主观数据集，GMLv2扩展了其在不同内容类型和编解码器配置下的泛化能力和适用性。<br/><br/>4. **广泛测试集评估**: GMLv2在多样化的测试集中进行了广泛的评估，证明其在相关性和跨多种内容类型和编码器配置上可靠地预测主观评分方面均优于广泛使用的指标（如PEAQ和ViSQOL）。<br/><br/>5. **提供可扩展的自动框架**: 最终结果是GMLv2不仅为感知音频质量评价提供了大规模应用的可能性，而且有望加速现代音频编码技术的研究与开发。 |
| [Speaker Anonymisation for Speech-based Suicide Risk Detection](https://arxiv.org/abs/2509.22148) | ### 贡献点:<br/><br/>1. **开创性研究**: 首次系统地探索了基于语音的自杀风险检测中的说话者匿名化方法。该研究关注于在保护易受伤害群体的身份的同时，通过自动检测来预防青少年自杀事件。<br/><br/>2. **多维度技术整合**: 探讨了传统信号处理技术、神经语音转换和语音合成等多种匿名化手段，为实现有效的身份保护与信息保真之间找到平衡提供了多种策略选择。<br/><br/>3. **全面评估框架**: 建立了一个综合性的评估体系来量化保护说话者身份与维持对自杀风险检测至关重要的信息之间的权衡关系。这有助于在确保数据隐私的同时，不牺牲检测的有效性。<br/><br/>4. **复合匿名方法的优化**: 结果表明，通过结合保留互补信息的不同匿名化技术方法可以在保护易受伤害群体的身份方面取得显著效果，并且仍能保持与原始语音相当的检测性能。<br/><br/>### 中文总结：<br/><br/>本文提出了一个开创性的研究项目——对基于语音的人工自杀风险检测中的说话者匿名化进行了全面而系统的探讨。研究利用多种不同的方法，包括传统信号处理技术、神经语音转换和语音合成来保护个人身份。建立了一个综合评估框架，用以平衡在防止数据泄露或恶意滥用时的隐私保护需求与自动检测自杀风险时所需的关键信息之间的权衡。研究表明，通过组合使用保留互补信息的不同匿名化策略，在不牺牲检测准确率的情况下显著提高了对脆弱群体的身份保护能力。 |
| [Audio dequantization using instantaneous frequency](https://arxiv.org/abs/2510.16813) | ### 贡献点：<br/><br/>1. **提出了一种基于相位感知的去量化方法**：该论文介绍了一种新的音频处理技术，名为Phase-Aware Audio Dequantizer (PHADQ)，用于解决音频信号中的去量化问题。这种方法利用了一个相位意识正则化器，这在之前的一个音频填充问题中被成功应用。<br/><br/>2. **促进时间频谱中正弦成分的时域连续性**：通过这种方式，该方法旨在提高音频信号的时间频谱内的正弦组件之间的连续性，从而避免了使用基于l1范数的正则化方法时常见的能量损失伪影。<br/><br/>3. **评估与现有最佳方法进行比较**：论文对所提出的PHADQ方法进行了详细的评估，包括使用SDR（信噪比）和PEMO-Q ODG目标度量等标准。通过这些指标以及一种类似于MUSHRA的主观测试来与其他最先进的方法进行对比分析。<br/><br/>4. **多维度验证方法的有效性与实用性**：通过综合的客观和主观评测结果，论文证明了PHADQ在解决音频去量化问题时的优势，展现了该方法在实际应用中的潜力。 |
| [WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning](https://arxiv.org/abs/2509.04744) | ### 贡献点：<br/><br/>1. **创新的音乐评分多模态推理与分析基准（WildScore）** - 该研究首次提出了一个在野外收集的数据集，用于评估大型语言模型（MLLMs）在理解和回答复杂的音乐学问题时的能力。这个数据集的特点是从真实的音乐作品中抽取样本，并附带了用户生成的问题和讨论，全面涵盖了实用的音乐分析细节。<br/><br/>2. **多维音乐学分类体系** - 研究者提供了一个系统化的分类框架，包括高阶和细粒度的音乐学元组学（ontologies），旨在为MLLMs的能力评估提供一个结构化的方法。<br/><br/>3. **复杂音乐推理建模** - 将复杂的音乐推理问题转化为多选题回答的形式，使得MLLMs在象征性音乐理解方面的表现可以得到有控制、可扩展性的评估。这一方法允许研究人员和开发者对模型的性能进行系统分析和比较。<br/><br/>4. **实证基准测试与发现** - 通过使用WildScore数据集，该研究揭示了MLLMs在视觉-符号推理方面的一些有趣模式，展示了它们在象征音乐推理和分析中的潜力以及面临的挑战。这一实验提供了关于如何改进这些模型的宝贵见解。<br/><br/>5. **公开的数据集与代码发布** - 研究团队不仅分享了他们的发现结果，还发布了WildScore数据集及其相应的代码库，以促进学术界和工业界的进一步研究和应用开发。这为后续的研究者提供了一个宝贵的资源和起点。 |
| [SONAR: Self-Distilled Continual Pre-training for Domain Adaptive Audio Representation](https://arxiv.org/abs/2509.15703) | 贡献点如下：<br/><br/>1. **提出SONAR方法**：该论文提出了一个名为“SONAR”的持续预训练框架，用于领域适应的音频表示学习。这个框架旨在使用BEATs作为基础，并针对新数据和已有数据实施联合采样策略。<br/><br/>2. **解决知识遗忘问题**：面对新领域的快速变化和大量未标记音频数据的涌入，SONAR通过减轻灾难性遗忘来有效适应新域，确保模型不会忘记之前的训练知识。<br/><br/>3. **应对挑战**：<br/>   - 实施了新的及先前数据之间的联合采样策略。<br/>   - 应用了正则化技术以平衡特定性和泛化能力。<br/>   - 动态扩展分词器代码本（tokenizer codebook），以便处理新颖的声学模式。<br/><br/>4. **跨领域实验验证**：论文通过在四个不同领域的实验证明了SONAR方法的有效性，展示了其在保持高适应性和抵抗遗忘方面的优势。 |
| [Etude: Piano Cover Generation with a Three-Stage Approach -- Extract, strucTUralize, and DEcode](https://arxiv.org/abs/2509.16522) | ### 贡献点:<br/><br/>1. **Etude模型的引入**:<br/>   - 提出了一个三阶段的架构，包括提取、结构化和解码阶段（Extract, strucTUralize, and DEcode），用于钢琴曲目生成任务。<br/>   <br/>2. **融合节奏信息的重要性**:<br/>   - 强调了在钢琴曲目自动生成中融合节奏信息的关键性，这直接影响到音乐的整体质量和结构相似性。<br/><br/>3. **简化REMIX标记化方法**:<br/>   - 采用了一种基于REMI（Rhythmic Encoding for Musical Information）的简化标记化技术，用于预提取节奏信息。<br/><br/>4. **增强生成音乐的质量和流畅度**:<br/>   - Etude模型能够生成保留原歌曲结构、增加音乐流畅性和动态性的钢琴曲目，并支持高度可控制的风格注入生成。<br/><br/>5. **人类听众主观评估**:<br/>   - 通过与人工作曲家进行对比，Etude在主观评价中显著超越了先前的模型，达到了接近人工作曲的质量水平。 |
| [Adaptive Multimodal Person Recognition: A Robust Framework for Handling Missing Modalities](https://arxiv.org/abs/2512.14961) | 贡献点如下：<br/><br/>1. **创新的多模态人识别框架**：引入手势作为情境增强因素，补充传统的语音和面部等模态信息。该框架采用统一的混合融合策略，在特征级和评分级整合信息，以最大化表示丰富性和决策准确性。<br/><br/>2. **多层次信息融合方法**：利用分任务学习处理不同模态，后续通过交叉注意力和门控融合机制整合这些模态的信息，最后通过动态自信加权策略适应缺失数据。<br/><br/>3. **多模态系统性能提升**：在CANDOR（基于访谈的全新多模态数据集）上进行评估时，证明了提出的三模态系统在人识别任务中达到99.51%的Top-1准确率。该框架表现出色，甚至在单模态或双模态情况下也能实现最优性能。<br/><br/>4. **VoxCeleb1基准测试**：使用VoxCeleb1作为基准进行评估，在二模态模式下达到99.92%的准确率，并且优于传统方法。<br/><br/>5. **适应性与鲁棒性**：结果显示，即使某些模态不可用，系统也能保持高准确性，表明其在现实世界的人体识别应用中具有强大的鲁棒性和适应性。<br/><br/>6. **开源与数据共享**：提供了此次研究的代码和数据集供公众访问。 |
