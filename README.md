# 五里墩茶社
---
| Title | Date | Summary |
| --- | --- | --- |
# 小工蚁创始人
---
| Title | Date | Summary |
| --- | --- | --- |
# AIGCLINK
---
| Title | Date | Summary |
| --- | --- | --- |
# 技术爬爬虾
---
| Title | Date | Summary |
| --- | --- | --- |
# 秋芝2046
---
| Title | Date | Summary |
| --- | --- | --- |
# GitHub All Languages Daily Trending
---
| Title | Summary |
| --- | --- |
| [liyupi/ai-guide](https://github.com/liyupi/ai-guide) | ### Vibe Coding AI 教程概览<br/><br/>#### **关于项目**<br/>Vibe Coding 是一款基于 AI 的创意生成工具，旨在帮助用户通过简单的操作和指令创造出各种各样的艺术作品、设计或文本。为了让更多人了解和使用 Vibe Coding，程序员鱼皮创建了一个全面的教程和知识库。<br/><br/>#### **主要内容**<br/><br/>1. **AI 引擎介绍**：Vibe Coding 拥有强大的 AI 生成功能，用户只需输入特定指令或描述即可生成图片、音乐、文本等创意内容。<br/><br/>2. **教程文档**：<br/>   - **基础使用**：涵盖入门指南和常见操作技巧。<br/>   - **高级技巧**：深入探讨进阶功能和优化方法。<br/><br/>3. **应用案例**：展示 Vibe Coding 在艺术创作、设计项目和个人表达上的实际应用。<br/><br/>4. **技术解析**：解析 AI 后端的技术原理，解释如何生成复杂的内容。<br/><br/>5. **资源与社区**：<br/>   - **交流群**：提供一个平台进行互动和问题解答。<br/>   - **官方公众号**：定期分享最新动态、教程和灵感案例。<br/>   - **共建机会**：邀请对 AI 和 Vibe Coding 感兴趣的人一起参与知识库的构建。<br/><br/>6. **社区支持与反馈**：<br/>   - **Star 支持**：鼓励用户通过星标项目来表示赞赏和支持。<br/>   - **交流与提问**：在官方社区中提供一个开放环境，收集用户意见和建议。<br/><br/>7. **学习与探索**：<br/>   - **教程文档**可在 GitHub 上获取原始代码和资源。<br/>   - **在线阅读**平台便于快速访问和互动。<br/><br/>#### **参与共建**<br/><br/>程序员鱼皮鼓励所有对 AI 和 Vibe Coding 感兴趣的人们一起贡献知识、经验和创意，共同提升和丰富项目内容。这不仅是一个学习之旅，也是一个共享与合作的过程。<br/><br/>#### **目标与愿景**<br/><br/>- **分享价值**：通过分享教程和资源，帮助更多人掌握 Vibe Coding 的使用技巧。<br/>- **促进创新**：鼓励用户探索 AI 在不同领域的应用，激发创意和创新。<br/>- **构建社区**：创建一个充满活力、支持和合作的社区环境。<br/><br/>#### **总结**<br/>Vibe Coding AI 教程是一个全面的学习资源库，旨在引导用户从零开始，逐步掌握 Vibe Coding 的强大功能，并鼓励大家将所学应用于实际创作中。无论你是AI初学者还是进阶使用者，这里都有适合你的学习路径和社区支持。加入我们，一起探索 Vibe Coding 的无限可能！ |
| [datawhalechina/hello-agents](https://github.com/datawhalechina/hello-agents) | 欢迎来到《你好，AI》项目！这个项目是一个全面的指南，旨在帮助大家从基础知识到高级实践，全方位了解人工智能。以下是对项目的核心内容和贡献者的快速概述：<br/><br/>**项目目标：**<br/>- **提供全面学习资源:** 通过一系列精心设计的内容模块、习题、案例研究以及社区分享，覆盖AI领域的各个层面。<br/>- **鼓励实践与交流:** 鼓励读者通过实践、讨论和分享增强对AI的理解。<br/>- **社群建设:** 建立一个充满活力的学习者社区，促进知识的传播和个人成长。<br/><br/>**核心贡献者：**<br/>- **陈思州** - 项目负责人，全面负责内容创作和校对工作（Datawhale 成员）。<br/>- **孙韬** - 联合发起者及CAMEL-AI成员，负责第九章的内容贡献与校验。<br/>- **姜舒凡** - 另一位联合发起者，负责章节习题的设计和校对任务。<br/>- **黄佩林** - Agent 开发工程师，贡献第五章内容。<br/>- **曾鑫民** - 牛客科技成员，参与第十四章案例开发。<br/>- **朱信忠** - 指导专家及Datawhale首席科学家（浙江师范大学杭州人工智能研究院教授），提供专业指导。<br/><br/>**额外章节贡献者：**<br/>- WH、周奥杰、张宸旭、黄宏晗和王大鹏等成员，分别对不同章节进行内容上的补充和贡献。<br/>  <br/>**特别感谢：**<br/>项目得到了多个组织和个人的支持与帮助。感谢Sm1les的贡献和支持，以及所有参与项目的开发者。<br/><br/>###开源许可协议：<br/>《你好，AI》项目采用知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议进行许可，鼓励用户在非商业用途下自由分享和再利用内容。<br/><br/>**交流平台：**<br/>加入读者交流群或关注Datawhale公众号，获取更多优质开源资源、最新动态及与AI领域专家和其他学习者互动的机会。扫描二维码即可访问相应的链接。<br/><br/>通过这个项目，我们旨在构建一个开放、共享的学习环境，鼓励所有人探索人工智能的世界，提高技能，并在全球社区中分享知识和经验。 |
| [D4Vinci/Scrapling](https://github.com/D4Vinci/Scrapling) | Scraping Library的概览和使用方法：<br/><br/>1. **安装**：<br/>   - 使用pip安装基础版本：`pip install scrapling`<br/>   - 安装额外功能（如fetchers、MCP服务器、shell命令等）: `pip install "scrapling[fetchers]"`, `"scrapling[ai]"`, `"scrapling[shell]"`或`"scrapling[all]"`<br/><br/>2. **核心功能**：<br/>   - 使用Scraping Library可解析HTML和XML文件，提取所需数据。<br/>   - 包括一个适应性元素查找模块（Element Finder），自动匹配所需的HTML元素。<br/><br/>3. **高级特性**：<br/>   - **Web Scraping Shell**：命令行界面用于自动化、批量或交互式的数据抓取任务。<br/>   - **MCP服务器**：用于处理和生成动态内容，帮助绕过反爬机制。<br/><br/>4. **适应性**：<br/>   - 自动化HTML解析，适配多种网站结构。<br/><br/>5. **依赖管理**：<br/>   - 必须先安装浏览器相关的系统依赖（如驱动、库等）。<br/><br/>6. **安全与法律**：<br/>   - 强调遵守当地和国际数据收集法规，尊重网站的使用条款和服务。<br/>   <br/>7. **贡献指南**：<br/>   - 鼓励社区参与并提供详细指导。<br/><br/>8. **许可证**：<br/>   - BSD-3-Clause许可。<br/><br/>确保在实际应用中遵循相关的法律、政策和道德标准。 |
| [bytedance/deer-flow](https://github.com/bytedance/deer-flow) | Deer Flow 是一个模型中立的 AI 平台，适用于与任何实现 OpenAI 兼容 API 的大型语言模型（LLM）进行交互。它专为支持深度研究、多步任务和具备推理能力的语言模型而优化。以下是一些总结性要点：<br/><br/>### 功能亮点：<br/>1. **长上下文窗口**：适合进行深入研究和复杂任务。<br/>2. **推理能力**：能够适应规划和分解复杂问题。<br/>3. **多模态输入支持**：处理图像理解与视频解释。<br/>4. **强大的工具使用**：确保稳定的功能调用和结构化输出。<br/><br/>### 使用场景：<br/>- 进行深度分析研究<br/>- 解决需要多个步骤的复杂问题<br/>- 利用多媒体信息进行决策或创建内容<br/><br/>### 开发背景及贡献：<br/>Deer Flow 基于 LangChain 和 LangGraph 的创新框架，为用户提供了一个易于使用的平台，能够整合和利用来自这两个库的强大功能。这些库在多代理协调、链式处理和结构化输出方面提供了核心支持。<br/><br/>### 技术架构与文档资源：<br/>包括详细的开发指南（CONTRIBUTING.md）、配置说明（CONFIGURATION.md）以及对架构的深入解释（Architecture Overview 和 Backend Architecture）。这些文档为开发者提供了一套全面的指导，从环境搭建到具体实现细节都有详尽介绍。<br/><br/>### 许可证信息：<br/>遵循 MIT License 协议进行开源发布，鼓励社区参与贡献和修改。<br/><br/>### 感谢与合作项目：<br/>对背后支持 Deer Flow 的其他开源项目以及关键贡献者表示感谢。LangChain 和 LangGraph 在平台的功能性和易用性上发挥了核心作用，并得到了社区的广泛认可。<br/><br/>Deer Flow 旨在推动 AI 技术在研究、分析和决策过程中的应用，通过强大的模型集成能力和定制化工具，为用户提供了一个高效、灵活的工作环境。 |
| [abhigyanpatwari/GitNexus](https://github.com/abhigyanpatwari/GitNexus) | GitNexus是一个基于本地机器的代码智能工具和社区发现平台，它允许开发者在其自己的计算机上解析和理解源代码。以下是关键点：<br/><br/>1. **功能概述**：<br/>   - **代码解析与理解**：通过Tree-sitter进行语法分析，帮助理解不同编程语言中的代码结构。<br/>   - **社区发现**：在大型代码库中自动识别相关和相关的代码块或函数，以增强代码的可维护性和重用性。<br/>   - **过程检测**：识别代码中的特定模式（如控制器、服务等），提供清晰的过程结构视图。<br/>   - **进程组搜索**：通过聚类相似过程来提高搜索效率和精确度。<br/>   - **360度上下文视图**：提供函数在项目中使用的广泛上下文信息，包括调用者、被调用者以及其内部实现。<br/><br/>2. **代码智能工具**：<br/>   - **MCP命令行界面（CLI）**：允许通过标准输入和输出与GitNexus交互。<br/>   - **Web前端**：使用React 18构建的用户界面，提供强大的浏览器集成体验。<br/>   - **尾风框架（Tailwind v4）**用于快速、灵活的设计。<br/><br/>3. **安全性与隐私**：<br/>   - 所有数据处理在本地执行，不涉及网络调用或上传代码到服务器。<br/>   - 原生运行在Web浏览器中，并使用Web Workers和Comlink进行并行计算，无需依赖后端服务。<br/><br/>4. **性能提升与优化**：<br/>   - 通过多线程（worker threads）处理并发任务，提高资源利用效率。<br/>   - 使用WebGPU或Web Workers结合Comlink实现高性能的机器学习操作，比如使用Hugging Face提供的transformers.js在浏览器中进行模型推理。<br/><br/>5. **社区与贡献**：<br/>   - 支持多种编程语言（包括但不限于9种），促进跨语言项目的代码理解。<br/>   - 包括安全性和隐私保护措施，确保用户数据的安全和隐私。<br/><br/>6. **开发进度与未来规划**：<br/>   - 完成了多项关键功能的开发，如wiki生成、多文件重命名支持等，并计划在未来增加更多社区发现能力。<br/>   - 项目代码开源，允许开发者审查源码。<br/><br/>通过这些特性，GitNexus旨在成为开发者的工作流程中的强大辅助工具，帮助更有效地理解和维护复杂项目。 |
| [shareAI-lab/learn-claude-code](https://github.com/shareAI-lab/learn-claude-code) | 这个文档主要讲述了一种设计思想和实现流程，目的是构建一个高效、灵活且易于扩展的自动化工具或系统。关键点包括：<br/><br/>1. **The Agent Loop**（代理循环）: 强调使用简单的脚本语言（如Bash）作为基础框架来实现自动任务处理。<br/><br/>2. **Tools (s02)**: 说明如何在不改变核心流程的基础上引入和利用工具，即“loop didn't change”。<br/><br/>3. **TodoWrite**（计划前置）：强调在执行操作前进行详细的计划或预览。<br/><br/>4. **Subagents**: 强调进程隔离等同于上下文隔离，表示通过子代理实现任务的独立处理和上下文管理。<br/><br/>5. **Skill Loading (s05)**: 提出加载技能时应按需加载，避免提前大量加载可能不使用的资源。<br/><br/>6. **Context Compact**: 引入战略遗忘的概念，即在必要时清理不再需要的信息或上下文，以优化性能和内存使用。<br/><br/>7. **Tasks and Task System**（任务系统）：实现一种任务管理系统，使状态可以在/compact的环境中生存。<br/><br/>8. **Background Tasks**: 实现后台任务处理机制，允许在无需等待或关注的情况下执行耗时操作。<br/><br/>9. **Agent Teams**: 介绍代理团队的概念，使用不同的请求ID来区分不同的协议或通信方式。<br/><br/>10. **Autonomous Agents**（自主代理）：定义和实现自动化的、能够自我调度、工作和重复执行任务的机制。<br/><br/>11. **Worktree and Task Isolation**（工作树与任务隔离）: 强调通过目录进行任务隔离，并使用任务ID来协调任务间的关系。<br/><br/>整个文档强调了将模型作为核心，围绕它提供工具和服务，并尽量减少干预的设计原则。这有助于构建可扩展、易于维护和高效率的自动化系统或工具链。 |
| [obra/superpowers](https://github.com/obra/superpowers) | 这个文档描述了一个名为“Superpowers”的工具或框架，旨在帮助开发者和程序员使用特定的技能集（或称为“技能”）来提高代码开发过程中的效率、质量以及协作。主要关注点包括测试驱动开发、系统化问题解决、复杂性降低、和基于证据而不是仅凭声称的结果决策等原则。<br/><br/>### 关键组件：<br/><br/>- **测试**：强调了测试优先的开发流程，通过创建测试用例在编写代码之前验证功能。<br/>- **调试**：引入了一个4阶段的根因分析流程来定位问题，并确保修复有效。此外，还包含了防止测试失败或忽略特定条件的情况等细节。<br/>- **合作**：提供工具支持如Brainstorming、Plan Writing、Code Review等过程，以促进有效的团队协作和规划。<br/><br/>### 原则：<br/><br/>1. **测试驱动开发（Test-driven development, TDD）**：优先编写测试用例来指导代码开发，确保每一部分功能的正确性。<br/>2. **系统化方法**：强调遵循明确的过程和步骤，而不是依赖直觉或随机猜测解决方案。<br/>3. **减少复杂性**：追求简洁、易懂且高效的解决方案，避免不必要的复杂度。<br/>4. **基于证据的决策**：在宣布成功之前验证所有变化和实现。<br/><br/>### 实践方式：<br/><br/>- 开发者可以通过创建新的技能（Skill）来扩展“Superpowers”框架的功能，遵循提供的指导原则和测试方法论。<br/>- 管理更新通过自动安装新版本或手动执行命令完成。<br/><br/>### 社区与资源：<br/><br/>- **问题报告**：用户可以在官方GitHub页面上提交问题反馈或建议。<br/>- **市场获取工具**：提供了专门的GitHub仓库来获取和管理技能库。<br/><br/>文档总结了“Superpowers”旨在提升软件开发流程的整体质量和效率，通过强调最佳实践、自动化过程管理和社区参与，促进开发者之间的协作与知识共享。 |
| [x1xhlol/system-prompts-and-models-of-ai-tools](https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools) | 该文档是关于一个名为`system-prompts-and-models-of-ai-tools`的项目或仓库，其中包含有关AI系统提示和模型功能的深入信息。以下是简要总结：<br/><br/>1. **概述**：<br/>   - 该项目旨在整理AI系统、模型以及相关工具的大量代码示例，提供超过30,000行的详细信息。<br/>   - 它通过Cloudback平台显示构建状态，并使用DeepWiki进行问题和建议的收集。<br/><br/>2. **支持项目**：<br/>   - 作者提供了多种方式来支持项目，包括捐赠比特币、莱特币或以太坊，以及访问Patreon和Ko-fi页面。<br/>   - 鼓励用户提供反馈并提出新功能请求。<br/><br/>3. **赞助机会**：<br/>   - 提供了与项目的合作机会，为对AI系统有深入理解的开发者提供展示平台。<br/><br/>4. **安全建议**：<br/>   - 对AI初创公司提供建议，强调保护数据安全的重要性，避免暴露API或模型代码给潜在的安全威胁。<br/><br/>5. **联系我们**：<br/>   - 列出了通过X、Discord和电子邮件联系作者的方式。<br/><br/>6. **项目贡献**：<br/>   - 邀请用户通过提出问题来参与项目的改进和发展。<br/>   <br/>7. **历史星标趋势**：<br/>   - 提供了一个图表显示该项目的星级历史，鼓励用户根据需要给予star支持。<br/><br/>简而言之，这个文档是一个关于AI工具、系统和模型的全面资源库的介绍与宣传材料，旨在为开发者提供深入信息并提供参与和贡献的机会。 |
| [huggingface/skills](https://github.com/huggingface/skills) | 本段文字主要讲述了如何在代码智能助手（如Claude）中使用和定制Hugging Face技能。以下为要点概述：<br/><br/>1. **已有技能的使用**：<br/>   - 当向代码智能助手提供指令时，可以直接提及已安装的技能名来使用功能，比如使用HF LLM训练器估算大型模型运行所需的GPU内存、或使用HF模型评估技能启动特定脚本等。<br/><br/>2. **定制和贡献新技能**：<br/>   - 复制现有的技能文件夹（如hf-datasets/），进行命名修改。<br/>   - 在新目录下的`SKILL.md`中更新描述部分，定义技能功能及使用场景。<br/>   - 添加或编辑支持的脚本、模板等，确保与说明相匹配。<br/><br/>3. **市场发布**：<br/>   - 使用`.claude-plugin/marketplace.json`文件来为每个技能编写面向人类用户的简短描述。<br/>   - 通过`./scripts/publish.sh`脚本来生成和验证所需的所有元数据。<br/><br/>4. **更新技能包**：<br/>   - 在智能助手中重新安装或刷新技能库，以使定制的技能版本可用。<br/><br/>5. **参考文档**：<br/>   - 可直接在GitHub上的huggingface/skills仓库浏览最新的指令、脚本和模板。<br/>   - 检阅Hugging Face官方文档来理解所引用库或工作流程的具体用法。<br/><br/>简而言之，这段文字是关于如何在代码智能助手中集成、使用和改进Hugging Face提供的技能集的指南。 |
| [katanemo/plano](https://github.com/katanemo/plano) | Plano是一个轻量级的自动化工具，它帮助开发者在没有构建大型基础设施的情况下实现多个AI模型之间的高效协同和管理。以下是对原文的中文概述：<br/><br/>1. **自动代理组合与模型管理**：<br/>   - 无需编写复杂的意图分类器或路由逻辑代码。通过简单的YAML配置文件即可定义代理描述。<br/>   - 统一管理不同API提供商的服务，简化模型切换和升级流程。<br/><br/>2. **自动可观测性与日志记录**：<br/>   - 提供端到端的自动追踪，利用OpenTelemetry技术收集数据，无需额外编写监控代码。<br/>   - 实时获取详细的执行过程、性能指标和错误日志信息。<br/><br/>3. **轻量级AI模型路由**：<br/>   - 使用定制化的4B参数AI模型进行任务路由决策，相比大型GPT模型更高效、成本更低且延迟更低。<br/><br/>4. **轻松添加新代理与服务升级**：<br/>   - 通过简单的配置更改即可增加新代理或更新现有服务。无需深入代码内部，减少维护和部署时间。<br/>   <br/>5. **社区与贡献方式**：<br/>   - 邀请开发者加入Discord服务器讨论和提供反馈。<br/>   - 支持多方面贡献包括修复错误、添加新功能、优化文档等。<br/><br/>6. **快速上手指南与资源**：<br/>   - 提供了详细的操作指南，帮助用户快速启动并使用Plano。<br/>   - 包括LLM路由管理、代理组合、过滤链规则、提示目标配置等概念的介绍和实践指导。<br/><br/>7. **支持与反馈**：<br/>   - 鼓励用户通过GitHub页面上的项目路线图提供反馈和建议，同时欢迎任何形式的技术贡献。<br/>   - 通过stars方式表明对项目的认可和支持。<br/><br/>总之，Plano是为AI驱动的应用场景而设计的一站式解决方案，它简化了模型集成、调度和监控过程，为开发者提供了更多时间和精力用于创新。 |
| [siteboon/claudecodeui](https://github.com/siteboon/claudecodeui) | **项目概述**<br/><br/>此项目是一个基于React/Vite的用户界面工具，用于管理和操作Anthropic的Claude Code、Cursor CLI和OpenAI Codex等代码处理工具。主要功能包括文件浏览器、代码编辑器集成，并可能提供AI项目管理与任务规划特性。<br/><br/>### 技术栈<br/><br/>- **编程语言**：使用React进行前端开发，Vite作为构建工具。<br/>- **CSS框架**：采用Tailwind CSS以快速创建响应式设计。<br/>- **代码编辑器**：整合CodeMirror来增强代码编辑体验。<br/>- **AI功能**：可选集成TaskMaster AI，提供项目管理和任务规划服务。<br/><br/>### 项目组件<br/><br/>1. **文件浏览器** - 用于浏览和管理与Claude Code关联的项目文件。<br/>2. **代码编辑器** - 提供高级代码编写支持，包括语法高亮、代码完成等功能。<br/>3. **项目集成** - 直接与Anthropic的CLI工具（如Claude Code）进行交互。<br/>4. **AI增强功能** - 选项性整合TaskMaster AI提供自动化任务处理和项目管理。<br/><br/>### 发展路线<br/><br/>- **优化用户界面**：提升React组件设计，改进用户体验。<br/>- **增强文件浏览器功能**：增加更强大的文件操作和搜索特性。<br/>- **集成更多代码工具**：未来可能考虑与更多类似的代码工具兼容。<br/>- **AI功能扩展**：逐步引入或增强AI项目管理模块。<br/><br/>### 发布和贡献<br/><br/>- **开源许可证**：遵循GNU General Public License v3.0，允许自由使用、修改和分发源代码。<br/>- **社区参与**：鼓励通过“星星”和关注GitHub仓库来支持项目。同时欢迎通过Issue或Pull Request的方式参与贡献。<br/><br/>### 社区与支持<br/><br/>该项目面向Claude Code、Cursor和Codex等工具的用户社群，并得到了Sponsor Siteboon的支持，其提供AI驱动的网站构建服务，表明对社区发展的认可和支持。<br/><br/>---<br/><br/>### 中文翻译：<br/><br/>**项目概述**<br/><br/>这是一个使用React/Vite搭建的用户界面工具，专门用于管理和操作Anthropic的Claude Code、Cursor CLI和OpenAI Codex等代码处理工件。主要功能包括文件浏览、集成高级代码编辑器，并可选提供AI驱动的项目管理与任务规划服务。<br/><br/>### 技术栈<br/><br/>- **编程语言**：React作为前端开发工具，Vite用于构建。<br/>- **CSS框架**：使用Tailwind CSS实现快速响应式设计。<br/>- **代码编辑组件**：集成CodeMirror增强代码编写体验。<br/>- **AI功能**：可选接入TaskMaster AI提供项目管理和任务规划能力。<br/><br/>### 构建模块<br/><br/>1. **文件浏览器** - 用于浏览和操作与Claude Code相关的项目文件。<br/>2. **代码编辑器** - 包含高级代码功能，如语法高亮、代码自动完成等。<br/>3. **工具集成** - 直接支持与CLI工具（如Claude Code）的交互。<br/>4. **AI增强服务** - 选择性整合TaskMaster AI提供自动化项目管理和任务处理。<br/><br/>### 未来发展<br/><br/>- **改进用户界面**：优化React组件以提升用户体验。<br/>- **增强文件浏览器功能**：增加更多强大的文件操作和搜索特性。<br/>- **拓展工具集成**：考虑与更多类似的代码处理工件兼容。<br/>- **AI功能扩展**：逐步引入或增强AI项目管理模块。<br/><br/>### 发布与贡献<br/><br/>- **开源许可证**：遵循GNU通用公共许可证v3，允许自由使用、修改和分发源代码。<br/>- **社区互动**：通过GitHub仓库的“星星”和关注来支持项目，并鼓励通过Issue或Pull Request参与开发工作。<br/><br/>### 社区及支持<br/><br/>该项目面向Claude Code、Cursor和Codex等工具用户社群，并由Siteboon提供支持，这是一个AI驱动网站构建服务提供商。其对社区发展的认可表明了对项目的积极评价和支持。 |
| [muratcankoylan/Agent-Skills-for-Context-Engineering](https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering) | 该文档是一个关于如何构建和使用AI能力的指南，主要包括以下关键点：<br/><br/>1. **技能（Skill）**：定义了一种通用格式来组织和共享AI模型或组件的知识。每个技能都包含以下几个部分：<br/>   - `SKILL.md`：提供指令和元数据。<br/>   - 可选的脚本 (`scripts/`) 和引用 (`references/`) 文件，用于示例和额外文档。<br/><br/>2. **结构**：遵循统一模板来保持一致性，并简化分享和复用AI组件的过程。每个技能都遵循类似的组织结构，便于理解、评估和集成。<br/><br/>3. **贡献指南**：<br/>   - 遵循特定的提交模板和规则。<br/>   - 提供清晰和可操作的指令，包括示例代码或文档。<br/>   - 限制`SKILL.md`文件大小以保持读取性能，并在需要时补充额外信息。<br/><br/>4. **社区与合作**：鼓励AI开发者社区参与贡献，通过@Muratcan Koylan或其他渠道进行联系以寻求合作和反馈。<br/><br/>5. **许可**：遵循MIT开源许可证，允许自由使用、修改和分发这些技能。<br/><br/>6. **参考资料**：每个技能都基于来自顶级AI实验室或框架开发者的研究和实际生产经验。提供引用以支持其理论基础和实践建议。<br/><br/>文档旨在创建一个开放的、合作的环境，促进不同开发者之间的知识共享，并加速AI能力的创新和发展。 |
| [VectifyAI/PageIndex](https://github.com/VectifyAI/PageIndex) | PageIndex是一个创新的文本索引和检索系统，它不依赖于向量表示或传统搜索引擎架构。其核心目标是提供高效的文档搜索、树搜索以及更广泛的多模态推理任务的能力。该系统的独特之处在于：<br/><br/>1. **高效全文本检索**：无需构建大型向量空间，PageIndex能够快速地对大量文本数据进行索引和查询处理。<br/><br/>2. **多模态推理能力**：除了文本搜索，它还支持理解图像、代码等非文本内容，并能进行跨模态的关联与推理。<br/><br/>3. **文档结构的理解**：通过解析和提取文档的内部结构信息（如章节、标题），PageIndex能够提供更精确的结果定位。<br/><br/>4. **实时更新**：对于不断增长的数据集，系统能够动态地添加新数据，无需重建索引。<br/><br/>5. **适应性架构**：相对于基于向量的空间搜索，PageIndex通过构建抽象表示来处理复杂查询和上下文理解问题，提供了更好的灵活性和可扩展性。<br/><br/>6. **应用领域广泛**：从自然语言处理、信息检索到多模态AI系统，PageIndex的性能优势使其在多个应用场景中具有潜力。<br/><br/>###关键点：<br/>- 革新了传统的文本搜索方式。<br/>- 提供多模态推理能力，支持跨文本、图像等多种类型数据的交互与分析。<br/>- 独立于向量空间方法，能够实时更新和适应变化的数据集。<br/>- 为复杂查询提供更精确的响应，提高了信息检索的效率与准确性。<br/><br/>###未来展望：<br/>PageIndex系统的发展和应用将不断推动AI领域在文本处理、多模态融合等方面的技术进步，特别是对于实时动态数据环境中的高效搜索和理解有着巨大潜力。随着技术的成熟和完善，它有望成为下一代AI基础设施的核心组件之一，在构建更智能、更交互式的应用和服务中发挥关键作用。<br/><br/>###引用：<br/>请将以下BibTeX代码加入您的论文引用列表以引用此工作：<br/><br/>```<br/>@article{zhang2025pageindex,<br/>  author = {Mingtian Zhang and Yu Tang and PageIndex Team},<br/>  title = {PageIndex: Next-Generation Vectorless, Reasoning-based RAG},<br/>  journal = {PageIndex Blog},<br/>  year = {2025},<br/>  month = {September},<br/>  note = {https://pageindex.ai/blog/pageindex-intro},<br/>}<br/>```<br/><br/>###支持我们：<br/>在GitHub上给此项目点个星🌟，表示您对其的支持和认可。感谢您的参与！ |
| [NVIDIA/Megatron-LM](https://github.com/NVIDIA/Megatron-LM) | Megatron Core项目是一个用于训练大型参数语言模型的工具。它使用模型并行性在大量的GPU上并行处理计算任务，从而显著提高大模型的训练效率。<br/><br/>以下是对一些重要点的中文解释：<br/><br/>1. **MoE（Multiple Experts）Roadmap**：项目计划通过DeepSeek-V3、Qwen3等模型进一步优化平行性和FP8（半精度浮点数）优化，旨在提供更多的专家模式（Mixture of Experts），这是一种在大型语言模型中使用的方法，用于提高计算效率和性能。<br/><br/>2. **弱扩展性结果**：项目展示了从6144个H100 GPU到更大规模GPU的扩展能力。随着GPU数量增加，实现了超线性扩展，即随着模型大小增长，整体效率提升至47%-48%，这归因于大GEMM（General Matrix Multiply）操作在大规模计算中更高的算子密度。<br/><br/>3. **强扩展性结果**：对于标准的GPT-3模型（参数量超过175亿），使用相同序列批处理大小在96个到4608个GPU上进行了扩展测试。在更大规模下，通信开销增加导致整体效率从47%下降至42%，但仍然显示了有效的超线性扩展。<br/><br/>4. **文档和帮助资源**：项目提供官方文档用于指导用户理解如何使用Megatron Core，并有GitHub页面处理问题报告、功能请求等。此外，还提供了提交代码贡献的指南。<br/><br/>5. **贡献方式**：鼓励社区成员通过报告错误、提出新功能建议、改善文档或直接提交代码来贡献。对于具体贡献途径，可以参考项目提供的指导手册。<br/><br/>6. **引用**：如果在研究或项目中使用了Megatron Core，应正确引用论文《Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism》，该论文于2019年发表在arXiv预印本上。<br/><br/>总之，Megatron Core是为大规模语言模型训练设计的高效并行计算框架。通过采用先进的计算优化和模式，并支持从低到高规模的GPU扩展测试，它能够极大地提升训练效率。 |
| [NevaMind-AI/memU](https://github.com/NevaMind-AI/memU) | 以下是关于MemU项目的摘要和关键信息：<br/><br/>1. **项目概述**：<br/>   MemU是一个AI驱动的自然语言处理系统，旨在提供语义理解和生成能力。它采用分布式架构设计，支持多模态数据输入（文本、图像）并生成响应。<br/><br/>2. **主要功能**：<br/>   - 多模态理解：MemU能够处理文本和非文本信息（如图片），为跨模态任务提供解决方案。<br/>   - 语义深度解析：系统具备深入理解用户需求的能力，进行复杂的自然语言理解和生成过程。<br/>   - 分布式架构：利用分布式计算资源提高性能、处理大规模数据以及实现高可用性。<br/><br/>3. **技术栈**：<br/>   - 使用PyTorch作为主要的深度学习框架。<br/>   - 与其他开源项目合作（如LazyAGI、LazyLLM）来增强功能或进行联合开发。<br/>   - 通过uv工具管理依赖和构建环境，确保开发者体验一致且高效。<br/><br/>4. **使用场景**：<br/>   MemU适合应用于需要多模态理解和生成能力的各类应用，例如智能客服、内容生成、创意设计辅助等领域。<br/><br/>5. **贡献方式**：<br/>   - 开源社区欢迎对MemU进行技术改进和功能扩展。<br/>   - 通过GitHub提交代码修正、新功能开发或文档更新。<br/>   - 遵循详细的贡献指南以确保贡献的质量和兼容性。<br/><br/>6. **许可条款**：<br/>   MemU采用Apache License 2.0授权，允许商业和非商业使用，修改以及分发。<br/><br/>7. **社区参与**：<br/>   - GitHub Issues用于报告错误、提出功能请求或寻求技术支持。<br/>   - Discord频道提供实时交流、讨论和技术支持。<br/>   - Twitter账号用于发布项目更新与活动通知。<br/><br/>8. **目标**：<br/>   MemU致力于通过不断的技术创新提升AI处理自然语言的能力，服务于更广泛的用户群体和产业需求。 |
| [ruvnet/ruvector](https://github.com/ruvnet/ruvector) | 这个文档提供了一个高级概述，介绍了“ruvector”项目的主要组件、目标和贡献方式。以下是关键要点的简要汇总：<br/><br/>1. **Project Overview**：<br/>   - “ruvector”是一个集成向量数据库、图数据库、深度神经网络（GNN）层、人工智能代理路由（FastGRNN）、WebAssembly和Node.js绑定的大型跨库项目。<br/>   - 它旨在构建一个智能容器体系，称为RVF，以提供“越来越聪明”的向量搜索功能。<br/><br/>2. **Core Components**：<br/>   - **ruvector-core**: 包含HNSW索引等底层存储和数据结构。<br/>   - **ruvector-graph**: 负责图形数据库、Cypher查询解析和Hyperedges操作。<br/>   - **ruvector-gnn**: 提供GNN层、压缩技术和训练工具。<br/><br/>3. **AI Agent Routing**：<br/>   - 使用FastGRNN来路由不同技能水平（如Haiku到Opus的复杂性级别）的人工智能代理。<br/><br/>4. **Contribution Guide**：<br/>   - 文档提供了详细的指南，说明如何参与项目贡献代码和文档。<br/>   - 包括测试、基准测试、构建WebAssembly和Node.js绑定等步骤。<br/><br/>5. **License**：<br/>   - 项目采用MIT许可，允许商业和个人自由使用。<br/><br/>6. **Community Resources**：<br/>   - 提供了GitHub仓库链接（https://github.com/ruvnet/ruvector）、npm包页面（https://npmjs.com/package/ruvector）和crates.io页面（https://crates.io/crates/rvf_runtime），方便社区成员访问项目资源。<br/>   - 官方文档位于指定位置，RVF框架的介绍可以通过以下链接查找：https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/<br/><br/>7. **GitHub and NPM Usage**：<br/>   - 项目的GitHub仓库和npm包可以用于安装和部署“ruvector”库。<br/>   - 提供了命令行指令示例，如测试、构建WASM等。<br/><br/>8. **Development and Documentation**：<br/>   - 强调了对贡献者开放的文档和指导文件（CONTRIBUTING.md）以促进社区协作。<br/><br/>整体来看，“ruvector”项目旨在提供一个高度自适应和智能的搜索解决方案，并通过其广泛的组件集和技术堆栈，为用户提供从底层数据结构到上层应用功能的全面支持。该框架不仅适用于数据库和查询优化领域，还考虑了向人工智能代理和复杂系统路由方面的扩展能力。 |
# eess.AS updates on arXiv.org
---
| Title | Summary |
| --- | --- |
| [iMiGUE-Speech: A Spontaneous Speech Dataset for Affective Analysis](https://arxiv.org/abs/2602.21464) | 贡献点如下：<br/><br/>1. **提出了iMiGUE-Speech数据集**：这是对iMiGUE原始数据集的扩展，提供了关于情感和情绪状态的自发性语音语料库，用于研究在真实比赛结果中自然产生的自发情感。<br/><br/>2. **新增元数据**：该数据集增加了额外的元数据信息，包括语音转录、面试者与被采访者的角色分离以及以词级进行强制对齐的数据。<br/><br/>3. **对比评估的任务引入**：为了展示数据集的效用并建立初始基准，引入了两项用于比较评估的任务：即语音情绪识别和基于文本的情绪分析。这些任务利用最先进的预训练表示，评估数据集捕获自发情感状态的能力，从声学和语言模式两个方面进行。<br/><br/>4. **多模态资源**：iMiGUE-Speech可以与原iMiGUE数据集中提供的微手势注释同步匹配，形成一个独特的多模态资源，用于研究言语-手势情绪动力学。<br/><br/>5. **可用性声明**：该扩展的数据集通过GitHub（https://github.com/CV-AC/imigue-speech）提供。 |
| [A Knowledge-Driven Approach to Music Segmentation, Music Source Separation and Cinematic Audio Source Separation](https://arxiv.org/abs/2602.21476) | ### 贡献点:<br/><br/>1. **知识驱动的音频分割方法**: 提出了一种基于先验知识和模型驱动的方法，用于将音频划分为单一类别和混合类别的片段。这种方法旨在应用于声源分离。<br/><br/>2. **结合音乐分数的知识**: 引入了与数据相关联的信息（如音乐乐谱）作为"知识"的一部分，这有助于提高音频分割的准确性和效果。<br/><br/>3. **模型驱动的方法**: 使用诸如隐马尔可夫模型等工具进行音频分割和识别。这种方法不需要事先标注的数据来指导学习过程。<br/><br/>4. **自动生成模型**: 提出的方法能够直接从输入音频及其相关知识源中构建所有必要的模型，而无需依赖预先分割的训练数据。<br/><br/>5. **评价结果**: 在模拟数据上进行评估，证明了基于乐谱引导的学习方法在音乐分割和分离方面取得了很好的结果。通过使用电影轨道数据测试，在没有使用类别信息的情况下，利用声音类别知识实现了比纯粹的数据驱动技术更好的声源分离效果。 |
| [TG-ASR: Translation-Guided Learning with Parallel Gated Cross Attention for Low-Resource Automatic Speech Recognition](https://arxiv.org/abs/2602.22039) | ### 贡献点:<br/><br/>1. **提出TG-ASR框架**：针对低资源自动语音识别（ASR）中的台湾闽南语戏剧语音识别问题，引入了基于翻译指导的TG-ASR框架。该框架利用多语言翻译嵌入来提高在数据稀缺环境下的识别性能。<br/><br/>2. **平行门控跨注意力机制（PGCA）**：TG-ASR的核心是PGCA机制，它通过适应性地整合各种辅助语言的嵌入信息到ASR解码器中，实现了跨语言语义指导的同时保持优化稳定性和减少语言间的干扰。<br/><br/>3. **数据集YT-THDC的贡献**：为持续的研究提供了一个基础，发表了一个包含30小时台湾闽南语戏剧语音的数据集，该数据集配有与之对齐的普通话字幕和人工验证的台湾闽南语转写。这使得研究人员能够进一步探索低资源语言的ASR问题。<br/><br/>4. **增强ASR性能**：通过分析确定最有效的辅助语言，并在实际应用中展示翻译指导学习对于代表性不足的语言（如台湾闽南语）的有效性，实现了14.77%相对减少字符错误率的结果。 |
| [EmoOmni: Bridging Emotional Understanding and Expression in Omni-Modal LLMs](https://arxiv.org/abs/2602.21900) | ###贡献点:<br/><br/>1. **Emotion-aware Multimodal Dialogue Understanding and Expression**:<br/>   该研究提出了一种名为EmoOmni的框架，旨在实现多模态情感对话中准确的理解和表达。这表明了在处理复杂的真实世界场景时，现有的全模式大型语言模型（ Omni-LLMs）通常会遇到浅层理解和与上下文不符的情感反应问题。<br/><br/>2. **Introducing Emotional Chain-of-Thought (E-CoT)**:<br/>   引入了一种名为情感链式思维（E-CoT）的概念。E-CoT通过从精细的多模态感知推理到文本响应，强制执行了逻辑推断过程。这一机制旨在确保在对话中能够准确地处理和表达情感细节。<br/><br/>3. **Explicit Emotion Guidance for the Talker**:<br/>   将E-CoT视为高阶的情感指令，并将其明确纳入框架中以指导谈话者。这一创新使得EmoOmni能够在多模态情感对话任务中实现更准确的情感表达，通过提供具体的情感指导来优化对话流程。<br/><br/>4. **Data Construction and Benchmarking**:<br/>   为了支持EmoOmni的研究和评估，研究团队构建了EmoOmniPipe，用于获取现实世界标注的对话数据，并在此基础上建立了EmoOmniEval基准。这为多模态情感对话任务提供了一套系统性的评估标准。<br/><br/>5. **Competitive Performance**:<br/>   实验结果表明，使用相同的说话者时，EmoOmni-7B在与Qwen3Omni-30B-A3B-Thinking模型相比，在多个评价指标上达到了相当的性能水平。这验证了EmoOmni在多模态情感对话处理上的有效性和竞争力。<br/><br/>###总结：<br/>该论文的主要贡献在于提出了EmoOmni框架，通过引入E-CoT机制和明确的情感指导来改进全模式大型语言模型在理解与表达多模态情感对话方面的表现，并通过构建数据集和基准评估系统，为研究者提供了一套用于评测情感对话性能的工具。实验结果展示了EmoOmni在与先进同行模型竞争时的优秀性能，证明了其在多模态情感处理方面的能力提升。 |
| [MIDI-Informed Singing Accompaniment Generation in a Compositional Song Pipeline](https://arxiv.org/abs/2602.22029) | 贡献点如下：<br/><br/>1. **任务分解**：论文提出了将歌曲生成任务分解为旋律创作、歌唱语音合成和伴奏生成三个子任务的构想。这种分层处理方法旨在提高模型的可编辑性和效率。<br/><br/>2. **MIDI-informed singing accompaniment generation（MIDI-SAG）**：引入了一种基于符号声乐-MIDI进行伴奏生成的方法，以改善歌唱与乐器之间的节奏和和声对齐，从而提升生成歌曲的质量。这是通过条件化伴奏来完成的，条件是基于象征性的声乐-旋律MIDI。<br/><br/>3. **处理间歇性歌声**：论文解决了传统SAG设置中假设持续歌唱的问题，并提出了一种结合明确的节奏/和声控制与音频继续的方法，以确保背景音乐在有声和无声区域保持一致。这是通过保持伴奏的一致性和跨不同的有声和无声部分来实现的。<br/><br/>4. **轻量级模型**：构建了新的、轻量级的组件，这些组件仅需要2500小时的声音数据训练，并且只使用一个RTX 3090单卡即可。这使得整个管道在几个指标上接近最近的开源端到端基线的质量水平。<br/><br/>5. **提供演示和开源**：论文提供了音频示例，并计划通过`https://composerflow.github.io/web/`公开模型源代码，为社区提供实验和进一步开发的机会。<br/><br/>这些贡献点强调了在歌曲生成领域，通过分解任务、引入符号指导的伴奏生成、解决间歇性歌声问题以及构建轻量级模型等方面的技术创新。 |
| [Discrete Optimal Transport and Voice Conversion](https://arxiv.org/abs/2505.04382) | 贡献点如下：<br/><br/>1. **提出kDOT框架**：提出了一个名为kDOT的离散最优运输（OT）框架，用于在预训练语音嵌入空间中进行声音转换。与kNN-VC和SinkVC中的平均策略以及MKL所采用的独立性假设不同，该方法利用离散OT计划的重心投影来构建源发言人与目标发言人嵌入分布之间的传输映射。<br/><br/>2. **全面的消融研究**：通过在运输嵌入的数量上进行彻底的消融研究，并系统分析了源和目标语音持续时间对性能的影响。这表明，通过重心投影应用最优运输可以一致地改善分布的对齐，并且经常比基于平均的方法在WAV、MOS和FAD方面表现得更好。<br/><br/>3. **增强领域适应性**：展示了将离散OT作为后处理步骤应用于欺骗性语音中，能够生成样本，这些样本被最先进的欺骗检测器错误地标记为真实样本。这表明了OT在嵌入空间中的强大领域适配能力，并揭示了对欺骗检测系统的重要安全性含义。<br/><br/>4. **安全影响的示范**：通过将离散最优运输应用于伪造的语音数据，验证了其在欺骗性语音识别和潜在的安全应用方面的实际效果，提供了有关最优运输方法在安全领域（尤其是语音识别系统的鲁棒性）的重要见解。 |
| [Aligning Audio Captions with Human Preferences](https://arxiv.org/abs/2509.14659) | ### 贡献点：<br/><br/>1. **提出了一种基于人类反馈的强化学习框架**（Reinforcement Learning from Human Feedback，RLHF）进行音频字幕生成。该方法旨在解决当前音频字幕依赖成本高昂且可能无法准确反映现实世界中人类偏好的问题。<br/><br/>2. **利用CLAP（Contrastive Language-Audio Pretraining）为基础的奖励模型进行了训练**。通过使用人工标记的二元偏好数据，此奖励模型能够捕捉到微妙的偏好细节。<br/><br/>3. **将训练后的奖励模型集成到强化学习框架中**，用于在不使用真实标注的情况下微调任何基线字幕生成系统。<br/><br/>4. **跨多个数据集进行了广泛的人类评估**。结果显示，与基线模型相比，该方法产生的字幕更符合人类偏好，尤其是在基线模型未能提供正确自然的字幕时更为明显。<br/><br/>5. **在无需真实标注的情况下达到了与监督方法相媲美的性能水平**（与使用实际标注的数据的方法相当），这证明了其有效的人类偏好对齐和在现实世界应用中的可扩展性。 |
| [MDM-ASR: Bridging Accuracy and Efficiency in ASR with Diffusion-Based Non-Autoregressive Decoding](https://arxiv.org/abs/2602.18952) | 该论文的贡献点主要体现在以下几个方面：<br/><br/>1. **提出基于Masked Diffusion Models的非自回归（Non-Autoregressive，NAR）序列到序列（Sequence-to-Sequence）语音识别（ASR）框架**：该框架旨在解决在Transformer ASR中自回归模型（Autoregressive，AR）准确性高但解码速度慢的问题与非自回归模型（NAR）能够并行解码但性能有所下降的矛盾。<br/><br/>2. **结合预训练的语音编码器和条件于声学特征及部分遮蔽转录本的Transformer扩散解码器**：通过这种方式，构建了用于并行预测词元的概率模型。这种方法试图在保留准确性的同时提高解码效率。<br/><br/>3. **引入Iterative Self-Correction Training（迭代自我校正训练）**：这一方法通过让模型暴露于其自身的过程预测中来减少训练和推理之间的不匹配问题，从而提高模型的适应性和性能稳定性。<br/><br/>4. **设计一种Position-Biased Entropy-Bounded Confidence-based Sampler（具有位置偏差的熵限制置信度采样器）**：该采样器在保持序列预测精度的同时，通过引入位置偏斜来进一步优化结果。这一策略有助于提升非自回归模型的性能。<br/><br/>5. **多基准实验**：论文中的实验结果显示，在多个标准上与先前的NAR模型相比，该框架能够持续获得性能提升，并且与强自回归（Autoregressive）基线模型竞争性地保持了并行解码效率。这一发现强调了所提框架在实际应用和理论研究方面的价值。<br/><br/>综上所述，这篇论文贡献了一种基于Masked Diffusion Models的NAR ASR框架，通过创新的设计如迭代自我校正训练和位置偏斜熵限制置信度采样器来改善非自回归模型在语音识别任务中的表现，并且实验证明了其在提高性能的同时保持并行解码效率的优势。 |
| [Sonic4D: Spatial Audio Generation for Immersive 4D Scene Exploration](https://arxiv.org/abs/2506.15759) | ### 贡献点:<br/><br/>1. **解决空间音频缺失问题**：现有的4D生成技术虽然能够合成逼真的动态3D场景渲染，但几乎忽略了与对应4D场景同步生成的空间音频。Sonic4D框架旨在弥补这一不足，提供沉浸式探索4D场景所需的真实空间音频。<br/><br/>2. **分阶段的创新方法**：Sonic4D的方法由三个核心步骤组成：<br/>   - 首先，通过使用预训练专家模型来捕获动态视觉内容和原始音频信息，并生成与之匹配的4D场景及其对应的单声道音频。<br/>   - 其次，通过空间坐标估计策略对声源在4D场景中的位置进行定位和跟踪，将单声道音频转换为具有方位感的空间音频。<br/>   - 最后，基于计算出的声音来源的位置，进一步合成视点和时间戳下变化的物理基础模拟生成的真实空间音频。<br/><br/>3. **增强沉浸式体验**：经过广泛实验验证，Sonic4D方法能够以无需训练的方式产生与合成的4D场景一致、真实可信的空间音频，显著提升了用户体验的沉浸感。提供在线示例供用户探索和评估。 |
| [OmniCustom: Sync Audio-Video Customization Via Joint Audio-Video Generation Model](https://arxiv.org/abs/2602.12304) | 贡献点如下：<br/><br/>1. **任务提出**：论文提出了一个名为“sync audio-video customization”的新任务，该任务的目标是同时定制视频的身份和音频的音色。具体而言，在给定参考图像$I^{r}$和参考音频$A^{r}$的情况下，生成既保持参考图像身份又模仿参考音频音色的视频，且通过用户提供的文本提示可以自由指定语音内容。<br/><br/>2. **框架设计**：论文中提出了一个名为“OmniCustom”的强大模型，该模型基于DiT（双任务训练）体系，用于同步生成遵循参考图像身份、音频音色以及文本提示的视频。OmniCustom能够以零样本方式同时合成参照图像身份、音频音色和文本指示的信息。<br/><br/>3. **关键贡献**：<br/>   - 引入了分开的操作模块来分别实现身份和音频音色控制，通过基模型内部的自注意力层在基础音频-视频生成模型中进行操作。<br/>   - 提出了一个对比学习目标与标准流匹配目标相结合的方法。该方法利用条件于参考输入预测得到的流作为正例，并将不包含参考条件的情况下的流视为负例，以此增强模型保持身份和音色的能力。<br/>   - OmniCustom在构建的大规模、高质量音频-视觉人类数据集上进行了训练。<br/><br/>4. **性能验证**：论文通过广泛的实验结果证明了OmniCustom在生成具有一致的身份和音色准确性的音频-视频内容方面优于现有方法。 |
