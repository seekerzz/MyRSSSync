# GitHub All Languages Daily Trending
---
| Title | Summary |
| --- | --- |
| [bleedline/aimoneyhunter](https://github.com/bleedline/aimoneyhunter) | 这个项目在2023年12月17日登陆GitHub Trending，并在截止日期21日获得了3.7千颗星的赞誉。项目承蒙大家的喜爱，作者也表示希望能对一些人有所帮助。<br/><br/>此外，还提供了一个链接到一个图表，显示了项目的星历史（Star History），这可能是用来跟踪项目在GitHub上的受欢迎程度变化的。 |
| [amplication/amplication](https://github.com/amplication/amplication) | 这段文字是关于一个项目中部分组件的许可证信息。主要包含以下几个要点：<br/><br/>1. 项目的很大一部分组件，其许可证遵循Apache 2.0。<br/><br/>2. 这个许可的例外是位于"ee"（企业版）目录下的组件，它们的许可证基于Amplication Enterprise Edition。<br/><br/>总结来说，这段文字主要是关于项目中部分组件的许可证详细信息。 |
| [danny-avila/LibreChat](https://github.com/danny-avila/LibreChat) | 这个项目之所以能够保持其当前的状态，得益于所有贡献者的努力。项目的GitHub页面上有详细的贡献者图表，展示了不同人的贡献程度。<br/><br/>如果你对这个项目感兴趣，或者想要了解如何参与贡献，可以访问GitHub页面并查看相关的讨论和PR（Pull Request）。<br/><br/>总之，这个项目的成功离不开每一个贡献者的支持。 |
| [1Panel-dev/MaxKB](https://github.com/1Panel-dev/MaxKB) | MaxKB 是一个基于 MaxKB 搭建的智能问答系统，嵌入到 DataEase 产品及在线文档中。它是一个现代化、开源的 Linux 服务器运维管理面板，适用于数据可视化分析工具。<br/><br/>该项目的许可证是 GNU General Public License version 3 (GPLv3)，这意味着软件在分发时遵循特定的条款和条件，包括但不限于知识产权保护和用户许可协议。 |
| [tensorflow/tensorflow](https://github.com/tensorflow/tensorflow) | 这段文字是关于TensorFlow项目的。它首先提供了几个资源链接，包括Coursera、Udacity和Edx的课程搜索，以及TensorFlow的GitHub代码页面。<br/><br/>然后，它明确指出了TensorFlow的许可证信息，即Apache License 2.0，这意味着任何基于这个许可证的修改或分发都是合法的。 |
| [omnivore-app/omnivore](https://github.com/omnivore-app/omnivore) | 本文主要介绍了如何使用Docker和PostgreSQL部署Omnivore应用，包括设置环境变量、安装依赖和服务等步骤。同时提到了Omnivore和其Readability.js扩展的许可证为AGPL-3.0。 |
| [ckeditor/ckeditor5](https://github.com/ckeditor/ckeditor5) | CKEditor 5 是一个模块化、多包的项目，它基于一组创建编辑框架的包。这些包构建了基础工具和辅助工具，用于开发工作流，如构建器和测试运行器。<br/><br/>要贡献代码到项目，请参阅官方的贡献者指南。报告问题或提出功能需求可以在项目的GitHub仓库中进行，详细信息可以查阅文档中的支持部分。<br/><br/>CKEditor 5 的许可证是 GNU General Public License Version 2 或后续版本，详细信息可以在项目的LICENSE.md文件或官方网站的法律条款页面查看。 |
| [practical-tutorials/project-based-learning](https://github.com/practical-tutorials/project-based-learning) | 本文主要介绍了几个编程语言的学习资源，包括但不限于：<br/><br/>1. **Rust**:<br/>   - 学习 Rust的博客文章：`Create a simulation of evolution using neural network and genetic algorithm, and compile the application to WebAssembly`<br/>   <br/>2. **Scala**：<br/>   - 使用Hacking with Swift学习Scala的项目链接：`https://github.com/nicklockwood/RetroRampage` <br/>   <br/>3. **Swift**：<br/>   - 通过Udemy.com平台获取Swift语言的学习资源：`https://udemy.com/topic/swift-programming-language/` <br/><br/>4. **额外资源**：<br/>   - 提供了多个编程学习社区的链接，如CodeCrafters.io等。<br/><br/>这些资源可以帮助初学者快速入门并深入理解所选编程语言。 |
| [gunnarmorling/1brc](https://github.com/gunnarmorling/1brc) | 这篇文章是关于一个名为"1BRC"的挑战，这个挑战的目标是在Java中读取和处理一亿行的数据。文章列出了多个相关的博客、帖子和代码示例，展示了参与者如何解决这一挑战，包括优化性能、编写可读代码等方面的内容。同时，文章也强调了参与者的合作精神和学习乐趣，而非单纯为了赢得比赛。 |
| [pytorch/pytorch](https://github.com/pytorch/pytorch) | 这段文字是关于PyTorch项目团队和贡献者的介绍，以及项目的维护者。它还提到了项目的许可证类型，即BSD-风格。<br/><br/>如果需要更具体的摘要，可以提供更具体的信息需求。 |
| [rustdesk/rustdesk](https://github.com/rustdesk/rustdesk) | 这段文字是关于一个名为"rustdesk"的项目。该项目包含多个文件结构，如"libs/…"用于存放视频编码、配置、网络通信等模块，还有"src/…"用于原始代码和服务。<br/><br/>此外，还提到了一些截图，展示了连接管理器、与Windows PC建立连接的画面，以及进行文件传输和TCP隧道化操作的界面。<br/><br/>总的来说，这段文字是关于一个跨平台桌面应用开发项目，详细描述了项目的结构和部分功能展示。 |
| [hpcaitech/Open-Sora](https://github.com/hpcaitech/Open-Sora) | 这段文字是关于一个名为"Open-刘ora"的项目。该项目是一个开源平台，旨在促进高效视频生产的民主化。项目的作者包括多个贡献者，他们共同致力于这个项目的发展。<br/><br/>此外，这段文字还提到了一个星历史图表（Star History Chart），它展示了项目从创建到当前日期的发展历程。 |
| [tencentmusic/supersonic](https://github.com/tencentmusic/supersonic) | 本文是SuperSonic，一个用于构建聊天BI平台的项目。项目旨在通过集成Chat插件，利用大型语言模型（LLM）来理解和生成自然语言查询，从而实现对数据的智能查询和分析。<br/><br/>简要来说，这个项目提供了一个在线平台，用户可以通过它注册并开始探索，同时也可以通过我们提供的微信联系以获取更多帮助和支持。 |
| [PowerShell/PowerShell](https://github.com/PowerShell/PowerShell) | 这段内容是关于 PowerShell 项目的一些指导和信息。主要包括以下几个方面：<br/><br/>1. **Building the Repository**：提供了如何通过Git克隆源代码的步骤。<br/><br/>2. **Downloading Source Code**：如果不想通过Git，也可以直接下载源代码的链接。<br/><br/>3. **Developing and Contributing**：详细介绍了如何参与开发以及贡献代码的指南。<br/><br/>4. **Support**：指出了获取支持的方式，包括查看支持文档等。<br/><br/>5. **Governance Policy**：强调了项目管理政策，包括代码规范、治理结构等内容。<br/><br/>6. **Security Policy**：对于任何安全问题，提供了查看和遵循的安全策略链接。 |
# 36氪 - 24小时热榜
---
| Title | Summary |
| --- | --- |
| [小鹏智能驾驶换血，北美团队负责人林一树离职｜36氪独家](https://www.36kr.com/p/2827884792039681) | 本文是关于小鹏汽车智驾团队的新闻报道。主要内容包括：<br/><br/>1. 简介：林一树是小鹏汽车在智能驾驶领域的重要老将，曾是LinkedIn技术负责人。<br/><br/>2. 人物离职：近期，林一树已离职，接任其职位的是另一位资深成员Junyao。<br/><br/>3. 新加入团队：同时，小鹏汽车新加入了自动驾驶产品高级总监袁婷婷等人才，这些人员在社交平台活跃，显示了公司内部正在积极招聘和开放岗位。<br/><br/>4. 技术挑战与应对：文章提到，高阶智驾产品渴望商业成熟，但仍在技术剧烈变革阶段。这要求企业具备敏锐的技术嗅觉、持续的研发投入，并通过车辆销量等多维度指标来综合评估企业的经营状况。<br/><br/>5. 人才战略重要性：对于小鹏汽车而言，保持技术长板并构建稳固的人才堡垒是实现长远发展的关键。 |
| [144小时硬控老外，China Travel 火了](https://www.36kr.com/p/2828949410552710) | 这段内容是关于文旅行业创新对入境旅游的影响分析。主要提到通过提升便利度、应用数字技术等手段，吸引外国游客来华旅行，并且这种趋势可能会导致外国游客承包热门景点，而中国游客则涌向冷门目的地的奇特局面。<br/><br/>总结来说，这段内容强调了文旅行业的创新对于推动入境旅游增长的重要性，以及可能带来的市场变化。 |
| [小米SUV谍照曝光，疑似撞脸法拉利500万豪车](https://www.36kr.com/p/2829001709390342) | 小米汽车的二期工厂建设进展、新车型规划以及在车圈影响力持续释放的情况进行了概述。文章提到小米汽车销量目标和已取得的成绩，显示其市场竞争力。同时，小米未来可能涉及混动产品的新布局也被提及。总的来说，小米汽车在车圈的动态展现了其积极进取的形象。 |
| [「中杯」Claude 3.5突然上线，竟比GPT-4o还强，全新Artifacts改写模型交互](https://www.36kr.com/p/2828982547974403) | Claude 3.5 Sonnet是Anthropic推出的一款高级生成式模型。它在性能上有所提升，可能具有足够的吸引力让开发者和企业转向其平台。<br/><br/>值得注意的是，尽管Claude 3.5 Sonnet没有解决大模型的「幻觉」问题，但它可能是同类产品中最好的之一，这有助于保持客户并建立竞争优势。<br/><br/>此外，Anthropic还在工具研发上下功夫，比如实验性的引导AI、集成使其模型能在应用程序中执行操作等，这些举措进一步增强了其在生成式AI领域的竞争力。 |
| [欧洲杯也是阿里速卖通的赛场｜营销观察](https://www.36kr.com/p/2754191971597320) | 这篇新闻咨询摘要主要概述了速卖通在欧洲杯期间的一系列营销活动和供应链举措。<br/><br/>1. **体育赞助与产品销售**：速卖通通过赞助欧洲杯，获得了与赛事联动的广告曝光和品牌提升。同时，它推出与足球相关的商品，如足球袜、运动鞋等，借欧洲杯的热度促进销售。<br/><br/>2. **物流服务升级**：为应对欧洲杯期间订单激增的情况，速卖通联合菜鸟开通了欧洲杯专线，专门处理发往欧洲的跨境商品，确保物流畅通。<br/><br/>3. **半托管服务推广**：速卖通推出半托管模式，允许商家将仓储和物流环节交给平台管理。这一举措不仅帮助商家清理库存、降低成本，还让他们重新获得了定价权，从而吸引更多的商家加入。 |
| [lululemon需要“新”贾玲｜营销观察](https://www.36kr.com/p/2828056012966151) | 这段内容是关于品牌lululemon与代言人贾玲之间的一种营销策略分析。贾玲的粉丝特质与lululemon的目标用户高度重合，这使得贾玲成为品牌代言的理想人选。<br/><br/>此外，文章还提到lululemon正面临市场饱和、竞争激烈等问题，贾玲的出现为品牌的知名度提升和新客户争取提供了一个积极的形象。<br/><br/>总结来说，这段内容强调了贾玲作为lululemon代言人所具有的粉丝价值与品牌传播潜力，并暗示了她在帮助品牌应对市场挑战方面的作用。 |
| [8点1氪丨永辉超市首家胖东来调改店首日销售额达188万；苹果回应AI仅支持两款iPhone；韩国麦当劳宣布暂停销售炸薯条](https://www.36kr.com/p/2828803631548936) | 这段内容看起来像是在介绍一个公司的特点和目标。提到的古希腊神祇可能象征着公司安全或专注的特性，而只有三人的团队规模暗示公司规模较小，专注于某个领域。<br/><br/>目标只有一个，这可能是公司在特定战略方向上的明确承诺，或者是在资源有限的情况下，集中精力解决最核心问题。<br/><br/>如果需要更具体的信息来回答你的问题，可能需要更多的上下文信息。 |
| [阿里字节百度暗战AI硬件，能做成大模型“卖铲子”生意吗？](https://www.36kr.com/p/2828151869807489) | 这篇文章主要讨论了AI技术在可穿戴设备领域的应用和发展。文章提到了一些具体的产品，如搭载ChatGPT的智能眼镜Solos AirGo 3，以及家电、汽车智能座舱等品类。<br/><br/>文章强调了AI硬件产品体验提升的重要性，指出AI技术与用户体验相结合是未来趋势。同时，文中也提到目前市场上AI硬件产品还处于初级阶段，需要进一步探索和实践。<br/><br/>总的来说，这篇文章通过具体案例分析了AI在可穿戴设备中的应用前景以及挑战，为读者提供了关于AI硬件发展趋势的思考。 |
# eess.AS updates on arXiv.org
---
| Title | Summary |
| --- | --- |
| [Automatic Speech Recognition for Biomedical Data in Bengali Language](https://arxiv.org/abs/2406.12931) | 1. 开发了针对孟加拉语医学数据的自动语音识别（ASR）原型系统。<br/><br/>2. 该项目针对孟加拉语医疗领域，如症状、严重程度、疾病等进行了专门定制。<br/><br/>3. 系统设计包含了两种主要方言：孟加拉语和锡兰语。<br/><br/>4. 使用了两种流行的ASR框架进行训练和评估，基于一个包含46小时的全面孟加拉语医学语料库。<br/><br/>5. 目标是创建可部署的医疗领域ASR系统，用于数字健康应用，最终提高非技术用户在医疗保健领域的访问能力。 |
| [Self-Train Before You Transcribe](https://arxiv.org/abs/2406.12937) | 1. 提出在域间转移设置中，使用噪声学生教师训练（Noisy Student Teacher Training）作为测试时的适应性方法。<br/><br/>2. 将这种方法与动态评估语言建模中的类似策略相结合，强调了信息跨句边界转移的能力。<br/><br/>3. 通过实验展示了这种方法的有效性，包括在域内和域外数据集上进行了大量相对增益的测量，最大增益可达32.2%。<br/><br/>4. 最后，对比了通常使用单独适应数据的自我训练设置，发现本方法在某些情况下表现出更大的优势。 |
| [Instruction Data Generation and Unsupervised Adaptation for Speech Language Models](https://arxiv.org/abs/2406.12946) | 1. 该论文提出三种生成合成样本的方法，用于训练和评估能够处理文本和语音输入的多模态大型语言模型。<br/><br/>2. 主要针对缺乏包含两种模态数据的样本问题，强调了通过合成数据生成来提升这类系统性能的关键策略。<br/><br/>3. 研究过程利用大型语言模型生成文本部分，同时使用文本到语音系统生成语音部分。<br/><br/>4. 实验结果表明这些方法有助于实现对文本和语音的整合理解，并且指出了利用未标注语音数据生成合成样本的可能性，这有助于将这些模型扩展到更多的语言。 |
| [Articulatory Encodec: Vocal Tract Kinematics as a Codec for Speech](https://arxiv.org/abs/2406.12998) | 1. 提出新的神经编码-解码框架，用于语音的articulatory encodec。<br/><br/>2. 构建了两个模型：articulatory analysis model 用于从语音音频中推断 articulatory features，以及 articulatory synthesis model 用于根据 articulatory features 合成语音音频。<br/><br/>3. 关注articulatory features，它们是声门、舌头等喉部和口腔结构的运动轨迹，这些特征直观可解释且可控。<br/><br/>4. 训练模型时使用大规模的语音数据，以实现对未知说话人的完全可理解高质量合成。<br/><br/>5. 通过联合训练 speaker identity encoder 和 articulatory synthesizer，确保了每个说话者的独特声音纹理。 <br/><br/>6. 提出的框架有望作为强大的编码系统，用于处理和生成人类语音这一复杂现象。 |
| [Audio Fingerprinting with Holographic Reduced Representations](https://arxiv.org/abs/2406.13139) | 1. 提出了一种基于 holographic reduced representation (HRR)的音频指纹模型，该模型减少了存储的指纹数量。<br/><br/>2. 与传统的神经音频指纹技术相比，这种方法不需要为每个音频文件存储大量的高精度指纹，以达到高的准确性和时间分辨率。<br/><br/>3. 利用 HRR 的特性，通过圆周卷积和求和将多个指纹聚合成一个复合指纹，这减少了存储的指纹数量，同时保持了与原始指纹相同的空间维度。<br/><br/>4. 提出的搜索方法能够高效地找到包含查询指纹在内的复合指纹。<br/><br/>5. 通过 HRR 的逆操作，可以恢复复合指纹中各个部分的相对位置，从而保留原始的时间分辨率。 <br/><br/>6. 实验结果表明，这种方法在适度降低准确率的情况下，成功减少了存储的指纹数量，并保持了时间分辨率，优于简单基于减法和求和的聚合方法。 |
| [CEC: A Noisy Label Detection Method for Speaker Recognition](https://arxiv.org/abs/2406.13268) | 1. 提出基于新统计指标的噪声标签检测方法：连续不一致计数（CIC）和总不一致计数（TIC），这些通过交叉epoch计数（CEC）计算。<br/><br/>2. 分类样本：根据预测结果，将样本分为三类：不一致样本、硬样本和易样本。<br/><br/>3. 训练过程中的模型参数更新策略：针对硬样本，逐渐增加其难度以更新模型参数，防止噪声标签被过度拟合。<br/><br/>4. 与对比性方案的比较优势：除了在说话验证任务中表现出色外，还擅长噪声标签检测，这使得它在处理数据质量问题时具有更强的能力。 |
| [Pushing the Limit of Sound Event Detection with Multi-Dilated Frequency Dynamic Convolution](https://arxiv.org/abs/2406.13312) | 1. 提出部分频率动态卷积（PFD conv），通过将静态常规2D卷积分支输出和动态FDY卷积分支输出相结合，以最小化模型尺寸增加的同时保持性能。<br/><br/>2. 推广多尺度膨胀频率动态卷积（MDFD conv），它整合了多个具有不同扩张率的频率动态卷积（DFD conv）分支，每个分支都有不同的扩张大小集合，并且还包括一个静态分支。这种方法在单个卷积模块内实现了3.2%的多项乐器声音检测分数（PSDS）提升，超过了FDY卷积。<br/><br/>3. 进行了广泛的 ablative研究，以进一步增强对FDY卷积变体的理解和实用性。 |
| [Medical Spoken Named Entity Recognition](https://arxiv.org/abs/2406.13337) | 1. 提供了VietMed-NER，这是第一个在医疗领域内的 spoken Named Entity Recognition (NER) 数据集。<br/><br/>2. 该数据集是世界上最大规模的 spoken NER 数据集之一，它拥有18种不同的实体类型。<br/><br/>3. 提供了基于多种先进预训练模型（如XLM-R）的基准结果，这些模型包括编码型和序列到序列型。<br/><br/>4. 发现预训练的多语言模型在参考文本和ASR输出上都优于所有单语言模型。<br/><br/>5. 结论是，在NER任务中，编码器通常比序列到序列模型表现更好。此外，该数据集具有跨语言适用性。 |
| [Explainable by-design Audio Segmentation through Non-Negative Matrix Factorization and Probing](https://arxiv.org/abs/2406.13385) | 1. 提出基于非负矩阵因子化（NMF）的解释性音频分割模型，设计为可解释的表示。<br/><br/>2. 该模型在音频分割任务上达到良好的性能，表明其有效性和实用性。<br/><br/>3. 对于提取自非负矩阵的潜在表示进行了深入分析，这有助于理解模型的工作原理和决策过程。<br/><br/>4. 提出的新方法为评估可解释性表示的质量提供了新的视角，关注于"好"属性。 |
| [Online Domain-Incremental Learning Approach to Classify Acoustic Scenes in All Locations](https://arxiv.org/abs/2406.13386) | 1. 提出在线领域增量学习方法，用于音频场景分类。<br/>2. 方法针对不同位置的序列数据进行训练，但容易导致对先前知识的遗忘。<br/>3. 研究者提出只通过少量样本修正批标准化层统计的方法，以在新位置学习场景分类而不会忘记之前学习的知识。<br/>4. 实验使用来自11个不同地点的音频场景数据，并将初始任务设置为6个地点的数据，剩余5个增量任务分别代表从不同位置学到的新场景。<br/>5. 研究结果表明，提出的在线领域增量学习方法优于基于微调的方法，并在序列中最后一个任务的学习后平均准确率达到48.8%，这证明了该方法的有效性和实用性。 |
| [Diffusion-based Generative Modeling with Discriminative Guidance for Streamable Speech Enhancement](https://arxiv.org/abs/2406.13471) | 1. 提出使用判别模型的歧视性分数作为RDP(反向扩散过程)的第一步，以减少计算量。<br/><br/>2. 这种方法允许性能改进，通过增加使用判别分数的步骤数，可以调整生成和判别能力之间的平衡。<br/><br/>3. 针对流式SE系统，提出一种新的、具有算法延迟50ms的时域生成模型。该模型在性能上与离线模型相比没有显著下降。 |
| [CONMOD: Controllable Neural Frame-based Modulation Effects](https://arxiv.org/abs/2406.13935) | 1. 提出Controllable Neural Frame-based Modulation Effects(CONMOD)模型，这是一个单一的黑盒模型，能够以帧为单位模仿各种LFO驱动的效果。<br/><br/>2. CONMOD模型具有控制能力，可以调节LFO频率和反馈参数，从而对输出进行操纵。<br/><br/>3. 模型还具备学习能力，能够学习两个不同 Phaser效果的连续嵌入空间，这使得在两个效果之间进行转换成为可能。<br/><br/>4. 与之前的工作相比，CONMOD模型不仅性能优越，而且具有控制性和普适性，为现代LFO驱动音频效果的创新提供了机会。 |
| [Decoding Vocal Articulations from Acoustic Latent Representations](https://arxiv.org/abs/2406.14379) | 1. 提出了一种新的神经编码系统，用于实现声学到 articulatory 反转。<br/><br/>2. 利用 Pink Trombone 语音合成器，它能揭示出包括舌头位置和声道配置在内的articulatory参数。<br/><br/>3. 系统设计旨在识别产生特定声学特征的 articulatory 特征。<br/><br/>4. 利用了两种主要方法来生成必要的嵌入。第一种是自监督的变分自动编码器（VAE），从头开始训练以重建输入信号。<br/><br/>5. 第二种方法利用了预先训练好的模型：EnCodec 和 Wav2Vec，它们简化了编码过程的学习，使得重点放在训练投影网络上。<br/><br/>6. 项目的主要目标之一是验证这些神经架构能够有效地封装声学和 articulatory 特征。<br/><br/>7. 提供的资源包括数据集、代码以及详细的研究结果，以支持未来在这个领域内的研究。 |
| [Global-Local Convolution with Spiking Neural Networks for Energy-efficient Keyword Spotting](https://arxiv.org/abs/2406.13179) | 1. 利用神经网络的能源效率，提出一个端到端的轻量级关键词识别模型。<br/><br/>2. 模型包含两个创新模块：(a) 全局-局部脉冲卷积(GLSC)模块，用于实现更稀疏、能量效率更高的语音特征提取；(b) 压缩瓶颈-PLIF模块，进一步处理GLSC模块产生的信号，以达到更高精度但参数较少的目标。<br/><br/>3. 在Google Speech Commands Dataset (V1和V2)上进行了广泛的实验。结果表明，该方法在参数较少的SNN基关键词识别模型中实现了竞争力的表现。 |
| [Enhancing Automated Audio Captioning via Large Language Models with Optimized Audio Encoding](https://arxiv.org/abs/2406.13275) | 1) 使用预训练的音频编码器通过一致的ensemble distillation（CED）改进了声令牌的效果，通过Q-Former跨模态连接到LLM，并压缩声令牌。<br/><br/>2) 探究使用Llama 2，具有7B参数的解码器的优点。<br/><br/>3) 另一个预训练的LLM用于纠正由于训练数据不足和标注歧义导致的文本错误。音频编码器和文本解码器都通过-Lora Base进行优化。实验表明这些改进都是有效的，其方法在DCASE 2023 Task 6A的比赛中获得了33.0 SPIDEr-FL分数，超过了获胜者。 |
| [SD-Eval: A Benchmark Dataset for Spoken Dialogue Understanding Beyond Words](https://arxiv.org/abs/2406.13340) | 1. 提出问题：论文指出当前Chat-导向大型语言模型在处理多模态输入，包括语音时，对生成适当响应的不足。<br/><br/>2. 解决方案：为解决这一问题，作者提出SD-Eval，一个针对多维度口语对话理解与生成评估的基准数据集。<br/><br/>3. 数据特点：SD-Eval专注于副语言信息和环境信息，并包含7,303个会话，总计8.76小时的语音数据。<br/><br/>4. 评估方法：为了评估SD-Eval，作者实施了三种不同的模型，并构建了一个遵循SD-Eval类似过程的训练集。<br/><br/>5. 实验结果：实验结果显示，条件基于副语言和环境信息的模型在客观和主观指标上都超过了不考虑这些因素的模型。此外，LLM-基于指标与人类评估的相关性也高于传统指标。 |
| [Transferable speech-to-text large language model alignment module](https://arxiv.org/abs/2406.13357) | 1. 利用大型语言模型（LLMs）和语音基础模型，简化了先进的多模态工作，如口语翻译（ST）和问答系统（SQA）。<br/><br/>2. 通过Whisper编码器和预训练的Yi-6B模型，实现了模态对齐，使用单层模块即可实现。<br/><br/>3. 实验表明，使用数百小时的多任务语音文本语料库，可以有效地达到模态对齐的目标。<br/><br/>4. 在推理阶段，进一步将Yi-6B替换为与人类偏好一致的版本（Yi-6B-Chat），发现模态对齐的能力同样适用。<br/><br/>5. 通过奇异值分解（SVD）揭示的模态对齐子空间，暗示了线性模态对齐子空间是稀疏的。这为结合其他特征如声纹或视频来扩展模态提供了可能性。 |
| [Straight Through Gumbel Softmax Estimator based Bimodal Neural Architecture Search for Audio-Visual Deepfake Detection](https://arxiv.org/abs/2406.13384) | 1. 提出STGS( Straight-through Gumbel-Softmax )框架，用于搜索多模态融合模型架构。<br/><br/>2. 利用两层搜索策略优化网络结构、参数和性能。<br/><br/>3. 通过关键特征的高效识别和多源信息的权重融合，实现信息的有效整合。<br/><br/>4. 提出基于温度和采样时间变化的参数调整方法，以优化分类性能。<br/><br/>5. 在FakeAVCeleb和SWAN-DF等数据集上进行实验验证，结果显示STGS框架下模型的AUC值达到94.4\%，且参数较少。 |
| [Children's Speech Recognition through Discrete Token Enhancement](https://arxiv.org/abs/2406.13431) | 1. 研究儿童语音识别系统的输入，尝试将离散的语音令牌整合进来，而不显著降低ASR性能。<br/><br/>2. 探索了单视图和多视图策略来创建这些离散标签，这可能影响到生成的令牌质量和一致性。<br/><br/>3. 对模型进行了泛化能力测试，使用未见过的领域和母语数据集，以评估模型在不同情况下的表现。<br/><br/>4. 结果表明，儿童语音识别系统的离散令牌ASR性能接近原系统，但参数量减少了大约83%。 |
| [Automatic Voice Classification Of Autistic Subjects](https://arxiv.org/abs/2406.13470) | 1. 提出自动语音分类算法，用于特征化区分自闭症的音素，支持传统诊断。<br/><br/>2. 研究针对自闭症和非自闭症人群收集的录音演讲数据集，用于评估算法性能。<br/><br/>3. 通过实验验证算法在区分自闭症与非自闭症的语音特征方面的能力。 |
| [ManWav: The First Manchu ASR Model](https://arxiv.org/abs/2406.13502) | 1. 研究针对高资源和极度低资源语言的自动语音识别（ASR）研究差距，特别关注了曼珠语这一濒临灭绝的语言。<br/><br/>2. 曼珠语是边缘化语言群体在获取先进科技时面临挑战的一个典型例子。<br/><br/>3. 作者提出了一项开创性工作，推出了第一个曼珠语ASR模型ManWav，利用了Wav2Vec2-XLSR-53这一先进的模型。<br/><br/>4. 研究结果表明，经过他们增补数据训练的Wav2Vec2-XLSR-53模型，在对比原始数据训练的模型时，显示了CER下降0.02和WER下降0.13的好成绩。 |
| [Automated Bioacoustic Monitoring for South African Bird Species on Unlabeled Data](https://arxiv.org/abs/2406.13579) | 1. 开发了自动提取特定鸟类物种标签数据的框架，从现有平台获取数据。<br/><br/>2. 将提取的标签数据嵌入到包括环境声音和噪音在内的录音中。<br/><br/>3. 利用这些带有标签的数据训练卷积循环神经网络（CRNN）模型。<br/><br/>4. 评估模型在未经处理的真实世界数据上的性能，这些数据是在城市KwaZulu-Natal栖息地中录制的。<br/><br/>5. 提供了一种方法，使得被动声学监测（PAM）可以轻松适应到其他物种和不同生态环境中，为未来的保护项目提供便利。 |
| [Joint vs Sequential Speaker-Role Detection and Automatic Speech Recognition for Air-traffic Control](https://arxiv.org/abs/2406.13842) | 1. 提出了一种基于Transformer的联合自动语音识别（ASR）-角色检测（SRD）系统，该系统能够同时解决这两个任务。<br/><br/>2. 与传统的分步方法相比，这个联合系统具有更高的效率和一体化处理能力。<br/><br/>3. 对多个空中交通管制（ATC）数据集进行了实验对比，研究了联合系统在何时能超越传统方法。<br/><br/>4. 进一步探讨了音频和词汇差异如何影响所有架构，并提出了针对联合系统的优化策略。 |
| [Improved Remixing Process for Domain Adaptation-Based Speech Enhancement by Mitigating Data Imbalance in Signal-to-Noise Ratio](https://arxiv.org/abs/2406.13982) | 1. 提供了基于CHiME-7 UDASE任务数据的实证证据，表明伪数据的信号-to-noise ratio(SNR)对模型性能有显著影响。<br/><br/>2. 强调了在DASE（域适应性语音增强）中平衡SNR的重要性，这有助于提高对于数据不平衡情况下（如某些声学属性欠丰富）的处理能力。<br/><br/>3. 提出采用 curriculum learning 的策略，以涵盖广泛范围的 SNRs，从而提升对未充分代表数据的性能。 |
| [Seamless Language Expansion: Enhancing Multilingual Mastery in Self-Supervised Models](https://arxiv.org/abs/2406.14092) | 1. 提出一种将LoRA整合到已存在的SSL模型中的适应方法，以扩展对新语言的支持。<br/><br/>2. 开发了保护策略，包括数据组合和重新聚类，旨在保留对现有语言的能力。<br/><br/>3. 应用于mHuBERT模型，并在语音再合成任务中进行了有效性研究。<br/><br/>4. 实验结果表明，提出的适应方法使mHuBERT能够以约1.6的 MOS值增加的方式应用到新语言（普通话），并且WER的相对价值减少了高达61.72%。同时，保护策略确保了对现有和新语言性能的完整性。 |
| [A Multi-Stream Fusion Approach with One-Class Learning for Audio-Visual Deepfake Detection](https://arxiv.org/abs/2406.14176) | 1. 提出音频-视觉深度伪造检测的挑战，需要开发具有强大泛化能力的模型。<br/><br/>2. 强调模型需要具备解释性，能够明确指出视频中哪些部分被认为是虚假的。<br/><br/>3. 通过提出多流融合策略，并结合一类学习作为表示层的正则化技术，研究了音频-视觉深度伪造检测的泛化问题。<br/><br/>4. 实验结果表明，提出的框架在检测未见过的攻击时，平均性能比基线模型提高了7.31%。同时，该框架提供了可解释性，有助于理解模型识别虚假的依据。 |
| [SimulSeamless: FBK at IWSLT 2024 Simultaneous Speech Translation](https://arxiv.org/abs/2406.14177) | 1. 描述了FBK参与IWSLT 2024年Simultaneous Translation Evaluation Campaign的情况。<br/><br/>2. 提出了他们的投稿方案SimulSeamless，通过结合AlignAtt和SeamlessM4T的中等配置实现。<br/><br/>3. 强调SeamlessM4T模型是“现成”的，并且能够通过同步推理启用。<br/><br/>4. 参与了所有共享任务语言（包括英语到德语、日语、中文以及捷克语到英语）。<br/><br/>5. 他们的SimulSeamless方案在与去年提交的项目相比时，取得了可接受或甚至更好的结果。并且该模型已发布在GitHub链接中。 |
| [DASB -- Discrete Audio and Speech Benchmark](https://arxiv.org/abs/2406.14294) | 1. 提出问题：针对音频和语言处理中理想音频令牌的缺失，以及现有研究在评估设置上的不一致性，提出了研究需求。<br/><br/>2. 创新方法：发布Discrete Audio and Speech Benchmark (DASB)，这是一个综合排行榜，用于跨领域比较音频令牌在多种任务上的性能。这是对音频令牌评估标准创新的一种方式。<br/><br/>3. 结果分析：结果显示，平均而言，语义令牌优于压缩令牌，在大多数判别性和生成性任务上表现更好。然而，语义令牌与标准连续表示之间的性能差距仍然显著，这表明该领域的研究仍有提升空间。 |
| [LARP: Language Audio Relational Pre-training for Cold-Start Playlist Continuation](https://arxiv.org/abs/2406.14333) | 1. 提出LARP，一个多模态冷启动播放列表延续模型。<br/>2. LARP是一个基于对比学习的三阶段框架，整合了多模态和关系信号到学习表示中。<br/>3. 模型架构使用任务特定的抽象层次：语言-音频对内对比损失、跨轨道对比损失、以及轨道-播放列表对比损失。<br/>4. 实验结果在两个公开可用数据集上验证了LARP相对于单模态和多模态模型的有效性，尤其是在冷启动设置下的播放列表延续任务中。 |
| [A Review of Common Online Speaker Diarization Methods](https://arxiv.org/abs/2406.14464) | 1. 提供在线说话者分段的概述，包括其历史背景。<br/>2. 探讨在线说话者分段的分类体系和相关数据集，用于训练和评估系统。<br/>3. 深入讨论在线说话者分段的方法和技术，以及现有的系统和应用。<br/>4. 结论部分提出在线说话者分段领域面临的挑战，并对未来研究方向进行展望。 |
| [Proceedings of The second international workshop on eXplainable AI for the Arts (XAIxArts)](https://arxiv.org/abs/2406.14485) | 1. 举办了第二次国际研讨会，主题为"解释性人工智能在艺术中的应用(XAIxArts)"。<br/><br/>2. 这个研讨会聚集了来自人机交互（HCI）、互动设计、人工智能、解释性人工智能（XAI）以及数字艺术领域的研究人员。<br/><br/>3. 研讨会的目标是探索XAI在艺术领域的作用和可能的应用。<br/><br/>4. 该研讨会是在第16届ACM创新与认知会议（C&amp;C 2024）上举办的，地点在美国芝加哥。 |
| [Disentangled Representation Learning for Environment-agnostic Speaker Recognition](https://arxiv.org/abs/2406.14559) | 1. 提出基于特征解耦的框架，用于学习健康新闻播报员的说话嵌入，这些嵌入在环境变化时具有鲁棒性。<br/><br/>2. 该框架利用自编码器作为解耦工具，将输入的说话嵌入分解为与说话者相关的信息和剩余的无关细节。<br/><br/>3. 利用一组客观函数来确保自编码器的代码表示——用于作为优化后的嵌入——只包含说话者的特征，而不会包含其他无关信息。<br/><br/>4. 该框架具有通用性，能够与任何现有的说话嵌入提取器兼容，无需对结构进行修改或适应。<br/><br/>5. 研究通过将该框架集成到两个流行的嵌入提取器中，并在各种基准上进行实验，验证了其有效性。实验结果显示，框架的使用可以显著提高嵌入的性能，最高提升达16%。 |
| [SuperM2M: Supervised and Mixture-to-Mixture Co-Learning for Speech Enhancement and Robust ASR](https://arxiv.org/abs/2403.10271) | 1. 提出直接在真实目标域数据上训练神经语音增强模型的设想。<br/>2. 推广并适应混合到混合（M2M）训练，最初设计用于说话者分离，应用于语音增强，通过模拟多源噪声信号作为单一、综合的噪声源来建模。<br/>3. 提出联合学习算法，即在M2M和监督方法之间进行协同训练。DNN通过交替接收真实近距离对话和远距离混合样本以及模拟混合和清晰语音样本的迷你批次，同时损失函数分别计算为(a)真实近距离对话混合样本的重建损失和(b)模拟数据上的增强损失。<br/>4. 实验结果证明这种联合学习方式能够使DNN从真实和模拟数据中学习，从而实现对真实数据更好的泛化能力。这种方法被命名为SuperM2M（监督与混合到混合协同学习）。 |
| [Differentiable All-pole Filters for Time-varying Audio Systems](https://arxiv.org/abs/2404.07970) | 1. 提供了一种解决无限 impulse response (IIR)滤波器在自动微分框架下的训练限制的方法。<br/><br/>2. 通过将时间变化的全极点滤波器重新表达，使得滤波器的梯度可以通过自身反向传播。<br/><br/>3. 这种实现方法可以应用于包含具有极点的滤波器的音频系统中，以进行高效的梯度计算。<br/><br/>4. 实验展示了这种方法在 Phaser、动态音效合成器和前馈压缩器等真实世界动态音频系统的建模中的训练效率和表达能力。 |
| [UrBAN: Urban Beehive Acoustics and PheNotyping Dataset](https://arxiv.org/abs/2406.03657) | 1. 提供了一项多模态研究，基于加拿大蒙特利尔的一个蜂巢收集数据。<br/><br/>2. 数据集涵盖了2021年至2022年的多个时间段，包括音频（超过2000小时的高质量原始音频）、传感器数据（如温度和湿度）以及与蜂群健康相关的指标记录。<br/><br/>3. 详细描述了数据收集过程、传感器数据内容以及数据结构，为后续研究提供了清晰指南。<br/><br/>4. 展示了如何利用这个数据集进行实践应用，通过提取音频中的特征来预测蜂群的数量，这展示了数据的潜力和价值。 |
| [Differentiable Time-Varying Linear Prediction in the Context of End-to-End Analysis-by-Synthesis](https://arxiv.org/abs/2406.05128) | 1. 提出训练音频合成中线性预测(LP)操作的端到端方法，但现有方法由于其递归形式导致训练速度慢。<br/><br/>2. 识别到帧级近似作为加速手段的局限性，即这种方法难以在测试时条件（如样本级LP计算）下泛化。<br/><br/>3. 提出关键解决方案：开发一种高效的可微分、样本级线性预测(LP)操作，用于端到端训练，以消除上述障碍。<br/><br/>4. 通过将GOLF语音合成器中有效的时间不变LP实现方法推广至时间变化情况，展示了这种方法的通用性和适应性。<br/><br/>5. 结合经典的源-滤波模型，实验结果表明改进后的GOLF在学习LP系数和重建声音方面优于帧级近似版本，且在主观听觉测试中，其合成输出的质量评分更高于当时的主流不同iable WORLD vocoder。 |
| [WenetSpeech4TTS: A 12,800-hour Mandarin TTS Corpus for Large Speech Generation Model Benchmark](https://arxiv.org/abs/2406.05763) | 1. 提供了WenetSpeech4TTS，这是一个基于开源的WenetSpeech数据集的多领域普通话语料库。<br/><br/>2. 专门针对文本到语音任务进行了优化，通过调整段落边界、提升音频质量以及消除每个段内的说话者混合，对原始的WenetSpeech数据进行了精细化处理。<br/><br/>3. 按照更准确的转录过程和基于质量的数据过滤流程，获得了包含12,800小时音频-文本配对数据的WenetSpeech4TTS语料库。<br/><br/>4. 创造了不同大小的子集，按照段落质量评分进行分类，以便于TTS模型的训练和微调。<br/><br/>5. 使用VALL-.E和NaturalSpeech 2系统在这些子集中进行了训练和微调，以验证WenetSpeech4TTS语料库的实际可用性，并为后续的TTS系统性能比较设立了基准。 |
| [HuBERT-EE: Early Exiting HuBERT for Efficient Speech Recognition](https://arxiv.org/abs/2204.06328) | 1. 提出HuBERT-EE，一种针对自动语音识别(ASR)的早期退出方案。<br/><br/>2. 在HuBERT-EE中，通过在中间层添加多个早期退出分支来实现模型的动态停止。<br/><br/>3. 研究了适当的早期退出准则，包括如何判断某个分支的预测足够自信。<br/><br/>4. 探讨了精细调优策略，以确保在执行早期退出时，模型性能和延迟之间的平衡。<br/><br/>5. 在LibriSpeech数据集上进行实验验证，结果显示HuBERT-EE能够在保持HuBERT性能的同时显著加速推理速度。 |
| [Can Large Language Models Aid in Annotating Speech Emotional Data? Uncovering New Frontiers](https://arxiv.org/abs/2307.06090) | 1. 该论文探讨了大型语言模型（LLMs）在标注丰富语音数据方面的潜力，以提升情感识别的最新技术水平。<br/><br/>2. 论文通过使用公开可用的语音情绪分类数据集进行评估，跨越了多种设置，实验性地展示了LLMs在语音情绪数据标注中的作用。<br/><br/>3. 作者还探讨了单次射击（one-shot）和少量射击（few-shot）场景下的性能差异，揭示了情感识别结果的可变性。<br/><br/>4. 最后，通过数据增强技术，将ChatGPT标注的样本融入现有的数据集中，作者实现了更好的结果。这项工作为语音情绪分类开辟了新的研究领域，并预示着未来LLMs在该领域的广泛应用。 |
| [COVID-19 Detection System: A Comparative Analysis of System Performance Based on Acoustic Features of Cough Audio Signals](https://arxiv.org/abs/2309.04505) | 1. 提出研究目标：探索用于COVID-19咳嗽音频信号检测的多种声学特征，以提升机器学习(ML)模型性能。<br/><br/>2. 研究内容：对比分析三种特征提取技术（MFCC、Chroma和Spectral Contrast）在支持向量机(SVM)和多层感知器(MLP)这两种ML算法上的效果。<br/><br/>3. 提出系统方案：基于研究结果，提出一种名为CovCepNet的高效COVID-19咳嗽音频信号检测系统。<br/><br/>4. 实验与性能评估：通过实验验证系统的有效性，并在两个数据集（COUGHVID和Virufy）上对COVID-19检测性能进行评估，如AUC值。 |
| [CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models](https://arxiv.org/abs/2310.08753) | 1. 提出CompA，一个包含两个专家标注的基准集合，用于评估音频语言模型在音频事件顺序理解（compositional reasoning）上的能力。<br/><br/>2. CompA-Order任务旨在衡量模型对音频中事件顺序的理解，即理解事件是否按照特定的顺序发生。<br/><br/>3. CompA-Attribute任务则针对模型如何绑定音频中的事件属性进行评估。<br/><br/>4. 通过提出改进对比训练的方法，以及设计模块化的对比损失，论文展示了如何通过这些方法来提升模型在复杂音频语义理解上的能力。 |
| [Multimodal Sentiment Analysis with Missing Modality: A Knowledge-Transfer Approach](https://arxiv.org/abs/2401.10747) | 1. 提出一种新型的知识转移网络，用于在不同模态之间进行翻译，以重构缺失的音频模态。<br/><br/>2. 开发了一种跨模态注意力机制，旨在保留重建和观察模态中最大信息量，以支持情感预测。<br/><br/>3. 在三个公开可用的数据集上进行了大规模实验，结果显示该方法显著优于基线，并且在多模态完全监督的情况下达到了相当的性能。 |
| [Do End-to-End Neural Diarization Attractors Need to Encode Speaker Characteristic Information?](https://arxiv.org/abs/2402.19325) | 1. 应用信息瓶颈方法对神经端到端的说话人分割（EEND-EDA）进行研究。<br/><br/>2. 通过使用吸引子，即对话中每个发言者的向量表示，来探讨模型所需的关键信息。<br/><br/>3. 分析发现，吸引子并不一定包含特定的发言者特性信息。相反，给吸引子更多的自由度，让它们编码一些额外的信息（可能是特定于发言者的），可以带来微小但一致的说话人分割性能提升。<br/><br/>4. 尽管EEND系统在架构上存在差异，但吸引子和帧嵌入的概念是大多数系统共有的，并非特属于EEND-EDA。<br/><br/>5. 作者认为，这项工作的主要结论可能适用于其他EEND变体。因此，希望这篇论文能为指导社区做出更明智的设计决策提供有价值的观点。 |
| [Multichannel Long-Term Streaming Neural Speech Enhancement for Static and Moving Speakers](https://arxiv.org/abs/2403.07675) | 1. 该研究扩展了之前提出的离线SpatialNet，用于长期流媒体多通道语音增强，适用于静态和移动演讲者场景。<br/><br/>2. SpatialNet利用空间信息，如语音的三维方向（空间/指向性），来区分目标语音和干扰，表现出卓越性能。<br/><br/>3. 研究的核心是窄带自注意力模块，用于学习空间向量的时间动态特性。<br/><br/>4. 为了实现长期流媒体语音增强，研究提出将离线自注意力网络替换为在线网络，这些网络具有线性推理复杂度，且能学习到长期信息。<br/><br/>5. 三种变体基于不同的策略开发：(i) 带掩码的自注意力；(ii) Retention，一种具有线性推理复杂度的自注意力变体；(iii) Mamba，一个结构化的状态空间基于的RNN-like网络。<br/><br/>6. 研究还探讨了不同网络的长度外推能力，即在训练信号远大于测试信号的情况下进行测试，并提出了一种结合短期信号训练和长期信号微调的方法，大大提高了网络在有限训练时间内的长度外推能力。 |
| [Listenable Maps for Audio Classifiers](https://arxiv.org/abs/2403.13086) | 1. 提出Listenable Maps for Audio Classifiers (L-MAC)这一后处理解释方法，用于音频分类器生成可信且可听的解释。<br/><br/>2. L-MAC通过在预训练分类器之上添加一个解码器来工作。解码器生成二进制掩模，强调输入音频中的相关部分。<br/><br/>3. 训练解码器时使用了一个损失函数，该函数最大化了分类器决策对掩模中音频部分的信心，同时最小化模型输出对于掩模区域的概率。<br/><br/>4. 通过量化评估，在内域和跨域数据上，L-MAC证明其能够比几种基于梯度和掩模的方法生成更可信的解释。<br/><br/>5. 进行用户研究进一步证实，平均而言，用户更倾向于接受由该技术生成的解释。 |
| [MUSE: Flexible Voiceprint Receptive Fields and Multi-Path Fusion Enhanced Taylor Transformer for U-Net-based Speech Enhancement](https://arxiv.org/abs/2406.04589) | 1. 提出基于Multi-Path Enhanced Taylor(MET) Transformer的U-Net结构，构建名为MUSE的轻量级语音增强网络。<br/><br/>2. 设计了新型的MET Transformer块，它结合了Deformable Embedding（DE）以实现灵活的接收领域，适用于语音识别中的声纹处理。<br/><br/>3. 独特地设计了MET Transformer，融合了通道注意力和空间注意力分支，促进通道信息交流并解决Taylor-Transformer框架中空间注意力不足的问题。<br/><br/>4. 通过在VoiceBank+DEMAND数据集上进行广泛的实验，证明MUSE在性能方面具有竞争力，同时显著降低了训练和部署成本，参数量仅为0.51M。 |
| [TokSing: Singing Voice Synthesis based on Discrete Tokens](https://arxiv.org/abs/2406.08416) | 1. 介绍TokSing，一个基于离散 tokens 的歌唱语音合成系统。<br/><br/>2. 系统配备了一个灵活的 token 形成器，能够实现不同 token 混合的能力。<br/><br/>3. 提到在使用离散 tokens 进行 SVS 时，面临更高的旋律表达要求带来的挑战。<br/><br/>4. 描述了为解决这一问题，TokSing 中引入了专门设计的旋律增强策略，并将其融入音乐编码器中。<br/><br/>5. 实验结果表明，TokSing 在性能上超越了基于 Mel spectrograms 的基线，同时在中间表示空间成本和收敛速度方面具有优势。 |
| [SingOMD: Singing Oriented Multi-resolution Discrete Representation Construction from Speech Models](https://arxiv.org/abs/2406.08905) | 1. 提出SingOMD，一种新的方法从语音SSL模型中提取面向歌唱的多分辨率离散表示。<br/><br/>2. 通过复原合成任务适应语音SSL模型的特征，并基于重采样引入多分辨率模块，以更好地服务于歌唱生成。<br/><br/>3. 这些适应后的多分辨率特征被进一步离散化，通过聚类实现。<br/><br/>4. 实验结果证明了这些针对歌唱生成设计的离散表示在歌唱合成器和歌唱语音合成中的有效性、鲁棒性和效率。 |
| [Exploring Multilingual Unseen Speaker Emotion Recognition: Leveraging Co-Attention Cues in Multitask Learning](https://arxiv.org/abs/2406.08931) | 1. 提出解决多语言情感识别（SER）中未见过说话者问题的新型架构，名为CAMuLeNet。<br/><br/>2. CAMuLeNet利用基于协同注意力融合和多任务学习的方法来处理多语种SER中的泛化挑战。<br/><br/>3. 除了提出模型外，还对Whisper、HuBERT、Wav2Vec2.0和WavLM等预训练编码器进行了基准测试。<br/><br/>4. 使用10-倍的留说话者-出交叉验证策略在五种现有的多语言基准数据集上进行评估。<br/><br/>5. 作为贡献点之一，还发布了用于印地语情感识别的新数据集BhavVani。 |
| [Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos](https://arxiv.org/abs/2406.09272) | 1. 提出了一种新的环境感知音频生成模型，AV-LLDM。<br/>2. 设计了音频条件机制，用于学习在野外训练视频中分离前景动作声音和背景环境声音。<br/>3. 通过检索增强生成的方法，使模型能够根据无声视频的内容创建匹配的音频，包括语义和时间上的同步。<br/>4. 在Ego4D和EPIC-KITCHENS两个野外ego-centric视频数据集上进行训练和评估，证明了模型的有效性。<br/>5. 比较多种现有的方法，表明模型在生成环境声音方面具有优势，并且有潜力应用于计算机图形游戏剪辑的通用音频生成。 |
| [SingMOS: An extensive Open-Source Singing Voice Dataset for MOS Prediction](https://arxiv.org/abs/2406.10911) | 1. 提出SingMOS，一个针对歌唱领域的高质量、多样化的MOS（Mean Opinion Score）数据集。<br/><br/>2. 数据集涵盖了中文和日语的多个声音合成项目，通过先进的歌唱合成模型生成。<br/><br/>3. 数据集中的合成歌声由专业标注员与真实歌声一起进行评级。<br/><br/>4. 通过对SingMOS的数据分析，证明了这个数据集的多样性和可靠性。<br/><br/>5. 进一步在SingMOS上进行了探索，为歌唱领域的MOS预测提供了见解，并为SingMOS的持续扩展提供了指导。 |
