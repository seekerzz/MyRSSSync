# GitHub All Languages Daily Trending
---
| Title | Summary |
| --- | --- |
| [ollama/ollama](https://github.com/ollama/ollama) | 这段文字是关于一些与Ollama（一种AI助手）相关的插件和项目。它们支持使用Ollama作为后端，为各种用途如代码辅助、聊天机器人等提供服务。<br/><br/>列表中提到的插件包括但不限于：用于Raycast扩展的插件；Discord AI聊天/管理模版的bot；以及一些个人项目，如llama.cpp项目。<br/><br/>总的来说，这段文字是关于Ollama作为AI助手在不同场景下的应用和工具支持。 |
| [state-spaces/mamba](https://github.com/state-spaces/mamba) | Mamba是一个用于线性时间序列建模的库，它使用选择性状态空间（Selective State Spaces, SSMs）的概念。SSMs通过结构化的状态空间对等性来实现通用模型和高效的算法。<br/><br/>如果你在项目中使用了Mamba或者受益于其工作，建议你在相关论文或代码发布时引用以下信息：<br/><br/>```<br/>@article{mamba,<br/>  title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces},},<br/>...<br/>```<br/><br/>请确保更新到最新的版本。 |
| [KRTirtho/spotube](https://github.com/KRTirtho/spotube) | 这段代码是用Dart语言编写的，它包含一个详细的列表，展示了10个与Flutter（一个Google开发的移动应用开发平台）桌面应用程序相关的插件或库。每个插件都有自己的链接和简短描述。 |
| [TheRealJoelmatic/RemoveAdblockThing](https://github.com/TheRealJoelmatic/RemoveAdblockThing) | 这是一个关于如何解决YouTube广告屏蔽问题的用户手册。如果遇到"Ad Blockers violate Youtube Terms Of Service"或全屏黑屏的问题，这里提供了可能的解决方案：检查是否误禁了YouTube或其他浏览器内置的广告过滤器；排查其他可能干扰广告显示的Tampermonkey脚本；检查网络设置，如Wi-Fi/DNS/防火墙等可能屏蔽广告的规则。<br/><br/>如果想要贡献意见、报告bug或者参与到这个用户脚本的开发中来，可以在这个GitHub仓库创建问题或拉取请求。<br/><br/>最后，这个项目遵循MIT开源许可协议。 |
| [microsoft/generative-ai-for-beginners](https://github.com/microsoft/generative-ai-for-beginners) | 这个表格是关于一系列AI生成的课程介绍。每一行代表一个课程，列标题包括课程名称、创建者（John Aziz）、课程类型（GitHub Actions/工作流程）以及其他课程链接。<br/><br/>例如，第一行表示的是名为"ML for Beginners"的课程，由John Aziz创建，并通过GitHub Actions进行自动化部署和工作流程管理。<br/><br/>这个表格提供了一个快速查看不同AI课程信息的平台。 |
| [isaac-sim/IsaacLab](https://github.com/isaac-sim/IsaacLab) | Isaac Lab是一个统一的机器人学习框架，旨在简化机器人研究中的常见工作流程，如强化学习、模仿学习和运动规划。它基于NVIDIA Isaac Sim进行开发，利用该软件的强大模拟功能。<br/><br/>我们鼓励在学术出版物中引用Isaac Lab，特别是在提及我们的贡献时。以下是参考文献格式：<br/><br/>@article{mittal2023orbit,<br/>   author={Mittal, Mayank and Yu, Calvin and Yu, Qinxi and Liu, Jingzhou and Rudin, Nikita and Hoeller, David and Yuan, Jia Lin and Singh, Ritvik and Guo, Yunrong and Mazhar, Hammad and Mandlekar, Ajay and Babich, Buck and State, Gavriel and Hutter, Marco and Garg, Animesh}, <br/>   journal={IEEE Robotics and Automation Letters}, <br/>   title={Orbit: A Unified Simulation Framework for Interactive Robot Learning Environments}, <br/>   year={2023}, <br/>   volume={8}, <br/>   number={6}, <br/>   pages={3740-3747}, <br/>   doi={10.1109/ LRA.2023.3270034} <br/>} |
| [VikParuchuri/marker](https://github.com/VikParuchuri/marker) | 这段文字是关于一个名为"marker"的项目，该项目旨在进行文本提取性能的基准测试。作者提到了如何安装和使用这个项目，以及如何运行基准测试并生成报告。<br/><br/>此外，作者还对项目的背后支持进行了感谢，特别提到了一些开源模型和数据集，这些都为项目的成功提供了重要的基础。 |
| [paul-gauthier/aider](https://github.com/paul-gauthier/aider) | 这段英文内容是关于Aider，一个AI代码助手的评价。用户们对Aider的使用体验给出了高度赞扬，认为它是现有代码库中最佳的开发代理。其中提到了一些具体优点，如提高工作效率、减少学习新代码的时间等。此外，还有人将Aider与市场上其他类似服务进行比较，并明确表示Aider在性能和实际应用价值上超越了竞争对手。 |
| [syncthing/syncthing](https://github.com/syncthing/syncthing) | 这段文本是关于Syncthing，一个开源的持续文件同步程序。它提到了几个关键点：<br/><br/>1. **安全目标**：确保用户数据的安全，防止未经授权的访问或修改。<br/><br/>2. **使用指南**：为用户提供易于理解和操作的指南，包括安装、运行和故障排查等步骤。<br/><br/>3. **Docker部署**：提供如何在Docker环境中运行Syncthing的指导。<br/><br/>4. **签名发布**：自v0.10.15版本起，Syncthing的二进制发布文件已通过GPG签名，确保了发布的完整性。<br/><br/>5. **文档和更新机制**：链接到详细的文档站点，并提到有一个自动升级机制（在某些分发渠道中关闭）以及代码是按照MPLv2许可发布的。 |
| [Lightning-AI/litgpt](https://github.com/Lightning-AI/litgpt) | 本文展示了几个使用 LitGPT（由 Lightning AI 开发的大型语言模型）的项目。这些项目包括NeurIPS 2023 LLM Efficiency Challenge的官方起点，一个名为TinyLlama的小语言模型研究，以及一个关于如何用更少的数据训练小型语言模型的研究论文。<br/><br/>每个项目都展示了LitGPT的强大功能和灵活性，可用于各种自然语言处理任务。 |
| [OpenBMB/MiniCPM-V](https://github.com/OpenBMB/MiniCPM-V) | 这段代码是多个关于大型多语言模型（LLMs）的学术论文引用。它们分别讨论了不同模型如RLHF-V、VisCPM和LLaVA-UHD的能力，以及如何通过AI反馈来提升这些模型的信任度。<br/><br/>如果需要更详细的解释或摘要，请告知具体需求。 |
| [VinciGit00/Scrapegraph-ai](https://github.com/VinciGit00/Scrapegraph-ai) | ScrapeGraphAI是一个基于Python的库，用于从网页抓取数据。它利用大型语言模型进行数据解析和信息提取。<br/><br/>项目的主要贡献者包括Marco Vinciguerra、Marco Perini以及Lorenzo Padoan等。<br/><br/>ScrapeGraphAI旨在为数据探索和研究提供便利，但使用者必须遵守相关法律法规，不得用于非法目的。如有任何问题或滥用行为，我们不负任何责任。 |
| [FlowiseAI/Flowise](https://github.com/FlowiseAI/Flowise) | 本文是一个关于Flowise AI服务的自托管部署指南。它包括了如何在AWS、Azure、DigitalOcean等不同基础设施上部署Flowise，以及支持的其他云提供商。此外，还提到了技术支持和贡献者的联系方式等内容。 |
# 36氪 - 24小时热榜
---
| Title | Summary |
| --- | --- |
| [上海租售比最高的房子出现了](https://www.36kr.com/p/2807761029379462) | 这篇文章的摘要是：<br/><br/>01. 中环内次新板块苏河湾租金回报率反超豪宅，核心原因是老大楼拉高了板块租金回报率。<br/><br/>02. 外环外的租金回报率参考意义不大，因为住房持有阶段成本低，租金是净收入。<br/><br/>03. 通过租金回报率反应产品性价比是可以的，但需关注某些房子特别高的情况，可能并非真正的投资价值。<br/><br/>总结来说，这篇文章讨论了房价租金回报率在不同区域和条件下的表现，并强调了通过这个指标来评估产品性价比的重要性。 |
| [谁在《庆余年2》里赚得最多？](https://www.36kr.com/p/2807634737084038) | 《庆余年2》作为爆款续集，不仅带来了高额的广告收益，还带动了主演张若昀的商业价值提升。原著作者猫腻则通过稿费和版权费获得了收入。此外，剧集的成功也推动了相关品牌代言、衍生品销售以及中插广告投放等商业活动的发展。 |
| [雷军向左，董明珠向右](https://www.36kr.com/p/2807054104435072) | 这篇文章讨论了创始人IP在企业节省广告费用中的作用。尽管创始人IP短期内可能是有效的流量入口，但文章强调了建立平等对话能力的重要性，这对于真正赢得用户共情是长远的策略。 |
| [OpenAI前员工预测：2027年AGI降临，GPT智商飙升，4年从幼儿园蹿到高中生](https://www.36kr.com/p/2807667660649862) | 这篇文章的摘要如下：<br/><br/>Leopold Aschenbrenner，一位来自德国的学者和人工智能领域的研究者，最近在离开OpenAI后计划创办一家专注于AGI（通用人工智能）领域的投资公司。<br/><br/>他已经获得了包括Stripe创始人Collison兄弟以及GitHub前CEO Nat Friedman在内的投资者的支持。<br/><br/>本文介绍了Aschenbrenner的学术背景、兴趣广泛以及他未来商业计划的初步情况。 |
| [奔驰将采用Momenta轻图城区方案，CLA车型搭载，明年4月落地｜独家](https://www.36kr.com/p/2805495652382344) | 这段文本是关于奔驰汽车智能驾驶的进展和分析。主要内容包括：<br/><br/>1. 智驾落地加速：新款E级长轴距版已搭载高速导航辅助驾驶功能，由本土研发团队主导开发，进度较快。<br/><br/>2. 国内品牌追赶：小米SU7和问界M7等车型已支持城市NCA或无图导航辅助驾驶，与奔驰智驾水平相当。<br/><br/>3. 技术积累与海外布局：奔驰自2021年起在德国和美国获得了多项自动驾驶商用许可，并在中国北京、上海等地获得测试牌照，显示出其在智能驾驶领域的技术积累和全球化的市场布局。<br/><br/>4. 中国市场的重要性：尽管奔驰在全球范围内有良好的智驾布局，但中国市场的销量贡献不容忽视。因此，打好中国市场这场仗对于奔驰来说至关重要。 |
| [默多克，为什么这么爱结婚？](https://www.36kr.com/p/2806926822087552) | 这篇资讯主要讲述了媒体大亨默多克的五次婚姻经历。默多克在92岁时第五次结婚，他的新娘是比他小26岁的前模特邓文迪。这段婚姻再次引发了公众对默多克爱情和健康状况的关注。<br/><br/>此外，资讯还提到了默多克早在13年前就创建了一所长寿研究机构，这表明他对长寿有着极高的追求。<br/><br/>总结起来，这篇资讯主要讲述了默多克的五次婚姻以及他对于长寿的研究。这些内容展现了默多克在个人生活和事业追求上的复杂性和多样性。 |
| [8点1氪丨周大福有员工称收到N+3赔偿；东方甄选客服回应“嚎叫式直播”；于东来称大多数企业家不懂得生命的意义](https://www.36kr.com/p/2807559228470914) | 这段内容看起来像是多个新闻或短视频节目的摘录。每个部分都提到了不同的事件：<br/><br/>1. **AMD年更发力AI**：这可能是一个关于半导体公司AMD在人工智能领域持续研发和投入的新闻。<br/><br/>2. **小学生迷上“烟卡”**：这可能是关于学生中流行的一种卡片游戏，这种游戏与烟草行业相关，引起社会关注或教育讨论。<br/><br/>每个部分都提供了事件的概述，并暗示了更深入的报道或分析。 |
| [OpenAI员工们开始反抗了](https://www.36kr.com/p/2806911513696646) | 这段文字是关于OpenAI面临的一系列问题的概述。主要涉及安全问题、内部人员离职和与内容创作者的法律纠纷。OpenAI在去年的动荡中尚未完全恢复，目前正面临着舆论压力和管理挑战。 |
# eess.AS updates on arXiv.org
---
| Title | Summary |
| --- | --- |
| [Hear Me, See Me, Understand Me: Audio-Visual Autism Behavior Recognition](https://arxiv.org/abs/2406.02554) | 1. 介绍音频-视觉自闭症行为识别的新问题，包括社交行为识别，这是之前AI辅助自闭症筛查研究中忽略的重要方面。<br/><br/>2. 定义任务为音频-视觉自闭症行为识别，该任务利用音频和视觉线索，包括可能存在的语音信息，来识别与自闭症相关的行为。<br/><br/>3. 为了促进这项新研究方向的发展，作者收集了一个音频-视觉自闭症谱系数据集（AV-ASD），目前这是最大的用于自闭症筛查的基于行为的方法视频数据集。它涵盖了广泛的各种与自闭症相关的行为，包括社交沟通和互动相关的行为。<br/><br/>4. 为了为进一步研究这个新问题铺平道路，作者深入探索了利用跨模基础模型和多模态大型语言模型在不同模态之间的融合方法。实验结果表明，在AV-ASD数据集上整合音频、视觉和语音信息显著提高了自闭症行为识别的性能。<br/><br/>5. 此外，作者还探讨了在多模态大型语言模型中使用后处理到临时管道的方法来研究其增强模型解释能力的可能性，特别是在自闭症行为识别过程中。这些研究成果将随数据集、代码和预训练模型一起发布。 |
| [PhoWhisper: Automatic Speech Recognition for Vietnamese](https://arxiv.org/abs/2406.02555) | 1. 介绍PhoWhisper，这是一个为越南自动语音识别设计的五个版本。<br/><br/>2. PhoWhisper通过在包含多种越南口音的844小时数据集上微调 Whisper 模型实现了其鲁棒性。<br/><br/>3. 实验研究展示了PhoWhisper在基准越南ASR数据集上的领先性能，表明它达到了行业标准。<br/><br/>4. 该研究团队将PhoWhisper开源，链接为：https://github.com/VinAIResearch/PhoWhisper。 |
| [Less Peaky and More Accurate CTC Forced Alignment by Label Priors](https://arxiv.org/abs/2406.02560) | 1. 该论文提出了一种缓解Connectionist Temporal Classification(CTC)模型输出分布尖峰问题的方法。<br/><br/>2. 利用标签先验信息，通过训练过程对包含更少空格的路径分数进行提升和最大化。<br/><br/>3. 这种方法使得CTC模型产生的后验概率分布更为平滑，减少了尖峰现象。<br/><br/>4. 该模型在预测令牌除了起始点之外的偏移时，准确性得到了提高。<br/><br/>5. 实际上，这种方法相比标准的CTC模型以及基于启发式的方法，在Buckeye和TIMIT数据集上的PBE/WBE错误率方面有12-40%的提升。 <br/><br/>6. 与广泛使用的FA工具包 Montreal Forced Aligner(MFA)相比，尽管在Buckeye上表现相似，但在TIMIT上我们的方法落后于MFA。<br/><br/>7. 尽管如此，我们的方法具有更简单的训练管道和更好的运行效率。我们提供的训练配方和预训练模型已发布在 TorchAudio 中。 |
| [Breaking Walls: Pioneering Automatic Speech Recognition for Central Kurdish: End-to-End Transformer Paradigm](https://arxiv.org/abs/2406.02561) | 1. 开发了针对低资源语言的Central Kurdish（CKB）语音识别系统。<br/>2. 使用端到端的transformers技术进行ASR系统的构建。<br/>3. 利用迁移学习技术来训练声学模型，提高了系统的性能。<br/>4. 系统在Asosoft测试集上达到了最先进的结果，词错误率（WER）为13%。 |
| [Gated Low-rank Adaptation for personalized Code-Switching Automatic Speech Recognition on the low-spec devices](https://arxiv.org/abs/2406.02562) | 1. 提出权重分离方法，通过参数效率高的微调方法减少模型在设备上的权重。<br/><br/>2. 讨论了多语言句中代码切换的情况，强调个性化ASR模型在此场景中的必要性。<br/><br/>3. 针对现有多语种语音识别模型的局限性，提出包含微调过的单语和多语识别模型的代码切换语音识别模型。<br/><br/>4. 引入门控低秩适应（GLoRA）方法，用于参数效率高的微调过程，同时保证性能不显著下降。<br/><br/>5. 通过在韩英代码切换数据集上进行实验，验证了针对代码切换场景进行微调的ASR模型性能优于传统从头训练的代码切换语音识别模型。此外，GLoRA也展示了其在参数效率微调中的优势。 |
| [A cost minimization approach to fix the vocabulary size in a tokenizer for an End-to-End ASR system](https://arxiv.org/abs/2406.02563) | 1. 提出问题：针对混合语音识别系统中，令牌选择受限于特定音素（如电话、双音节或三音节）的情况，论文探讨了在端到端自动语音识别（ASR）系统中如何自由选择和优化令牌。<br/><br/>2. 方法创新：提出构建一个成本函数，通过假设令牌化过程是一个黑箱，来实现对最优令牌数量的选择。这种方法有助于克服传统工具包如ESPNet预定义词汇大小限制的问题。<br/><br/>3. 实验验证：通过在LibriSpeech 100小时数据集上进行实验，证明了精心选择的令牌数量能够显著提高端到端ASR系统的性能。 |
| [Combining X-Vectors and Bayesian Batch Active Learning: Two-Stage Active Learning Pipeline for Speech Recognition](https://arxiv.org/abs/2406.02566) | 1. 介绍了一种基于数据中心的AI方法，提出了一种两阶段主动学习(Active Learning, AL)管道，用于自动语音识别(Automatic Speech Recognition, ASR))。<br/><br/>2. 第一阶段使用无监督AL，通过x-向量聚类来从未标注的语音数据中选择多样化的样本，从而建立一个对后续监督AL阶段有强鲁性的初始数据集。<br/><br/>3. 第二阶段引入了监督AL策略，特别为ASR设计了一种批AL方法，旨在通过选择多样性和信息丰富的批次样本进行学习。<br/><br/>4. 在这里，使用x-向量聚类也实现了样本多样性，而最具信息价值的样本则通过适应蒙特卡罗 dropout的贝叶斯AL方法来识别。<br/><br/>5. 这种方法能够精确地估计不确定性，从而在ASR模型训练中显著减少数据需求，同时展示了比竞争对手更优的表现。 |
| [Cluster-to-Predict Affect Contours from Speech](https://arxiv.org/abs/2406.02569) | 1. 提出了一种将连续情绪识别（CER）转化为动态情感-轮廓簇预测问题的新方法。<br/><br/>2. 定义了一个从集群到预测（C2P）的框架，该框架学习了由情绪-轮廓特征组成的动态情感-轮廓簇。<br/><br/>3. C2P通过无监督的迭代优化过程来运行，目的是通过最小化聚类损失和语音驱动的情感-轮廓预测损失来学习这些动态情感-轮廓簇。<br/><br/>4. 实验结果表明，基于语音驱动的聚类方法对于情绪的两种维度（唤醒度和愉悦度）都具有价值。<br/><br/>5. 在使用RECOLA数据集进行实验时，得到了令人鼓舞的分类结果，其中唤醒度的F1分数为0.84，愉悦度的F1分数为0.75。 |
| [Selfsupervised learning for pathological speech detection](https://arxiv.org/abs/2406.02572) | 1. 该论文研究了神经退行性疾病导致的病理语音，如帕金森病引起的失语症。<br/><br/>2. 研究中提到的病理语音特征包括异常的言语模式和不精确的发音。<br/><br/>3. 论文指出临床诊断这些语言障碍通常依赖于听觉感知测试，但这种方法耗时且诊断结果可能因医生经验、偏见和认知负荷的不同而有所差异。<br/><br/>4. 为了改善这一问题，论文提出自动病理语音检测（PSD）方法。这些方法旨在提供更高效准确的病理语音识别，从而有助于及时干预和支持受影响的个体。 |
| [PPINtonus: Early Detection of Parkinson's Disease Using Deep-Learning Tonal Analysis](https://arxiv.org/abs/2406.02608) | 1. 提供早期检测帕金森病（PD）的新系统，PPINtonus。<br/><br/>2. 利用深度学习的音调分析技术，提供一种成本效益高且易于访问的传统神经系统检查之外的替代方案。<br/><br/>3. 与帕金森语音项目（PVP）合作，使用半监督条件生成对抗网络（CGAN）来生成合成数据点，以增强多层深度神经网络的训练数据集。<br/><br/>4. 结合PRAAT语音学软件，该深度神经网络能够准确评估来自标准麦克风在典型家庭噪音条件下进行的简单120秒声测试中生物声音测量值的准确性。<br/><br/>5. 通过混淆矩阵验证模型性能，达到92.5%的惊人准确率，并且具有较低的假阴性率。这表明PPINtonus在早期PD检测方面具有很高的精度和可靠性。 |
| [Keyword-Guided Adaptation of Automatic Speech Recognition](https://arxiv.org/abs/2406.02649) | 1. 提出了一种基于上下文偏置的Whisper模型，用于改进特定领域术语（如专业词汇）的识别。<br/><br/>2. 利用关键词检测模型，利用Whisper编码器表示动态生成提示，引导解码器在转录过程中进行指导。<br/><br/>3. 推出了两种有效的方法来引导解码器：KG-Whisper旨在微调Whisper解码器，而KG-Whisper-PT则学习提示前缀，以更灵活的方式引导解码过程。<br/><br/>4. 实验结果表明，这种方法显著提高了特定关键词的识别准确率，并降低了整体词错误率。在未见过的语言泛化测试中，平均WER提升了5.1%。 |
| [RepCNN: Micro-sized, Mighty Models for Wakeword Detection](https://arxiv.org/abs/2406.02652) | 1. 提出了一种小卷积模型的训练方法，通过先重构计算为更大冗余的多分支架构。<br/><br/>2. 在推理阶段，采用代数重参数化技术将训练好的模型重新表示成单分支形式，参数更少以降低内存和计算成本。<br/><br/>3. 实验表明，使用这种方法训练的RepCNN模型在延迟与准确性的权衡中提供了良好的性能。相比单一分支模型，其精度提高了43%，同时保持相同的运行时间。<br/><br/>4. RepCNN还能够达到复杂架构如BC-ResNet的准确性，但它的峰值内存使用量减少了2倍，运行速度提高了10倍。 |
| [ConPCO: Preserving Phoneme Characteristics for Automatic Pronunciation Assessment Leveraging Contrastive Ordinal Regularization](https://arxiv.org/abs/2406.02859) | 1. 提出了一种针对基于回归的自动发音评估（APA）模型的新型对比性音素序秩正则化器（ConPCO）。<br/><br/>2. ConPCO设计用于生成更具有音素区分性的特征，同时考虑目标输出之间的有序关系。<br/><br/>3. 通过对比学习的方式，将APA模型的音素表示与文本中音素转录的嵌入进行对齐。<br/><br/>4. 在特征空间中，通过调节不同音位类别间的距离，保持音素特性，同时允许目标输出之间的有序关系。<br/><br/>5. 实施了针对speechocean762基准数据集的实验，以验证方法的有效性和可行性与一些先进的基线相比。 |
| [USM RNN-T model weights binarization](https://arxiv.org/abs/2406.02887) | 1. 研究大型通用语音模型（USM）的生产使用，表明模型已经在实际场景中应用。<br/><br/>2. 提出随着模型规模增大，服务成本也随之增长的问题，强调了模型尺寸减小的研究重要性。<br/><br/>3. 专注于使用权重仅量化的方法进行模型大小减少的工作，这种方法包括权重二值化。<br/><br/>4. 展示了对USM RNN-Transducer（RNN-T）模型进行权重二值化的操作，并指出其模型尺寸可以降低15.9倍。<br/><br/>5. 提供了一个成本效益的视角，即在增加词错误率（WER）1.9%的情况下，相比于浮点32模型，这种量化方法使得模型更适用于实际应用。 |
| [SYN2REAL: Leveraging Task Arithmetic for Mitigating Synthetic-Real Discrepancies in ASR Domain Adaptation](https://arxiv.org/abs/2406.02925) | 1. 提出'SYN2REAL'任务向量概念，针对自动语音识别（ASR）中的文本域领域适应问题。<br/><br/>2. 传统在合成语音上进行微调的策略常导致性能下降，因为存在声学不匹配的问题。<br/><br/>3. 针对这个问题，提出通过创建一个'SYN2REAL'向量来解决，该向量是基于模型在真实和合成语音上的微调差异计算得出的。<br/><br/>4. 实验结果证明了这种方法的有效性，特别是在处理未见过的目标领域时，平均提高了11.15%的词错误率（WER）。<br/><br/>总结：本论文提出了一种新的'SYN2REAL'任务向量，用于解决ASR中文本域领域的适应问题。实验结果显示，该方法能够显著提高目标领域下的性能。 |
| [4D ASR: Joint Beam Search Integrating CTC, Attention, Transducer, and Mask Predict Decoders](https://arxiv.org/abs/2406.02950) | 1. 提出联合建模的4D模型，其中四个解码器（CTC、RNN-T、注意力和掩码预测）共享同一个编码器。<br/><br/>2. 通过多任务学习训练4D模型，这有助于模型正则化，并提高其鲁棒性，因为它们具有互补特性。<br/><br/>3. 提出两阶段训练策略来稳定多任务学习的训练过程。<br/><br/>4. 针对4D模型，提出三种新的单次pass束搜索算法，通过结合不同解码器（CTC、RNN-T和注意力）来优化性能。<br/><br/>5. 对这三种新算法进行了详细性能评估，并分析了它们在计算效率上的权衡。实验结果表明，联合训练的4D模型优于单独使用一个解码器的E2E-ASR模型。同时，提出的单次pass束搜索算法也优于之前的CTC/注意力解码方法。 |
| [Singing Voice Graph Modeling for SingFake Detection](https://arxiv.org/abs/2406.03111) | 1. 提出SingGraph模型，这是针对歌唱声音深度伪造（SingFake）检测的一个创新方法。<br/><br/>2. 模型结合了MERT音乐理解模型的音高和节奏分析能力与wav2vec2.0语言模型对歌词语义的理解。<br/><br/>3. 推崇使用基于音乐领域知识的RawBoost和beat matching技术进行歌唱声音增强，以提升SingFake检测性能。<br/><br/>4. 实验结果表明，提出的SingGraph方法在歌唱声音深度伪造（SingFake）数据集上取得了新的最先进的（SOTA）水平，并在多个场景中超越了之前的最佳模型。 |
| [RevRIR: Joint Reverberant Speech and Room Impulse Response Embedding using Contrastive Learning with Application to Room Shape Classification](https://arxiv.org/abs/2406.03120) | 1. 提出双编码架构，用于直接从语音语句中估计房间参数。<br/><br/>2. 在预训练阶段，一个编码器处理RIR，另一个处理回声的语音信号。<br/><br/>3. 使用对比性损失函数，将语音和声学响应联合嵌入。<br/><br/>4. 在微调阶段，针对特定分类任务进行训练。<br/><br/>5. 在测试阶段，仅提供回声语音，使用其嵌入进行房间形状分类的任务。 |
| [Once more Diarization: Improving meeting transcription systems through segment-level speaker reassignment](https://arxiv.org/abs/2406.03155) | 1. 提出针对会议转录系统中演讲者混淆问题的解决方案，即段级演讲者重新分配。<br/><br/>2. 在语音增强后，对每个段落进行重新评估其对应的演讲者标签，以减少初始分段阶段的演讲者混淆错误。<br/><br/>3. 通过跨不同配置和数据集的实验进一步验证了该方法的有效性和适用性。<br/><br/>4. 结果表明，段级演讲者重新分配能够显著纠正至少40%的由于演讲者混淆导致的错误单词，这突显了其在提高会议转录系统中分段后演讲者标签准确性方面的潜力。 |
| [CoLLAB: A Collaborative Approach for Multilingual Abuse Detection](https://arxiv.org/abs/2406.03205) | 1. 本研究探索了在音频滥用检测（AAD）任务中，来自预训练的平行语模型（PTM）的表示，这是之前未被探索的应用领域。<br/><br/>2. 研究结果表明，与其他PTM代表相比，这些PTM在ADIMA基准上的性能优越。<br/><br/>3. 此外，结合PTM表示可以进一步提升AAD性能。这说明了预训练模型的有效性。<br/><br/>4. 尽管取得了这些改进，但跨语言泛化能力的挑战仍然存在，某些语言需要在同一语言环境下进行训练。<br/><br/>5. 这导致了为不同语言单独建模的问题，从而影响了系统的可扩展性、维护成本和资源分配。<br/><br/>6. 因此，研究者提出了CoLLAB框架，这是一个新颖的不依赖于特定语言训练的框架，它允许通过加权平均轻松合并不同语言训练模型。<br/><br/>7. 这种方法产生了具有竞争力的跨多语言AAD性能的统一模型。这有助于解决实际部署中由于语言差异而带来的挑战。 |
| [Reference Channel Selection by Multi-Channel Masking for End-to-End Multi-Channel Speech Enhancement](https://arxiv.org/abs/2406.03228) | 1. 提出适应性参考通道选择方法，解决传统多通道语音增强中固定参考通道的问题。<br/><br/>2. 利用多通道掩模为基础的方案，通过结合多个被掩蔽的信号生成单一输出信号，提升处理效果。<br/><br/>3. 采用最高可比不变信号-噪声比(SI-SDR)的调整方式来确定参考的清洁语音，确保性能最优。 <br/><br/>4. 在Spear挑战模拟数据集D4上进行实验验证，证明了所提出的适应性参考通道选择方法的有效性和优越性。 |
| [Multi-Microphone Speech Emotion Recognition using the Hierarchical Token-semantic Audio Transformer Architecture](https://arxiv.org/abs/2406.03272) | 1. 该研究探讨了新的方法来缓解情绪识别系统在真实世界（野外）环境中性能下降的问题，这些环境通常会受到回声污染。<br/><br/>2. 提出使用多麦克风信号处理来应对这些挑战，并提高情感分类的准确性。这是针对现有单通道系统不足的一种改进策略。<br/><br/>3. 采用了最先进的Transformer模型，即Hierarchical Token-semantic Audio Transformer（HTS-AT）, 来处理多通道音频输入，这表明在实际应用中，先进的深度学习模型能够有效提升情绪识别系统的性能。 |
| [Enhancing CTC-based speech recognition with diverse modeling units](https://arxiv.org/abs/2406.03274) | 1. 通过分析，研究者探讨了在E2E自动语音识别（ASR）模型演进中，除了系统组合效应外，改进来源的潜在机制。<br/><br/>2. 提出了一种有效的联合训练方法，这种方法是在训练E2E模型的同时，与多样化的建模单元进行联合训练。<br/><br/>3. 这种方法不仅能够强化基于音素和基于字符的模型的优势，还揭示了在协同作用下使用这些多元建模单元可以显著提升模型准确性。<br/><br/>4. 通过这项研究，作者为ASR系统开发中如何最优地整合不同类型的建模单元提供了新的见解。 |
| [The PESQetarian: On the Relevance of Goodhart's Law for Speech Enhancement](https://arxiv.org/abs/2406.03460) | 1. 提出风险：论文指出，过度优化语音增强模型以匹配评估使用的指标可能会导致过拟合。<br/><br/>2. 实例说明：通过引入利用广泛使用的PESQ指标的增强模型——"PESQetarian"，并展示其在VB-DMMD基准上的高PESQ分数（3.82），但实际听感评价却非常差。<br/><br/>3. 提出警示：论文强调，当优化模型以匹配特定评估指标时，仅基于该指标的孤立评估可能会误导结果。建议在评估和预测性能时，应包括其他相关指标，并通过听觉验证来确认预测结果。 |
| [Sequence-to-sequence models in peer-to-peer learning: A practical application](https://arxiv.org/abs/2406.02565) | 1. 研究探索了基于LSTM单元的序列到序列（Seq2Seq）模型在对等学习环境中自动语音识别（ASR）任务的应用可行性。<br/><br/>2. 利用两种不同的对等学习方法，研究模拟了代理的学习过程，并通过两个不同ASR数据集评估其在ASR任务中的性能。<br/><br/>3. 在集中式训练设置下，使用Deep Speech 2模型的简化版，单个模型在UserLibri和LJ Speech数据集上分别达到了84%和38%的Word Error Rate（WER）。<br/><br/>4. 相反，在包含55个代理的对等学习环境中，对于UserLibri数据集，WER范围从87%到92%，而对于LJ Speech数据集，WER范围从52%到56%。这些结果表明在去中心化设置中使用Seq2Seq模型是可行的，尽管其WER略高于集中式训练方法。 |
| [Operational Latent Spaces](https://arxiv.org/abs/2406.02699) | 1. 研究了通过自监督学习构建潜在空间的过程，以支持语义上有意义的操作。<br/><br/>2. 提出类似运算放大器的“操作性潜在空间”(OpLaS)，它们不仅展示了如聚类这样的语义结构，还支持具有内在语义含义的常见变换操作。<br/><br/>3. 发现一些操作性潜在空间是无意中在向其他自监督学习目标进展的过程中产生的，这些空间中意外但仍然有用的属性被点在空间中的关系中发现。<br/><br/>4. 提出通过自监督学习有意创建操作性潜在空间的方法，包括通过一种名为“FiLMR”的新颖层引入旋转操作，这可以用来启用某些音乐构造中存在的环状对称性。 |
| [Textless Acoustic Model with Self-Supervised Distillation for Noise-Robust Expressive Speech-to-Speech Translation](https://arxiv.org/abs/2406.02733) | 1. 提出了一种无文本的声学模型，该模型使用自我监督的蒸馏策略进行训练。<br/><br/>2. 为解决噪声环境下表达性语音到文本翻译（S2ST）系统脆弱的问题，设计了一个包含DINO自我监督训练的U2S生成器。<br/><br/>3. 通过这种方法，声学模型能够捕获噪声不敏感的表达性特征表示，即使在嘈杂环境中也能生成高质量的语音。<br/><br/>4. 实验结果证明了该方法显著提高了噪声环境下表达性S2ST系统的性能，并且在清洁环境下的表现保持竞争力。 |
| [LiveSpeech: Low-Latency Zero-shot Text-to-Speech via Autoregressive Modeling of Audio Discrete Codes](https://arxiv.org/abs/2406.02897) | 1. 提出LiveSpeech，一个基于全自回归语言模型的零-shot文本到语音方法。<br/><br/>2. 为实现低延迟输出音频流，提出使用适应性码本损失权重的方法。这种方法考虑了每个帧中代码本的贡献，并聚焦于困难实例。<br/><br/>3. 探索并提出将代码书分组并在并行处理组中的策略。这有助于提高模型在保证内容准确性、语音相似度、音频质量及推理速度的同时，适应低延迟流应用的能力。 |
| [Text Injection for Neural Contextual Biasing](https://arxiv.org/abs/2406.02921) | 1. 提出Contextual Text Injection (CTI)技术，用于增强上下文相关的自动语音识别（ASR）。<br/><br/>2. CTI利用了大量未配对的文本数据，优化ASR模型及其偏置组件，从而提高ASR性能。<br/><br/>3. 介绍CTI结合最小词错误率（MWER）训练的方法。这种方法在使用未配对文本注入模型时，能最小化因上下文偏置引起的预期词错误率。<br/><br/>4. 实验表明，采用100亿条文本句子的CTI技术，可以相对于一个强大的神经偏置模型降低43.3%的相对词错误率。<br/><br/>5. CTI-MWER进一步提供了23.5%的相对改进。这表明CTI不仅可以显著提高ASR性能，而且结合特定训练策略还能进一步优化结果。 |
| [Robots Have Been Seen and Not Heard: Effects of Consequential Sounds on Human-Perception of Robots](https://arxiv.org/abs/2406.02938) | 1. 机器人产生的机器声音对人们感知机器人有显著的负面影响。<br/>2. 听到机器人声音，参与者的情绪更负面，如感到不舒服或焦虑。<br/>3. 机器人声音的存在与参与者注意力分散和集中能力降低有关。<br/>4. 参与者报告说，在共享环境中与机器人共存的可能性更低。 |
| [Addressing Index Collapse of Large-Codebook Speech Tokenizer with Dual-Decoding Product-Quantized Variational Auto-Encoder](https://arxiv.org/abs/2406.02940) | 1. 该工作提出了一种名为PQ-VAE的模型，用于解决VQ-VAE在大型代码书中常见的"指数崩溃"问题。<br/><br/>2. PQ-VAE通过增加多个VQ子空间来编码语音特征，并将这些子空间的组合形成一个更大代码书中的codewords。<br/><br/>3. 为了更好地利用每个VQ子空间，论文还提出了一种双解码训练策略，结合编码和量化序列进行训练。<br/><br/>4. 实验结果表明，PQ-VAE有效地解决了"指数崩溃"问题，特别是在大型代码书中。采用提出的训练策略的模型进一步提高了代码书困惑度和重建质量，并优于其他多代码本VQ方法。<br/><br/>5. 最后，PQ-VAE在基于语言模型的TTS中展示了其有效性，支持使用更大代码书生成更高品质语音。 |
| [AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection](https://arxiv.org/abs/2406.02951) | 1. 提出问题：随着深度伪造视频内容的快速增长，需要更先进和通用的方法来检测它们。<br/><br/>2. 方法创新：提出Audio-Visual Feature Fusion (AVFF)方法，这是一种两阶段跨模学习方法，旨在明确音频和视觉模态之间的对应关系以提高深度伪造检测能力。<br/><br/>3. 技术特点：第一阶段通过自监督学习在真实视频上进行表示学习，捕捉内在的音频-视觉对应。第二阶段对学习到的表示进行调优，通过监督学习在真实和伪造视频上进行分类任务。<br/><br/>4. 实验与分析：通过大量实验和数据分析，证明AVFF方法具有高度的判别性，并在FakeAVCeleb数据集上取得了显著优于当前音频-视觉最先进的性能。 |
| [Dataset-Distillation Generative Model for Speech Emotion Recognition](https://arxiv.org/abs/2406.02963) | 1. 提出Dataset Distillation（DD）概念，旨在学习一个更小的语音数据集，同时保持在下游训练中的性能。<br/><br/>2. 将DD方法应用于语音情感识别任务，目标是IEMOCAP数据集。<br/><br/>3. 利用生成对抗网络（GANs）作为DD的方法。GANs不是用来模仿真实数据，而是用于提取IEMOCAP数据集的关键区分信息，这些信息对下游训练是有价值的。<br/><br/>4. 通过使用GANs，可以自动生成定制大小的合成语音数据集，并且保持与原始不平衡类别相当的性能。<br/><br/>5. 该方法还减少了存储需求，加速了下游训练，具体表现为95%的速度提升。此外，它也降低了包含个人信息的语音数据中的隐私风险。 |
| [StreamSpeech: Simultaneous Speech-to-Speech Translation with Multi-task Learning](https://arxiv.org/abs/2406.03049) | 1. 提出StreamSpeech，一个直接的Simul-SS2ST模型，该模型在多任务学习统一框架下联合学习翻译和同时的策略。<br/><br/>2. 基于多任务学习方法，StreamSpeech能够进行离线和同时的语音识别、语音翻译以及语音合成，通过一个无缝的"全方位一体"模型实现。<br/><br/>3. 在CVSS基准测试中，实验结果表明StreamSpeech在离线S2ST和Simul-SS2ST任务上达到了最先进的性能水平。此外，它还能提供高质量的中间结果（如ASR或翻译结果），为实时多语种交流提供了更全面的体验。 |
| [A Frame-based Attention Interpretation Method for Relevant Acoustic Feature Extraction in Long Speech Depression Detection](https://arxiv.org/abs/2406.03138) | 1. 提出问题：针对基于语音的抑郁症检测工具可能存在的临床实用性障碍，如段级标注噪声和模型解释性不足。<br/><br/>2. 解决方案：提出一种名为Audio Spectrogram Transformer（AST）的语音级别音频频谱变换器。该模型避免了段级标注带来的噪声问题。<br/><br/>3. 实验验证：通过对比，发现AST显著优于基于段级标注的模型，这为段级标注噪声的存在以及长时语音分析对抑郁症检测的优势提供了证据。<br/><br/>4. 模型解释性：引入帧级注意力解释方法，从预测相关的波形信号中提取临床可理解的声学特征，供临床医生解读。通过这一过程，观察到AST能够识别抑郁患者言语中的音量降低和音高（F0）变化等特征，这与临床研究中抑郁症患者语音特点相一致。 |
| [Generalized Fake Audio Detection via Deep Stable Learning](https://arxiv.org/abs/2406.03237) | 1. 提出一种基于稳定学习的训练方案，名为Sample Weight Learning (SWL)模块。<br/><br/>2. SWL通过训练样本的学习权重来解决分布差异问题，即通过装饰相关特征的方差来减小模型对新分布数据的敏感性。<br/><br/>3. SWL设计为轻量级、可插拔的组件，易于应用于多种基础模型，并在训练过程中无需额外数据即可增强模型泛化能力。<br/><br/>4. 通过在ASVspoof等多个不同分布的评估集上进行实验，SWL的有效性得到了明确证明。 |
| [Generalized Source Tracing: Detecting Novel Audio Deepfake Algorithm with Real Emphasis and Fake Dispersion strategy](https://arxiv.org/abs/2406.03240) | 1. 提出Real Emphasis and Fake Dispersion(REFD)策略，用于音频深度伪造算法识别。<br/><br/>2. 该策略在区分ID样本和识别OOD样本方面表现出有效性。<br/><br/>3. 对于有效的OOD检测，研究了现有的后处理OOD方法，并提出了NSD，一种新的OOD识别方法。<br/><br/>4. NSD通过考虑特征和logits分数的相似性来识别新深度伪造算法，展示了其先进的性能。 |
| [Genuine-Focused Learning using Mask AutoEncoder for Generalized Fake Audio Detection](https://arxiv.org/abs/2406.03247) | 1. 提出Genuine-Focused Learning (GFL)框架，旨在设计高度通用的Fake Audio Detection (FAD)方法。<br/><br/>2. 创造性地应用Counterfactual Reasoning Enhanced Representation (CRER)，基于音频重建，使用Mask AutoEncoder (MAE)架构来精确模拟真实音频特征。<br/><br/>3. 为减少训练过程中假音频的影响，引入了真实的音频重建损失，保持对学习真实数据特征的关注。<br/><br/>4. 提取内容相关的瓶颈(BN)特征，从MAE中提取，以补充原始音频的知识。<br/><br/>5. 这些BN特征被动态融合到CRER中，进一步提高了模型的鲁棒性。 |
| [ASoBO: Attentive Beamformer Selection for Distant Speaker Diarization in Meetings](https://arxiv.org/abs/2406.03251) | 1. 提出了一种基于自注意力的算法，用于选择固定空间滤波器银行的输出。<br/><br/>2. 该方法作为联合语音活动检测（VAD）和重叠 speech 检测（OSD）的特征提取器。<br/><br/>3. 使用这种方法进行远程VAD、OSD和说话者分段（Speaker Diarization, SD），并展示了在AISHELL-4数据集上14.5%DER的良好性能。<br/><br/>4. 分析自注意力权重，证明它们具有可解释性，因为权重与说话者的角度位置相关。 |
| [Audio Mamba: Bidirectional State Space Model for Audio Representation Learning](https://arxiv.org/abs/2406.03344) | 1. 提出问题：研究者探索在音频分类任务中，是否真的需要依赖于自我注意力（self-attention）。<br/><br/>2. 创新模型：提出Audio Mamba( AuM)模型，这是第一个不依赖自我注意力的纯粹基于状态空间模型（SSMs）的音频分类模型。<br/><br/>3. 实验验证：在多个音频数据集上进行评估，包括六个不同的基准测试，结果显示AuM在性能上可以与或超越现有的基于AST的模型。<br/><br/>综上所述，这项研究通过创新模型和实验验证，对音频分类任务中是否需要依赖自我注意力这个问题进行了贡献。 |
| [Physics and geometry informed neural operator network with application to acoustic scattering](https://arxiv.org/abs/2406.03407) | 1. 提出了一种基于物理和几何信息的神经操作网络，用于解决声散射的前向模拟问题。<br/><br/>2. 该网络设计旨在学习不同计算域下的解决方案操作，这是一个通用重要问题，适用于工程应用中的多种场景。<br/><br/>3. 提供了一种基于非均匀有理B-样值（NURBS）的几何参数化方法，用于表示任意形状的声散射源。<br/><br/>4. 这种方法不仅能够生成对复杂声源几何描述的紧凑表示，还减少了传统方法中因几何复杂性而产生的冗余计算。<br/><br/>5. 与现有的物理依赖型方法相比，该模型训练后可以快速学习并适应不同计算域下的解决方案，大大缩短了前向模拟的时间，提高了计算效率。<br/><br/>6. 此外，由于该网络能够预测声压场，因此在不需要大量标注训练数据的情况下，也能对声散射的声压场进行评估。 |
| [HAAQI-Net: A Non-intrusive Neural Music Audio Quality Assessment Model for Hearing Aids](https://arxiv.org/abs/2401.01145) | 1. 提出HAAQI-Net，一个非侵入性的深度学习模型，专门用于音乐音频质量评估，特别针对听力辅助用户。<br/><br/>2. 与传统方法如HAAQI相比，HAAQI-Net提供了一种更易访问和效率更高的评估方式。它使用双向长短期记忆（BLSTM）架构结合注意力机制，并利用预训练的BEATs模型提取特征。<br/><br/>3. 实验结果表明，HAAQI-Net的有效性，预测的评分与线性相关系数（LCC）、斯皮尔曼秩相关系数（SRCC）和均方误差（MSE）均有显著关联。通过参数减少75.85%和推理时间缩短96.46%，HAAQI-Net的效率和可扩展性得到了提升，使其在听力辅助技术中进行实际音乐音频质量评估成为可能。 |
| [Multi-Sample Dynamic Time Warping for Few-Shot Keyword Spotting](https://arxiv.org/abs/2404.14903) | 1. 提出多样本动态时间 warping（Multi-Sample DTW）方法，用于计算特定类别的成本张量，这些张量包含了所有查询样本的变异性。<br/><br/>2. 为显著降低推理时的计算复杂性，提出将成本张量转换为成本矩阵再进行动态时间 warping的策略。<br/><br/>3. 在几样本关键词检测实验中，验证了这种方法与使用所有单个样本来作为模板并使用Fr\'echet平均值相比，性能相当且运行速度略慢。 |
| [Improving the Adversarial Robustness for Speaker Verification by Self-Supervised Learning](https://arxiv.org/abs/2106.00273) | 1. 该论文提出了一种对抗性防御方法，用于自动说话者验证（ASV）系统，而无需知道具体的攻击算法。<br/><br/>2. 研究灵感来源于自我监督学习模型（SSLMs），这些模型能够通过消除输入中的表面噪声和从中断的样本中重构清洁样本来减轻噪声。<br/><br/>3. 该工作将对抗性perturbations视为一种噪音，并通过SSLMs进行ASV的对抗性防御。具体方法包括两个角度：1）对抗性perturbation净化，2）对抗性perturbation检测。<br/><br/>4. 实验结果表明，提出的检测模块能够有效地保护ASV系统，通过检测具有约80%准确性的对抗样本来屏蔽攻击。<br/><br/>5. 由于ASV对抗性防御性能的评估标准尚未统一，该工作还提出了针对净化和检测两种方法的评估指标，并鼓励未来研究进行基于这些指标的基准测试。 |
| [Parallel Synthesis for Autoregressive Speech Generation](https://arxiv.org/abs/2204.11806) | 1. 提出新的自回归生成思想：替代传统的逐时间步迭代预测，提出频率-自回归生成（FAR）和位-自回归生成（BAR）的方法。<br/><br/>2. 实现高效的语音合成：通过将自回归计算扩展到频域或位域，显著减少了生成的迭代次数，从而提高了语音合成的效率。<br/><br/>3. 结合后滤器进行信号采样：在输出后验概率的信号上应用后滤器，以从这些概率中有效地抽样信号。<br/><br/>4. 设计基于新方法特性的训练目标：根据提出的FAR和BAR生成策略的特点，设计适应这些方法的训练目标。 |
| [MultiPA: A Multi-task Speech Pronunciation Assessment Model for Open Response Scenarios](https://arxiv.org/abs/2308.12490) | 1. 提出MultiPA：一个多任务发音评估模型，用于开放响应场景的发音能力综合评估。<br/><br/>2. 评估内容全面：除了句子级别的准确性，还涵盖了流畅度、语调（prosody）、以及单词级别的准确度评估。<br/><br/>3. 研究方法：通过考察不同发音任务之间的相关性，并展示多任务学习的优势，来验证模型的有效性和价值。<br/><br/>4. 实验结果与应用：实验结果显示MultiPA在现有领域数据集上的性能达到先进水平，并且能够有效地泛化到一个新收集的跨领域数据集上。这证明了该模型在实际应用中的实用性和有效性。 |
| [Bidirectional Autoregressive Diffusion Model for Dance Generation](https://arxiv.org/abs/2402.04356) | 1. 提出基于双向自回归扩散模型(BADRM)的音乐到舞蹈生成方法，用于捕捉和生成人类行为。<br/><br/>2. 建立了双向编码器，确保生成的舞蹈在前后方向上具有和谐性。<br/><br/>3. 设计了局部信息解码器，用于增强局部舞蹈动作，使整体舞蹈更为流畅。<br/><br/>4. 提出通过迭代生成个体运动切片，并整合所有预测来实现舞蹈生成的未来设想。<br/><br/>5. 通过实验结果证明，提出的模型相比现有单向方法在音乐到舞蹈生成领域达到了领先水平。 |
| [Listenable Maps for Audio Classifiers](https://arxiv.org/abs/2403.13086) | 1. 提出Listenable Maps for Audio Classifiers (L-MAC)这一后处理解释方法，用于音频分类器生成可信且可听的解释。<br/><br/>2. L-MAC通过在预训练分类器之上添加一个解码器来工作。解码器生成二进制掩模，强调输入音频中的相关部分。<br/><br/>3. 训练解码器时使用了一个损失函数，该函数最大化了分类器决策对掩模中音频部分的信心，同时最小化模型输出的概率，针对掩模掉区域。<br/><br/>4. 通过量化评估，在内域和跨域数据上，L-MAC显著比几种基于梯度和掩模的解释方法生成更可信的解释。<br/><br/>5. 进行用户研究进一步证实，平均而言，用户更倾向于接受由该技术生成的解释。 |
| [SimpleSpeech: Towards Simple and Efficient Text-to-Speech with Scalar Latent Transformer Diffusion Models](https://arxiv.org/abs/2406.02328) | 1. 提出了一种简单且高效的非自回归（NAR）文本到语音（TTS）系统，名为SimpleSpeech。<br/><br/>2. 系统的简化体现在三个方面：(1) 可以在只有语音数据的语料库上进行训练，无需任何对齐信息；(2) 直接输入纯文本并生成语音，采用非自回归方式；(3) 试图通过有限且紧凑的潜在空间模型语音，这有助于缓解扩散模型的建模难度。<br/><br/>3. 具体来说，提出了一个名为SQ-Codec（Speech Codec Model with Scalar Quantization）的新型语音编码器模型，它使用量化技术，有效地将复杂的语音信号映射到有限且紧凑的标量潜在空间中，即所谓的标量潜在空间。<br/><br/>4. 由于SQ-Codec的存在，作者在SQ-Codec的标量潜在空间中应用了一种新颖的自注意力扩散模型。通过在4k小时的只有语音数据的语料库上进行训练，SimpleSpeech展示了自然的音调和语音克隆能力。<br/><br/>5. 与先前的大规模TTS模型相比，SimpleSpeech在语音质量改善和生成速度提升方面表现出显著优势。此外，还发布了Demo供用户试用。 |
