# 五里墩茶社
---
| Title | Date | Summary |
| --- | --- | --- |
# 小工蚁创始人
---
| Title | Date | Summary |
| --- | --- | --- |
# AIGCLINK
---
| Title | Date | Summary |
| --- | --- | --- |
# 技术爬爬虾
---
| Title | Date | Summary |
| --- | --- | --- |
# 秋芝2046
---
| Title | Date | Summary |
| --- | --- | --- |
# GitHub All Languages Daily Trending
---
| Title | Summary |
| --- | --- |
| [openai/codex](https://github.com/openai/codex) | 轻量级代码代理工具，可通过npm或Homebrew全局安装，在终端本地运行。提供IDE集成与基于Web的云服务版本；快速启动指南包含安装和使用CLI的方法及自定义平台选择步骤，并支持通过GitHub发布下载特定平台二进制文件。还提及了利用ChatGPT计划、API密钥认证，以及官方文档和开源许可信息。 |
| [VectifyAI/PageIndex](https://github.com/VectifyAI/PageIndex) | 此文档是一个关于名为PageIndex的项目或工具的介绍，着重强调了其在金融领域中的应用和成就。以下是中文版本的大致内容：<br/><br/>**项目简介与目标**<br/><br/>PageIndex似乎是一个提供深度索引、构建文档层次结构并帮助用户高效搜索和理解复杂文件（如财务报告）的系统或平台。<br/><br/>- **Mafin 2.5**：一个基于推理的关系型问答系统，用于金融文档分析。通过使用PageIndex，Mafin在FinanceBench基准测试中获得了98.7%的高准确率，远超传统的向量基线系统。<br/>  <br/>- **Vectorless RAG（无向量RAG）**：提供关于如何实现无需向量的RAG系统的指南和实例。<br/><br/>- **Cookbooks、Tutorials**：包括用于实际操作和高级使用案例的手册。此外，还有关于文档搜索与树搜索等策略的教学。<br/><br/>- **Blog文章**：分享技术文章、研究洞察以及产品更新等内容。<br/><br/>- **MCP（模型控制面板）设置与API文档**：提供集成详情、配置选项以及如何利用API进行交互的说明。<br/><br/>**支持与联系方式**<br/><br/>- 提供GitHub上项目的星标支持，鼓励用户给予反馈或合作。<br/>  <br/>- 邀请通过Twitter、LinkedIn和Discord等渠道联系项目团队。<br/>  <br/>- 使用Typeform表单收集用户的反馈和建议。<br/><br/>**版权声明**<br/><br/>文档中明确指出了版权信息，表明此内容归Vectify AI所有，且版权所有。 |
| [OpenBMB/UltraRAG](https://github.com/OpenBMB/UltraRAG) | UltraRAG是一个全面的文档库，提供了从基础到高级的技术指南、部署教程和深度研究案例。以下是总结：<br/><br/>1. **技术指南**：<br/>   - **代码集成**：学习如何在Python代码中直接调用UltraRAG组件，实现更灵活的定制开发。<br/>   - **错误跟踪与分析**：提供可视化Case Study界面来深入了解工作流中的每个中间输出，有助于问题定位和原因分析。<br/><br/>2. **部署教程**：<br/>   - **管理员模式**：快速启动UltraRAG UI并熟悉高级配置选项。<br/>   - **环境部署**：详细指导生产环境的搭建，包括Retriever、生成模型（LLM）以及Milvus向量数据库的设置。<br/><br/>3. **深度研究案例**：<br/>   - 实施一个旗舰级的研究流程，结合AgentCPM-Report模型实现自动化的多步骤检索与集成报告生成，处理数千字的调研报告。<br/><br/>4. **贡献指南**：邀请社区成员加入贡献代码和测试，共享建设RAG生态系统的经验。<br/><br/>5. **技术支持**：<br/>   - **问题反馈**：在GitHub上提交技术问题或功能请求。<br/>   - **交流讨论**：通过WeChat、Feishu或Discord与开发者团队和用户进行互动和交流。<br/><br/>6. **支持方式**：<br/>   - 请使用Star按钮在GitHub页面给予项目支持，表示认可和鼓励。<br/>   - 联系邮箱yanyk.thu@gmail.com提供反馈、建议或询问问题。<br/><br/>总结：UltraRAG作为文档库旨在为用户提供从入门到进阶的技术指导和案例研究。通过社区合作和技术支持机制，用户可以快速上手并深入探索各种RAG技术应用。 |
| [microsoft/VibeVoice](https://github.com/microsoft/VibeVoice) | VibeVoice 是一款文本到语音模型，它拥有三个主要版本：<br/><br/>1. **VibeVoice-Streaming**（实时流式TTS） - 这个版本特别轻量级且支持实时输入和流式文本，能够生成持续约10分钟的稳定语音输出。<br/><br/>2. **VibeVoice**（包含VibeVoice-Realtime、VibeVoice-Vocaloid和VibeVoice-Singing）- 提供了从快速响应到长音频内容创建的不同能力，适合多种应用场景。实时版本可以在大约300毫秒后开始听到声音，并且可以生成长达10分钟的语音流。<br/><br/>3. **VibeVoice-Vocaloid** - 特别设计用于音乐创作和演唱合成，在音乐制作领域提供了强大的工具。<br/><br/>这些模型都从Qwen2.5 1.5b基础模型中提取，因此可能有特定的偏差、错误或遗漏。使用时需要注意：<br/><br/>- 避免滥用生成内容进行欺诈或误导性传播。<br/>- 检查内容的准确性，并在分享合成语音之前确认其可靠性。<br/>- 应对生成的内容负责，在部署和使用时遵循所有相关法律法规。<br/><br/>VibeVoice 目前主要用于研究和开发，不建议直接用于商业或现实世界应用。请在进一步测试和开发之后谨慎使用。<br/><br/>###风险与限制<br/><br/>1. **意外结果**：模型可能产生出人意料、偏见、错误的结果。<br/>2. **深度伪造与误导**：高质量的合成语音可能会被误用，用于创建欺诈性或误导性的音频内容，以进行身份冒充或其他不法行为。用户在使用和分享生成的内容时需谨慎，并应充分披露。<br/><br/>###星标历史概览<br/><br/>通过图表直观展示了项目“Microsoft/vibevoice”的星级变化趋势。<br/><br/>这就是VibeVoice模型的介绍及总结，涵盖了其功能、风险与限制、以及未来发展的方向。 |
| [qarmin/czkawka](https://github.com/qarmin/czkawka) | Czkawka是一个用于查找文件和图像重复的跨平台工具。以下是对该项目的一些关键信息：<br/><br/>1. **项目概述**：<br/>   Czkawka支持各种操作系统（如Windows、macOS、Linux），使用C++编写，并通过其核心库`czkawka_core`供其他项目重用。<br/><br/>2. **功能**：<br/>   - 使用标准的MD5算法来比较文件，以高效地检测重复。<br/>   - 支持文件和图片重复查找。<br/>   - 提供命令行工具、图形用户界面（GTK GUI）以及Tauri框架下的GUI实现等不同前端。<br/>   - 支持Python语言接口。<br/><br/>3. **项目状态**：<br/>   作者感谢为Czkawka做出贡献的人，包括提供补丁、翻译、创建示例视频和文章的社区成员。Czkawka的核心库和其他官方项目由作者直接维护。<br/><br/>4. **可用版本与分发**：<br/>   包含预构建二进制文件的存储库、在crates.io和Flathub上的包，以及官方支持的项目。<br/>   <br/>5. **许可证**：<br/>   Czkawka GTK GUI应用和CLI工具遵循MIT许可证，而Krokiet采用GPL-3.0-only许可证。所有图像遵循CC BY 4.0许可。<br/><br/>6. **贡献与捐赠**：<br/>   鼓励使用Czkawka的用户通过赞助页面对项目进行支持以促进其发展。作者接受在GitHub上的捐款。<br/><br/>7. **官方资源与警告**：<br/>   Czkawka没有官方网站，应谨慎对待声称是官方源的其他站点。推荐直接从官方渠道获取安全包。<br/><br/>8. **代码和内容许可**：<br/>   项目的代码遵循MIT许可证，图像则使用CC BY 4.0许可。<br/><br/>总之，Czkawka是一个功能全面、跨平台且社区支持强大的文件重复检测工具，受到多语言接口的支持，并通过多个渠道提供不同的访问方式。 |
| [Psiphon-Inc/conduit](https://github.com/Psiphon-Inc/conduit) | 该文本主要介绍了Conduit App，它基于来自Psiphon-Labs的psiphon-tunnel-core运行，并适用于Android、iOS和Mac（通过Catalyst）。项目使用Git LFS管理大型文件如隧道核心库。对于翻译信息，请参阅i18n/README.md了解如何拉取并验证翻译内容。 |
| [remotion-dev/remotion](https://github.com/remotion-dev/remotion) | Remotion 是一个使用 React 创建视频的框架，利用 web 技术（CSS、Canvas、SVG、WebGL）和编程特性（变量、函数、APIs），以及 React 的优势（可复用组件、强大组合、快速刷新、包生态系统）。它允许以代码形式制作视频，并提供多种通过Remotion创建的示例。入门方式为安装 Node.JS 后，运行 `npx create-video@latest`，或参阅官方文档获取更多信息。 |
| [Blaizzy/mlx-audio](https://github.com/Blaizzy/mlx-audio) | MLX Audio是一个专为Apple Silicon平台设计的音频处理库，提供了文本转语音（TTS）、语音识别（STT）和语音到文本转换（STS）的功能。以下是关于MLX Audio的重要点：<br/><br/>1. **核心功能**：<br/>   - **文本转语音（TTS）**: 将文本转换为人类可听的声音。<br/>   - **语音识别（STT）**: 将用户的语音输入转化为文本形式。<br/>   - **语音到文本转换（STS）**: 该功能可能指将语音内容从一种语言翻译或转换成另一种语言。<br/><br/>2. **技术栈**：<br/>   - **基于MLX框架**：MLX Audio利用了Apple MLX框架，这为在M1、M2、M3和M4等架构的设备上运行提供了优化。<br/>   - **量化选项**：提供对模型进行量化（如4比特量化）以减小模型大小和提高性能。<br/><br/>3. **依赖与要求**：<br/>   - 需要Python 3.10及以上版本。<br/>   - 苹果硅处理器支持的环境。<br/>   - `ffmpeg`工具用于MP3或FLAC音频编码，必要时需安装此工具。<br/><br/>4. **可用性与扩展性**：<br/>   - 提供了Swift版本（mlx-audio-swift）以在macOS和iOS设备上使用MLX框架进行文本到语音转换。<br/>   - 提供了将模型转换为不同格式（如bfloat16、float32等）的工具，支持上传至Hugging Face Hub。<br/><br/>5. **开发与社区**：<br/>   - 项目遵循MIT许可证。<br/>   - 可通过GitHub浏览器访问源代码：[MLX Audio GitHub页面](https://github.com/Blaizzy/mlx-audio)。<br/><br/>6. **引用指南**：<br/>   - 需要时使用BibTeX格式的引用方式，提供了具体的信息如作者、年份及URL链接以帮助学术和项目管理中的引用。<br/><br/>7. **致谢**：<br/>   - 特别感谢Apple MLX团队开发的MLX框架支持。<br/><br/>总的来说，MLX Audio是一个高度优化和功能丰富的音频处理库，旨在为用户提供方便快捷的文本到语音转换和其他音频处理能力。对于需要在Apple生态系统中构建此类应用或服务的研发人员来说，这是一个值得考虑的选择。 |
| [supermemoryai/supermemory](https://github.com/supermemoryai/supermemory) | Supermemory是一个功能强大的笔记和知识管理工具。它提供了多种方式帮助用户组织、存储和检索信息，并与AI工具集成以提高生产力。<br/><br/>1. **基本操作流程**:<br/>   - **添加记忆**: 用户可以通过添加内容到记忆库，包括文本、图片等。<br/>   - **连接服务**: 与诸如Notion、Google Drive、OneDrive等应用进行同步，方便数据管理。<br/>   - **聊天功能**: 使用内置的聊天功能检索已保存的记忆内容。<br/>   - **AI工具集成**: 连接AI工具（如ChatGPT和Claude）以获得更智能的信息处理体验。<br/><br/>2. **用户支持**:<br/>   - 用户可以通过电子邮件、Discord社区或在线文档寻求帮助和支持。<br/><br/>3. **贡献与开发**:<br/>   - 软件接受来自所有技能水平的开发者贡献，包括但不限于修复bug、添加新功能、改进UI/UX和性能优化等。<br/>   - 开发者可以查看GitHub上的问题页面来查找“good first issue”或“help wanted”的任务开始贡献。<br/><br/>4. **更新与路线图**:<br/>   - 通过查看官方文档中的变更日志了解最新的改动，以及关注其Twitter账号获取更多动态。<br/><br/>Supermemory旨在为用户提供一个全面的知识管理和笔记工具平台，支持个性化需求，并整合AI技术提升效率。 |
# eess.AS updates on arXiv.org
---
| Title | Summary |
| --- | --- |
| [ES4R: Speech Encoding Based on Prepositive Affective Modeling for Empathetic Response Generation](https://arxiv.org/abs/2601.16225) | 贡献点如下：<br/><br/>1. **提出了一种新的框架（ES4R）**：该论文提出了一个名为ES4R的框架，专门用于基于语音的情感响应生成。这个框架旨在解决当前的大型语言模型在处理多轮对话时对情感信息和上下文连贯性处理不足的问题。<br/><br/>2. **明确的结构化情感语境建模**：与现有依赖自动语音识别（ASR）转录或使用编码器提取潜表征的方法不同，ES4R框架的核心创新在于在进行语音编码前明确地模型化了结构化的情感语境。这种方法不依赖于编码器隐式学习或者显式的情绪监督。<br/><br/>3. **双层注意力机制**：引入了一种双层的注意力机制来捕捉对话中的轮次级别情感状态和整体的对话层面情感动态，以此获取更准确的情感表示。<br/><br/>4. **跨模态关注下的文本与语音整合**：通过语音引导的跨模态关注机制将情感代表与文本语义相结合，生成具有共情性的响应。这使得模型能够更好地理解和生成具有情感色彩的对话响应。<br/><br/>5. **基于能量的选择策略和风格融合**：在输出语音时，使用了基于能量的选择策略和风格融合技术来实现共情性语言合成，提升了ES4R框架的表达力和适应性。<br/><br/>6. **多轮性能优势与鲁棒性**：实验结果表明，ES4R在自动评估和人工评价中均表现出优于强基线模型的一致优势，并且具有良好的通用性和稳定性，无论底层的语言模型（LLM）如何变化。 |
| [Zero-Shot Speech LLMs for Multi-Aspect Evaluation of L2 Speech: Challenges and Opportunities](https://arxiv.org/abs/2601.16230) | 贡献点如下：<br/><br/>1. **评估模型性能**：论文通过对Qwen2-Audio-7B-Instruct这一指令调整的语音语言模型（speech-LLM）在5000个Speechocean762发音片段上的零样本测试，评估了其在英语发音评估中的表现。<br/><br/>2. **多维度评分**：该模型能够生成与评分标准相匹配的准确性、流畅性、语调和完整性方面的分数。<br/><br/>3. **一致性验证**：模型产生的评分在+-2的容差范围内与人工评分高度一致，特别是对于高质量语音。<br/><br/>4. **局限性分析**：尽管整体表现良好，但模型在预测低质量发音分数时有过度预测的趋势，并且在错误检测方面缺乏精确度。<br/><br/>5. **潜在应用和改进方向**：论文表明了通过增强提示设计、校准和声学整合等方法来提升语音LLM在大规模发音评估中的潜力。这为未来改善计算机辅助发音训练（Computer-Assisted Pronunciation Training）提供了路径与建议。<br/><br/>这些贡献点强调了使用先进AI技术进行语言学习中发音评估的可能性，并指出了实际应用过程中的改进空间。 |
| [Test-Time Adaptation for Speech Emotion Recognition](https://arxiv.org/abs/2601.16240) | 贡献点如下：<br/><br/>1. **研究背景**：论文指出，语音情感识别（SER）系统的实用性受到了领域转移问题的影响，例如说话者的变化、表现情绪与自然表达之间的区别以及跨数据集的变异。这个问题使得在没有来源数据或目标标记数据的情况下使用传统方法进行适应变得困难。<br/><br/>2. **测试时适应（TTA）**：论文提出了一种名为“Test-time adaptation”的方法，它可以在推理阶段仅使用未标记的目标数据来调整模型。这是对现有领域适应和微调研究的补充，这些方法通常需要源数据或目标数据的标签。<br/><br/>3. **问题聚焦**：虽然已经对领域适应和微调进行了广泛的研究，但它们在SER中解决特定领域转移（如情绪表达的多样性）的能力尚未得到充分调查。论文旨在填补这一空白。<br/><br/>4. **系统评估与比较**：论文首次系统地评估并对比了11种TTA方法在三个代表性的SER任务上的性能。这提供了一个全面的视角，了解不同方法在处理领域转移时的效率和适用性。<br/><br/>5. **方法分类与分析**：研究结果表明，基于反向传播的方法并不是最有效的方法。相反，最小化熵和伪标签法通常失败，因为它们假设存在一个明确且自信的基线标签，而在情感表达中这种假设并不成立，情感表达往往具有内在的模糊性。<br/><br/>6. **方法选择与任务依赖**：论文结论表明，并没有一种方法可以普遍适用于所有情况。其有效性高度依赖于领域转移的具体分布和具体任务特征。<br/><br/>通过这些贡献，该论文为SER领域的TTA技术提供了一个全面评估框架，并指出了未来的研究方向和挑战。 |
| [EdgeSpot: Efficient and High-Performance Few-Shot Model for Keyword Spotting](https://arxiv.org/abs/2601.16316) | 论文的贡献点可概括如下：<br/><br/>### 输入：<br/>1. **创新模型**：引入了名为EdgeSpot的新模型，专门用于边缘设备上的少量样本关键词检测任务。该模型结合了优化后的基于BC-ResNet的声学骨干结构、可训练的通道能量归一化前端以及轻量级的时间域自注意力机制。<br/><br/>2. **知识蒸馏策略**：在训练过程中使用了由子中心ArcFace损失优化的自我监督教师模型进行知识蒸馏，这有助于提高模型的学习效率和泛化能力。<br/><br/>3. **性能提升**：实验结果表明，EdgeSpot模型在整个固定错误警报率（FAR）下提供优于强大的BC-ResNet基线模型的准确性。特别地，EdgeSpot的最大变体在1% FAR下的10-shot准确率从73.7%提高到82.0%，这需要仅消耗29.4M MACs和128k参数。<br/><br/>### 中文总结：<br/><br/>该论文的主要贡献是提出了名为EdgeSpot的新型关键词识别模型，专为边缘设备设计。通过结合优化后的BC-ResNet结构、自监督的知识蒸馏以及轻量级时间域自注意力机制，EdgeSpot在固定的错误警报率下表现出更优的准确性。具体而言，最大的EdgeSpot变体（EdgeSpot-4）在10-shot任务中以1%的错误警报率为82.0%的准确度提供性能提升，并且仅需计算29.4M MACs和拥有128k参数即可实现这一水平。这项研究展示了EdgeSpot模型在效率与性能之间找到了平衡，使得边缘设备上的关键词识别任务更具可行性和实用性。 |
| [TidyVoice: A Curated Multilingual Dataset for Speaker Verification Derived from Common Voice](https://arxiv.org/abs/2601.16358) | 贡献点如下：<br/><br/>1. **多语言演讲者识别数据集的开发**：论文提出了一个名为TidyVoice的新多语言语音识别数据集，该数据集是从Mozilla Common Voice语料库中生成并优化后的。这个数据集解决了当前领域缺乏大规模、公开和多语言数据集的问题，尤其是在读音式样上，对于诸如防欺骗等应用至关重要。<br/><br/>2. **数据集的类型**：TidyVoice数据集被分为两种条件：<br/>   - **Tidy-M（单一语言）**: 包含了来自81种语言的超过21万位单一语言发言者的训练和测试数据。<br/>   - **Tidy-X（跨语言与同语言）**: 包括来自约4500位多语种发言人、在相同语言及跨语言试验中的目标与非目标试验。<br/><br/>3. **模型性能**：论文使用了两种ResNet架构进行实验，并通过精细调整（fine-tuning）在Tidy-M子集上达到了0.35%的EER（错误率）。这种精细调整不仅提高了模型的泛化能力，还能在CANDOR语料库中的未见过的对话采访数据上表现出更好的性能。<br/><br/>4. **开放资源**：论文将完整的TidyVoice数据集、评估试验和所开发的模型公开提供给社区使用，为研究者和开发者提供了新的研究工具和资源。 |
| [FlowSE-GRPO: Training Flow Matching Speech Enhancement via Online Reinforcement Learning](https://arxiv.org/abs/2601.16483) | 贡献点:<br/><br/>1. **创新结合在线强化学习与语音增强**: 首次成功将组相对策略优化（GRPO）这一在线强化学习方法整合到基于流匹配的语音增强框架中。这为后训练期间通过较少迭代步骤对感知和任务导向指标进行调整提供了可能。<br/><br/>2. **适应性算法设计**: 为了适应该研究领域的时间序列特性以及流动生成模型的动力学，对GRPO算法进行了定制化修改。这一设计使算法更加适合处理连续的语音数据。<br/><br/>3. **多指标奖励优化策略**: 针对单一奖励优化可能导致的“奖励滥用”问题（即在评分升高但音频保真度下降的情况下），提出了一个多指标奖励优化策略，通过平衡不同目标之间的冲突来减少过拟合现象，并显著提高整体性能。<br/><br/>4. **理论与实践贡献**: 该研究不仅为基于强化学习的语音增强提供了实验证据，而且还为如何利用强化学习进行生成性音频模型的后训练调整提供了实际操作上的指导。 |
| [SoundBreak: A Systematic Study of Audio-Only Adversarial Attacks on Trimodal Models](https://arxiv.org/abs/2601.16231) | 贡献点如下：<br/><br/>1. **研究威胁模型**：论文关注了一种在真实世界中较未被充分探讨的威胁模型——针对音频-视频-语言（trimodal）多模态模型的非定向、仅基于音频的对抗性攻击。<br/><br/>2. **分析攻击目标**：六种互补的攻击目标被提出和分析，旨在针对多模态处理的不同阶段，包括但不限于音频编码器表示、跨模态注意力、隐藏状态以及输出似然度等关键环节。<br/><br/>3. **实验验证**：在三种最先进的模型上，通过多个基准测试表明，仅基于音频的扰动能够诱导严重的多模态失败现象，最高可达96%的攻击成功率。<br/><br/>4. **低感知失真和优化效果**：研究发现，攻击能够在较低的感知畸变（LPIPS <= 0.08, SI-SNR >= 0）下成功，并且从优化的角度受益更多，相比于数据量的增加。此外，跨模型和编码器的可转移性受限。<br/><br/>5. **不同系统的反应**：特别指出语音识别系统如Whisper主要对扰动幅度作出反应，在严重畸变情况下达到了超过97%的攻击成功率。<br/><br/>6. **揭示新问题并推动研究方向**：这些结果揭示了多模态系统中一个之前未被关注的一元模式攻击面，并激发了对跨模态一致性进行防御的研究需求。 |
| [Contrastive Knowledge Distillation for Embedding Refinement in Personalized Speech Enhancement](https://arxiv.org/abs/2601.16235) | 该论文的贡献点如下：<br/><br/>1. **提出了一种实时改进（On-the-fly Refinement）方法**，通过使用小型说话人编码器在推理过程中对说话者嵌入进行即时优化。这种方法旨在改善个性化语音增强（PSE）的性能。<br/><br/>2. **引入了新型对比知识蒸馏技术**：开发了一个基于复杂嵌入的150,000参数的编码器训练方法，用于培训具有较高质量的说话人编码器，以提高PSE系统的效率和效果。<br/><br/>3. **集成小型说话人编码器**：在增强系统中使用上述编码器进行推理，该方法能显著提升个性化语音增强（PSE）的性能表现，并保持低计算负载。<br/><br/>4. **优化与现有模型融合**：将此方法与上游模型相结合，旨在通过改进嵌入质量来增强已知目标声音的提取能力，特别是在存在干扰声音时。<br/><br/>5. **提高性能同时减少计算需求**：通过采用小型说话人编码器进行实时改进和对比知识蒸馏技术，该论文提出了一个有效的方案，能够在不牺牲太多计算资源的情况下显著提升PSE系统的性能。 |
| [The CMU-AIST submission for the ICME 2025 Audio Encoder Challenge](https://arxiv.org/abs/2601.16273) | 贡献点如下：<br/><br/>1. **系统构建**：基于BEATs模型，开发了一个音频编码器系统，该系统利用了来自不同语音、音乐和声音语料库的74,000小时数据进行扩展，并将架构规模提升至3亿个参数。<br/><br/>2. **预训练策略**：通过实验比较了以语音为中心和平衡的预训练混合策略，研究不同领域对最终性能的影响。<br/><br/>3. **系统组成**：提交的系统由Dasheng 1.2亿模型与两个根据上述预训练数据混合物定制并扩大的BEATs模型组成的集合组成。<br/><br/>4. **增强技术**：提出了一种简单的集成技术，保留了组成模型的最佳能力，并超越了基线和Dasheng 1.2B。<br/><br/>5. **开放科学贡献**：通过将训练检查点公开在huggingface平台（https://huggingface.co/shikhar7ssu/OpenBEATs-ICME-SOUND 和 https://huggingface.co/shikhar7ssu/OpenBEATs-ICME）上，为科学研究提供了便利。 |
| [Auditory Attention Decoding without Spatial Information: A Diotic EEG Study](https://arxiv.org/abs/2601.16442) | ### 贡献点:<br/><br/>1. **提出了一种新的听觉注意力解码（AAD）框架** - 研究中，作者提出了一个在双耳环境下的AAD框架，使用大脑信号如脑电图（EEG）来识别多讲话者环境中被关注的语音流。这为实现智能助听设备以解决鸡尾酒会问题和促进客观听力测试系统提供了关键的技术基础。<br/><br/>2. **解决实际场景中的挑战** - 传统AAD研究主要集中在二分环境，通过将不同的声音信号呈现给左右耳朵来识别方向注意力，而不是语音内容。这种方法依赖于空间信息，在现实世界的场景中（如鸡尾酒会）应用受限。新框架旨在提供在双耳环境中实现完全相同的混音输入，不依赖于空间线索。<br/><br/>3. **多模态数据融合** - 研究采用了一种方法将EEG和语音信号映射到共享的潜在空间，使用独立编码器完成这一任务。通过wav2vec 2.0提取语音特征，并用两层1D卷积神经网络（CNN）进行编码，同时利用BrainNetwork架构对EEG数据进行编码。<br/><br/>4. **准确度提升** - 在评估方法时，研究者在双耳EEG数据集上进行了测试，结果显示该方法的准确性为72.70%，相比于基于方向的传统AAD方法提高了22.58%。这表明了新框架在识别被关注语音方面的显著优势。<br/><br/>### 中文总结：<br/><br/>本文通过提出一种新的AAD框架，解决了传统AAD研究中主要局限于二分环境、依赖于空间信息的问题，并在实际应用如鸡尾酒会场景等进行了改进和优化。该框架成功地融合了EEG和语音信号的数据，利用先进的编码方法提高了模型的识别准确率，特别是在双耳环境中使用完全相同的混音输入情况下。通过对比实验，新框架的表现优于传统基于方向的AAD方法，展示了在多讲者环境下的显著提升空间。 |
| [Do Models Hear Like Us? Probing the Representational Alignment of Audio LLMs and Naturalistic EEG](https://arxiv.org/abs/2601.16540) | 贡献点如下：<br/><br/>1. **跨模型比较**：研究系统地将12个开源音频大语言模型（Audio Large Language Models, A-LLMs）的层内表示与电生理记录的数据集中的脑电信图（Electroencephalogram, EEG）信号进行比较。这种方法提供了关于A-LLMs内部表征与自然听觉中人类神经动力学之间的关系的研究。<br/><br/>2. **相似性度量方法**：采用包括Spearman相关性为基础的代表相似性分析（Representational Similarity Analysis, RSA）在内的8种相似度指标来描述句子内的表示几何结构。这种方法为评估A-LLMs与人脑响应的匹配程度提供了一套全面且标准化的方法。<br/><br/>3. **关键发现**：<br/>   - **排名依赖性分裂**：发现不同相似性度量方法下模型的排名存在显著差异，表明不同的分析视角可能揭示出不同的模型性能特征。<br/>   - **深度相关的时间空间对齐模式**：识别到了与深度相关的时空对齐模式，包括深度依赖的峰值和在250-500毫秒时间段内RSA的明显增加。这一发现与N400相关的大脑活动相吻合，提供了关于A-LLMs如何处理语言信息的时间序列分析。<br/>   - **情感分离**：观察到情感分离现象，即负向语调（通过提出的三模态邻域一致性（Tri-modal Neighborhood Consistency, TNC）准则识别）降低了几何相似性的同时增加了基于协方差的依赖关系。这表明A-LLMs在处理具有不同情感色彩的语言内容时可能存在特定的表征机制。<br/><br/>4. **神经生物学见解**：这些发现提供了关于A-LLMs代表机制的新神经生物学洞察，揭示了它们在语音感知与语言理解集成方面的能力与人类大脑响应之间的复杂关系。 |
| [CORD: Bridging the Audio-Text Reasoning Gap via Weighted On-policy Cross-modal Distillation](https://arxiv.org/abs/2601.16547) | ### 贡献点:<br/><br/>1. **理论假设与问题定位**：<br/>   - 提出当前LALMs（大型音频语言模型）在知识和推理能力方面存在退化现象，其根源可能是训练范式未能有效跨越特征表示空间中的声学-语义差距。<br/><br/>2. **解决方案的引入**：<br/>   - **CORD框架**：提出了一种统一的跨模态对齐框架(CORD)，旨在在线上实现跨模态自我蒸馏。该框架通过将音频条件推理与文本条件的等价形式在同一模型内进行对齐，来解决上述挑战。<br/><br/>3. **多粒度对齐策略**：<br/>   - 对齐方式分为两个层次：**token级**采用基于策略的反向KL散度和重要性感知加权方法，优先处理早期且语义上关键的令牌；**序列级**引入由裁判驱动的全局奖励机制，通过Group Relative Policy Optimization (GRPO)优化完整的推理轨迹。<br/><br/>4. **实验验证与结果**：<br/>   - 通过在多个基准测试中的实证结果显示，CORD能够一致地增强音频条件下的推理能力，并仅需80,000个合成训练样本就显著缩小了音频-文本性能差距。这证明了其策略导向、多层次跨模态对齐方法的有效性和数据效率性。<br/><br/>### 总结：<br/>该论文针对LALMs在知识和推理能力方面的局限性，通过引入CORD框架提供了一种创新的解决方案。通过实现统一的跨模态自适应学习过程，特别是在音频与文本之间的多粒度对齐策略上，CORD不仅验证了其在提升特定场景下的音频条件推理能力的有效性，还展现出了相对较低训练样本需求的优势，从而有效地缩小了音频与文本处理间的性能差距。 |
| [Omni-directional attention mechanism based on Mamba for speech separation](https://arxiv.org/abs/2601.16603) | 贡献点如下：<br/><br/>1. **提出Mamba模型**：Mamba是一种选择性的状态空间模型（SSM），它作为Transformer的高效替代方案在语音建模中崭露头角，能够以线性复杂度处理长序列。<br/><br/>2. **现有方法的局限性**：现有的语音分离方法通常沿单一维度将输入分解为短的一维序列进行处理，这限制了它们对二维频谱图上的全局依赖性的捕捉能力。<br/><br/>3. **提出多方向注意力（OMA）机制**：在单向Mamba的基础上构建出一种高效的全向注意力（OA）机制，该机制能够从频谱图的十个不同方向上建模全局依赖性。这使得模型能够在二维空间中更好地理解信号间的复杂关系。<br/><br/>4. **扩展到分离模型**：将提出的多方向注意力机制融入到两个基准分离模型之中，并在三个公共数据集上进行评估。<br/><br/>5. **实验结果**：研究结果显示，与基线方法相比，所提出的方法在保持线性复杂度的同时始终能获得显著的性能提升，并且超越了现有最先进的系统。 |
| [I Guess That's Why They Call it the Blues: Causal Analysis for Audio Classifiers](https://arxiv.org/abs/2601.16675) | 贡献点如下：<br/><br/>1. **创新方法的引入**：论文提出了一个新的利用因果推理的方法，旨在发现用于特定音频分类的频率空间中的充分且必要的特征。这一方法为理解音频分类器的工作原理提供了新的视角。<br/><br/>2. **工具FREQREX的开发**：通过实现这个算法到名为FREQREX的工具中，作者不仅提供了一种实际应用该方法的方式，也为其他研究者和实践者提供了探索音频分类过程的新手段。<br/><br/>3. **实验验证**：论文在标准基准数据集上进行了实证研究，结果显示因果充分且必要的子集能够通过微小的输入变化操纵模型输出。具体来说，对240,000个频率中的一个进行轻微调整，有58%的情况下会导致分类结果的变化，并且这种改变可能非常细微以至于人类难以察觉。<br/><br/>4. **实际应用价值**：这些实验不仅验证了因果分析在理解音频分类器推理过程的有效性，还展示了这种方法在成功操纵模型输出方面的潜力。这一发现对于增强安全性、提高透明度以及潜在的对抗攻击防御等方面具有重要意义。 |
| [E2E-AEC: Implementing an end-to-end neural network learning approach for acoustic echo cancellation](https://arxiv.org/abs/2601.16774) | 贡献点如下：<br/><br/>1. **新型端到端音频回声消除方法**：提出了一个基于神经网络的端到端音频回声消除（E2E-AEC）方法，该方法能够在流式推理中运行，并且在不依赖传统线性回声消除（LAEC）技术和时间延迟估计的情况下有效工作。<br/><br/>2. **进步学习策略的应用与优化**：引入并改进了渐进式学习（progressive learning），这是一种逐步增强回声抑制效果的方法。这种方法通过分阶段提高模型的性能，使其适应和优化音频处理的过程。<br/><br/>3. **知识迁移的利用**：将预先训练好的LAEC模型作为初始值进行初始化，利用从LAEC训练过程中获得的知识进行特征提取和模式识别，以此来提升整体模型在处理音频回声时的表现。<br/><br/>4. **注意力机制的优化**：通过在注意力权重上应用损失函数对注意力机制进行优化，从而实现参考信号与麦克风信号之间精确的时间对齐。这一策略帮助了更好地捕捉声音序列中的时间同步问题。<br/><br/>5. **语音活动检测（VAD）集成**：引入了语音活动检测功能，以增强语音质量并改进回声消除效果。当远端的说话人在讲话时，通过掩蔽网络输出的方式，有效地过滤掉了背景噪声和回声。<br/><br/>6. **实验验证**：通过在公共数据集上进行的实验证明了该方法的有效性，提供了实际应用中性能提升的证据。这一验证步骤为方法的实用性提供了科学依据。 |
| [A Novel Transfer Learning Approach for Mental Stability Classification from Voice Signal](https://arxiv.org/abs/2601.16793) | 1. **创新的跨领域学习方法**：论文提出了一种用于基于人类语音信号进行心理健康状态分类的新颖转换学习方法，以应对数据量有限的问题。<br/><br/>2. **使用卷积神经网络（CNN）分析**：采用了卷积神经网络来分析从录音中生成的频谱图图像，探索了在心理健康状态分类中的应用可能性。<br/><br/>3. **三种CNN架构评估**：对VGG16、InceptionV3和DenseNet121等三种CNN结构进行了实验性比较研究，并应用于非增强数据集、增强数据集以及转换学习阶段。<br/><br/>4. **转换学习策略**：通过预训练模型在增强的数据集上，然后在原始非增强数据集上进行微调，确保严格的数据分离以防止数据泄漏，形成了一种有效的转换学习策略。<br/><br/>5. **显著的性能提升**：与基线方法相比，实验结果表明了显著提高的分类性能。DenseNet121在这项研究中表现最优，达到了94%的准确率和99%的AUC得分。<br/><br/>6. **数据增强结合转换学习的优势**：结合数据增强和转换学习，可以提升基于频谱图的CNN心理健康状态分类效果，提供了一种用于精神健康诊断的非侵入性工具的前景。 |
| [Lightweight Implicit Neural Network for Binaural Audio Synthesis](https://arxiv.org/abs/2509.14069) | 贡献点如下：<br/><br/>1. **提出轻量级隐式神经网络（Lite-INN）** - 一种新颖的两阶段框架，专门用于高保真双耳音频合成。该框架旨在解决现有方法对计算资源要求高的问题，限制了它们在边缘设备上的应用。<br/><br/>2. **初步估计阶段** - Lite-INN通过时间域扭曲生成初始估计值，这是一种快速的预处理步骤，为后续的细化过程提供起点。<br/><br/>3. **隐式双耳校正模块（IBC）** - 一个隐式神经网络模块，用于预测幅度和相位修正。它在第一阶段初步估计的基础上进行精炼，最终输出高质量的音频数据。<br/><br/>4. **高度紧凑的模型架构** - IBC模块直接预测幅度和相位修正，从而构建了一个结构紧凑且参数量小的模型，相比其他方法，具有显著的优势。<br/><br/>5. **实验结果** - Lite-INN在感知质量上与最佳基准模型具有统计学上的可比性，并在计算效率上实现了显著提升。相较于前一代最先进的方法（NFS），Lite-INN减少了72.7%的参数量，并要求进行了大量减少的计算操作（MACs）。<br/><br/>6. **解决综合质量和计算效率之间的权衡** - 通过设计，Lite-INN有效地解决了合成质量与计算效率之间的折衷问题，为高保真边缘设备空间音频应用提供了新的解决方案。 |
| [A Lightweight Fourier-based Network for Binaural Speech Enhancement with Spatial Cue Preservation](https://arxiv.org/abs/2509.14076) | ### 贡献点：<br/><br/>1. **问题识别**：论文首先指出，二声道语音增强领域面临一个严重的技术挑战，即在实现先进性能的同时需要高度计算密集型的架构，而轻量级解决方案往往伴随着显著性能降级。<br/><br/>2. **解决方案引入**：为解决这一难题，作者提出了全局自适应傅里叶网络（GAF-Net），这是一种旨在在性能和计算效率之间建立平衡的轻量级深度复数网络。<br/><br/>3. **GAF-Net架构**：<br/>   - 第一阶段：采用结合短时傅里叶变换和Gamma tone特征的双功能编码器，增强音频表示的鲁棒性。<br/>   - 第二阶段：使用通道无关的全局自适应傅里叶调制器高效捕捉长期时间依赖关系，同时保留空间线索。<br/>   - 最终阶段：实现动态门控机制以减少处理伪像。<br/><br/>4. **实验结果**：<br/>   - GAF-Net在保持较少参数和计算成本的同时，实现了与现有技术相竞争的性能。<br/>   - 特别是在二声道提示（ILD和IPD误差）和客观可懂度（MBSTOI）方面表现出色。<br/>   <br/>5. **结论**：这些结果验证了GAF-Net提供了一种在资源受限设备上实现高保真二声道处理的可行方式。 |
| [Frame-Stacked Local Transformers For Efficient Multi-Codebook Speech Generation](https://arxiv.org/abs/2509.19592) | ### 贡献点：<br/><br/>1. **探索基于大型语言模型（LLMs）的语音生成模型**：该论文专注于研究如何处理在离散声码器上进行操作的语言生成模型，这些模型与文本令牌存在根本差异。它们的多编码本结构意味着预测过程需要在每个时间步联合预测N个代码库条目，引入了挑战性依赖。<br/><br/>2. **提出并分析层级策略**：为了解决上述预测之间的依赖关系带来的挑战，论文研究了采用局部变换器（LT）的方法来细化预测和捕获时间步骤内部的依赖性。这一方法结合了传统的并行预测和迭代预测技术，以提高解码效率和保真度。<br/><br/>3. **深入研究两种LT架构**：提出了两种不同的局部变换器设计作为实验对象——一种是顺序生成代码库的自回归转换器，另一种基于MaskGIT的实现，采用迭代掩蔽预测。这两种设计都支持帧堆叠功能，允许主模型同时预测多个帧，并由LT解码其对应的编码本。<br/><br/>4. **性能分析和策略选择**：通过广泛的实验证明了并行采样与迭代采样的权衡关系，特别是在不同处理速度和质量要求下的表现。这些结果有助于更好地理解各种配置下性能和效率之间的关系。<br/><br/>5. **提出实用指导原则**：根据部署优先事项（如计算效率和合成保真度）提供了一套实用性选择解码策略的指南，为实际应用提供了可操作的建议。 |
| [Enhanced Generative Machine Listener](https://arxiv.org/abs/2509.21463) | 贡献点如下：<br/><br/>1. **提出GMLv2模型**：GMLv2是一个基于参考的音频质量预测模型，用于根据MUSHRA评分预测主观音频质量。<br/><br/>2. **引入Beta分布损失函数**：GMLv2使用了基于Beta分布的损失函数来建模听众评级，这有助于更精确地捕捉和评估音频质量。<br/><br/>3. **整合神经音频编码（NAC）主体性数据集**：通过集成额外的NAC主观数据集，GMLv2增强了其在不同内容类型和编解码器配置下的泛化能力和适用性。<br/><br/>4. **全面性能评估**：GMLv2在多样化的测试集中进行了广泛的评估，显示其在与主观评分的相关性和稳定预测能力上均优于广泛使用的指标（如PEAQ和ViSQOL）。<br/><br/>5. **提供感知音频质量评估的可扩展和自动化框架**：GMLv2提供了用于感知音频质量评价的可扩展且自动化的体系架构，有助于加速现代音频编码技术的研究与开发。 |
| [Speaker Anonymisation for Speech-based Suicide Risk Detection](https://arxiv.org/abs/2509.22148) | ### 贡献点:<br/><br/>1. **首次系统研究基于语音的自杀风险检测中的说话者匿名化问题** - 论文专注于解决青少年自杀这一全球性健康问题，探讨了在自动自杀风险识别中保护说话者身份的重要性。特别是在数据泄漏或恶意滥用的情况下，语音内容可能会泄露个人可识别信息。<br/><br/>2. **广泛调查多种匿名化方法** - 包括基于传统信号处理技术、神经声音转换和语音合成等方法，以探索如何在最小影响自杀风险检测效果的前提下保护说话者身份。<br/><br/>3. **建立全面评估框架** - 该论文构建了一个综合评估体系，用于衡量保护说话者身份与保留对自杀风险识别至关重要的信息之间的权衡关系。这为不同匿名化策略的效果提供了明确的评价标准。<br/><br/>4. **发现组合使用保留互补信息的匿名化方法** - 结果表明，通过结合能够保持补充信息的不同匿名化技术，可以实现与原始语音相当的检测性能，并同时确保对脆弱群体说话者身份的安全保护。 |
| [Audio dequantization using instantaneous frequency](https://arxiv.org/abs/2510.16813) | ### 贡献点:<br/><br/>1. **提出了一种新的去量化方法**：“我们提出了一个利用感知相位的正则化方法，该方法最初成功应用于音频填补问题。这种方法在时频表示中促进了音频信号的瞬时连续性，并避免了以l1为基础的正则化方法中常见的能量损失副作用。”<br/><br/>2. **开发了一种新型去量化工具**：通过提出名为“相感知音频去量化器（Phase-Aware Audio Dequantizer，简称PHADQ）”，该研究提供了解决音频领域特定问题的新策略。<br/><br/>3. **理论基础与实践应用的结合**：该方法基于对时频表示中瞬时连续性的促进，并避免了能量损失的问题。这表明在实际应用上能够有效提升音频信号的质量和连贯性。<br/><br/>4. **评价标准明确**：“通过SDR（信号失真率）和PEMO-Q ODG目标指标评估，以及类似的MUSHRA听觉测试进行主观评测”，该研究提供了系统且全面的评价方式，用于检验方法的有效性和实际应用效果。这为其他研究人员提供了一个可参考的标准框架。<br/><br/>5. **创新性与实用性并重**：结合了理论研究和实践应用的双重价值，PHADQ不仅在理论上解决了去量化过程中的挑战，同时通过客观和主观测试证实了其实用性及在音频处理领域的潜在应用。 |
| [WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning](https://arxiv.org/abs/2509.04744) | ### 贡献点:<br/><br/>1. **WildScore基准库的建立**: 本文作者引入了WildScore，这是首个针对自然世界中的符号音乐推理与分析的多模态基准库。该基准旨在评估大规模语言模型在理解实际音乐乐谱和回答复杂音乐学问题方面的能力。<br/><br/>2. **丰富的数据源**: WildScore的数据集来源于真实的音乐作品，并配有真实用户生成的问题和讨论，能够捕捉到实践音乐分析的细节和复杂性。<br/><br/>3. **系统分类与词汇表**: 作者提出了一种系统分类方法，包括高层次和细粒度的音乐学本体论，为评估大规模语言模型在符号音乐理解方面的性能提供框架。<br/><br/>4. **将复杂音乐推理转化为选择题形式**: 将复杂的音乐推理问题设置为多选题的形式，这使得对这些模型进行可控且可扩展的评估成为可能。<br/><br/>5. **基准测试与发现**: 对当前最先进的大规模语言模型在WildScore上的表现进行了基准测试，揭示了它们在视觉-符号推理方面的模式，并指出了在符号音乐推理和分析中的有前景的方向以及持续存在的挑战。<br/><br/>6. **公开数据集和代码**: 作者发布了该数据集和相关代码，供研究社区进一步探索和利用。 |
| [SONAR: Self-Distilled Continual Pre-training for Domain Adaptive Audio Representation](https://arxiv.org/abs/2509.15703) | 贡献点如下：<br/><br/>1. **提出SONAR框架**：论文引入了名为SONAR（自蒸馏持续预训练以领域适应性音频表示）的持续预训练体系，旨在利用BEATs的基础进行改进。这一框架专为处理新域数据和避免灾难性遗忘提供了解决方案。<br/><br/>2. **创新的联合采样策略**：SONAR采用了综合考虑新数据和已有数据的联合采样方法，以此来适应新的领域环境。这种策略有助于模型同时学习到新信息与保留过往的知识。<br/><br/>3. **平衡特定性和一般性的正则化**：通过在预训练过程中应用正则化技术，SONAR旨在实现特定场景下的精准识别和广泛应用场景下的泛化能力之间的有效平衡。这有助于提升模型对不同领域数据的适应性。<br/><br/>4. **动态扩展分词代码本**：面对新出现的声学模式，SONAR设计了一种机制来动态增加分词代码本，从而更高效地捕捉和表示这些独特的音频特征。<br/><br/>5. **多域实验验证**：论文通过在四个不同的领域上进行广泛实验，证明了SONAR方法能够实现高适应性和强大的遗忘抵抗能力。这表明该框架具有广泛的适用性和稳定性。<br/><br/>总之，SONAR框架的贡献主要体现在提供了一种高效、灵活且具有高适应性的自监督学习方法，用于处理大规模音频数据集中的持续域适应问题，并通过实验验证其在不同领域内的有效性和鲁棒性。 |
| [Etude: Piano Cover Generation with a Three-Stage Approach -- Extract, strucTUralize, and DEcode](https://arxiv.org/abs/2509.16522) | 论文的贡献点如下：<br/><br/>1. **Etude模型介绍**：提出了一种名为"Etude"的三层架构，包括Extract（提取）、strucTUralize（结构化）和DEcode（解码）阶段。该模型旨在解决钢琴盖生成中的结构一致性问题。<br/><br/>2. **节奏信息的预提取与简化处理**：通过预先提取节奏信息并采用基于REMI的新型、简化的分词方式，Etude能够产生保持原曲结构的钢琴版乐谱。<br/><br/>3. **增强音乐流畅性与动态效果**：该模型生成的音乐作品在整体质量上更胜一筹，同时增强了音乐的流畅性和动态表现。<br/><br/>4. **高度可控制的风格注入**：通过风格注入机制，Etude支持对生成过程的高度可控性，允许用户调整生成结果以符合特定风格或偏好。<br/><br/>5. **主观评估的优越性能**：基于人类听者的主观评估，论文表明Etude在质量上显著优于先前模型，并达到了与专业作曲家相媲美的水平。 |
| [Adaptive Multimodal Person Recognition: A Robust Framework for Handling Missing Modalities](https://arxiv.org/abs/2512.14961) | ### 贡献点:<br/><br/>1. **提出了一种综合多模态的人员识别框架** - 该研究引入了一个基于手势作为情境增强的多模态人员识别系统，用于补充传统的语音和面部等传统模态。<br/><br/>2. **采用了统一混合融合策略** - 研究使用了在特征级别和评分级别上集成信息的方法，以最大化表示的丰富性和决策准确性。这包括独立处理各模态，随后使用交叉注意力和门控融合机制进行整合。<br/><br/>3. **采用多任务学习与动态适应缺失数据的策略** - 通过多任务学习分别对各模态进行处理，并通过动态调整策略来适应可能缺失的数据，确保在单一分类头的情况下也能实现最优性能，尤其适用于单模态或双模态场景。<br/><br/>4. **提出了一个新的面试为基础的多模态数据集CANDOR** - 研究团队首次在这个数据集上进行了基准测试，用于评估人员识别任务中的多模态系统。证明了他们的方法在人员识别任务中能够达到99.51%的第一位准确率。<br/><br/>5. **在VoxCeleb1数据集上的表现** - 该模型在双模态模式下达到了99.92%的准确性，超越了传统的解决方案，并且即使在缺少一种或两种模态的情况下，系统仍能保持高精度，证明其在实际应用中的鲁棒性。<br/><br/>6. **代码和数据的公开可用性** - 研究团队提供了用于验证他们方法的有效性的代码和数据集，促进了社区对这些技术的进一步研究和开发。 |
