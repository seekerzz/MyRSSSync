# 五里墩茶社
---
| Title | Date | Summary |
| --- | --- | --- |
# 小工蚁创始人
---
| Title | Date | Summary |
| --- | --- | --- |
# AIGCLINK
---
| Title | Date | Summary |
| --- | --- | --- |
# 技术爬爬虾
---
| Title | Date | Summary |
| --- | --- | --- |
# 秋芝2046
---
| Title | Date | Summary |
| --- | --- | --- |
# GitHub All Languages Daily Trending
---
| Title | Summary |
| --- | --- |
| [BloopAI/vibe-kanban](https://github.com/BloopAI/vibe-kanban) | Vibe Kanban 是一款开源的代码协作平台，用于团队进行代码审查和协作开发。以下是其关键特点：<br/><br/>1. **集成工具**：Vibe Kanban 通过与 Git、GitHub 和其他版本控制系统集成，提供无缝的代码管理体验。<br/><br/>2. **多语言支持**：它支持多种编程语言，并提供了为这些语言定制代码检查规则的能力。<br/><br/>3. **自动代码审核**：自动运行代码质量检查和静态分析工具（如 ESLint 或 Pylint），帮助开发者发现潜在问题并提高代码质量。<br/><br/>4. **协作功能**：允许团队成员评论代码、添加标签、设置优先级，并通过集成的讨论板进行交流。<br/><br/>5. **自动化流程**：可以通过集成 Jenkins 或其他 CI/CD 工具实现持续集成和自动部署，简化开发过程。<br/><br/>6. **API 可用性**：提供了 RESTful API 接口供外部系统或脚本调用，增强其可编程性和自定义能力。<br/><br/>7. **定制化设置**：允许根据团队需求调整平台的外观、功能及工作流程。<br/><br/>8. **远程部署支持**：通过 Cloudflare Tunnel 或类似服务简化在远程服务器上运行 Vibe Kanban 的过程，并确保易于访问和集成到现有开发环境中。<br/><br/>9. **性能优化**：为提高用户体验，提供了各种选项来调整应用的配置设置。<br/><br/>Vibe Kanban 旨在帮助团队更高效地协作、改进代码质量和提升整体开发流程。 |
| [openai/openai-cookbook](https://github.com/openai/openai-cookbook) | 该文档提供了使用OpenAI API进行常见任务的示例代码和指南，包括设置API密钥、运行示例所需的环境变量配置等。适用于多种编程语言，并附有其他相关资源链接。 |
| [afkarxyz/SpotiFLAC](https://github.com/afkarxyz/SpotiFLAC) | 一个免费工具，无需账号即可将Spotify音乐转码为真正的FLAC格式，支持Tidal、Qobuz与Amazon Music。兼容Windows、macOS和Linux系统。 |
| [google-gemini/computer-use-preview](https://github.com/google-gemini/computer-use-preview) | 使用Gemini模型的`computer_use`API和Playwright或Browserbase环境，可以实现根据自然语言指令执行浏览器操作。以下概要介绍了关键步骤、技术要点、问题解决方案以及支持的功能：<br/><br/>1. **环境选择**：<br/>   - **Playwright**: 提供本地操作系统集成能力，但可能在某些OS上无法捕获下拉菜单。<br/>   - **Browserbase**: 可替代Playwright，尤其适用于需要渲染非标准或复杂UI元素的场景。<br/><br/>2. **API配置**：<br/>   - 需要为Gemini模型提供有效的API密钥，并根据所选环境（Playwright或Browserbase）配置相应的访问密钥和项目ID。<br/><br/>3. **自然语言指令执行**：<br/>   - 使用`--query`参数指定要执行的操作，如打开链接、滚动页面或模拟点击等。<br/>   - 支持特性包括截图高亮显示（适用于开发调试）、初始URL加载（启动时自动导航）等。<br/><br/>4. **注意事项**：<br/>   - Playwright在某些OS上对下拉菜单的捕获有局限性，可考虑使用Browserbase作为替代方案。<br/>   - 为解决Playwright的局限性，可以通过注入脚本来定制非标准`<select>`元素的表现（如[proxy-select](https://github.com/amitamb/proxy-select)）。<br/><br/>5. **环境变量**：<br/>   - `GEMINI_API_KEY`, `BROWSERBASE_API_KEY`和`BROWSERBASE_PROJECT_ID`确保API认证和服务访问。<br/><br/>综上所述，通过合理选择执行环境、正确配置API密钥以及优化自然语言指令的输入，可以高效地利用Gemini模型自动化浏览器任务。在遇到特定技术挑战时（如下拉菜单处理），采用针对性解决方案（如脚本注入或服务替代）是关键策略。 |
| [organicmaps/organicmaps](https://github.com/organicmaps/organicmaps) | Organic Maps是一款开源的离线地图应用，提供了全球173个国家和地区的免费高精度离线地图数据。该应用允许用户在无需网络连接的情况下使用地图进行导航、查看地点信息、获取路线规划等。以下是关于该应用的一些关键点：<br/><br/>- **功能**：提供离线地图、搜索地点、查看照片、添加备注、计算距离和时间，以及在线更新地图。<br/><br/>- **开源社区**：基于Apache 2.0许可协议，并遵循CNCF代码规范。欢迎开发者参与贡献，包括报告错误、提出改进建议或提交代码更改。<br/><br/>- **贡献方式**：用户可以通过多种途径提供反馈和支持，如在应用商店评级、GitHub上star项目、报告问题、加入Telegram频道和矩阵空间进行更新订阅等。<br/><br/>- **白标合作**：有兴趣进行品牌定制或者使用其服务器的合作伙伴应事先联系官方邮箱`legal@organicmaps.app`。<br/><br/>- **代码贡献**：对项目感兴趣的开发者可以通过提交代码更改或问题修复来参与，详细指南可以在[CONTRIBUTING.md](https://raw.githubusercontent.com/organicmaps/organicmaps/master/docs/CONTRIBUTING.md)文件中找到。<br/><br/>- **反馈与支持**：用户可以加入Beta测试计划、在应用商店提供反馈、关注Mastodon、Facebook、X（Twitter）、Instagram等社交平台的更新，并通过邮箱`hello@organicmaps.app`联系官方获取帮助。<br/><br/>Organic Maps项目致力于为全球用户提供免费和开放的地图服务，鼓励社区合作和贡献。 |
| [nocodb/nocodb](https://github.com/nocodb/nocodb) | NocoDB是一个开源的数据库平台，旨在提供一个强大且易于使用的界面来管理和操作数据。其目标是让每个人都能以快速和简单的方式与复杂的数据进行交互。<br/><br/>**主要特性及功能**：<br/><br/>1. **丰富的表格接口**：<br/>   - 基本操作：创建、读取、更新和删除表、列和行。<br/>   - 字段操作：排序、过滤、分组、显示/隐藏列。<br/>   - 多种视图类型：网格（默认）、相册、表单、看板和日历视图。<br/>   - 视图权限类型：协作视图和锁定视图。<br/>   - 基于角色的访问控制：细粒度的数据访问权限管理。<br/><br/>2. **集成商店**：<br/>   提供自动化工作流与各种平台集成，如聊天工具（Slack、Discord等）、电子邮件服务（AWS SES、SMTP等）以及存储服务（AWS S3、Google Cloud Storage、Minio等）。<br/><br/>3. **程序化访问**：<br/>   支持REST APIs和NocoDB SDK来通过API调用进行数据操作，包括使用JWT或社交认证令牌进行授权。<br/><br/>4. **开源与社区驱动**：项目遵循AGPLv3许可，并且有活跃的贡献者社区参与开发和改进。<br/><br/>NocoDB致力于提供开放源码的数据库解决方案给全球每一个在线业务，旨在为用户提供一个强大而灵活的数据处理工具。其使命是打破大型企业级数据库的垄断，降低使用复杂数据管理工具的门槛，让更多的开发者和用户能够自由地在互联网上探索、构建新应用和功能。<br/><br/>通过NocoDB，目标实现的是将强大的数据库技术普及到更广泛的用户群体中去，激发更多的人参与在线创新。 |
| [resemble-ai/chatterbox](https://github.com/resemble-ai/chatterbox) | Chatterbox-TTS是一个由Resemble AI开发的文本转语音(TTS)工具，它利用了多种先进的人工智能技术来生成自然、流畅且高质量的声音。以下是其主要特点和功能：<br/><br/>1. **多语言支持**：Chatterbox-TTS支持包括中文在内的25种语言，满足不同场景的需求。<br/><br/>2. **水印技术**：每个生成的音频都包含有Resemble AI的Perth（感知阈值）水印，用于防止版权侵犯并检测未经授权的使用。<br/><br/>3. **灵活配置参数**：<br/>   - `cfg_weight`：控制声音和参考样本之间的一致性。<br/>   - `exaggeration`：调整语音的表达力和情感色彩。<br/><br/>4. **内置文本预处理**：包含对输入文本的自动断句、分词和情感分析，提升生成音频的质量。<br/><br/>5. **社区与支持**：<br/>   - 官方Discord群组提供技术支持和讨论。<br/>   - 公开的GitHub仓库允许用户贡献代码或获取帮助。<br/><br/>6. **责任AI**：遵循透明度原则，确保技术使用符合伦理标准。<br/><br/>7. **官方引用格式**：提供了适当的学术引用指南，以便在论文或项目中正确提及Chatterbox-TTS。<br/><br/>8. **模型训练数据来源**：明确指出模型基于公开可获取的数据集进行训练，并建议用户使用时应避免涉及不适当内容。<br/><br/>Chatterbox-TTS旨在为开发者、音频设计师和任何需要语音合成的领域提供强大且灵活的工具，同时强调负责任地应用技术。 |
| [sansan0/TrendRadar](https://github.com/sansan0/TrendRadar) | 您发布的文档包含了关于TrendRadar项目的综合信息，以下是对关键部分的中文概括：<br/><br/>- **项目概览**：这是一个自动化热点新闻监测与推送系统，目标是帮助用户获取特定关键词或主题的实时排名和报道。系统能够从12个社交媒体和在线平台中收集信息，并通过自定义设置（如通知方式、运行模式等）个性化服务。<br/><br/>- **部署选项**：文档提供了两种主要的部署方法——云端部署（Fork项目到GitHub并配置通知渠道）和本地部署（使用Docker）。用户还可以选择不同通知途径，包括企业微信、飞书、钉钉等，并配置关键词与优先级参数来调整筛选结果。<br/><br/>- **操作流程图**：展示了从用户开始至系统自动运行的整个过程。流程包括数据收集、关键词筛选、权重算法排序以及生成报告和推送通知，最后达到持续精准推送的目标。<br/><br/>- **许可证信息**：项目遵循GPLv3.0许可条款，允许自由分发、修改及共享源代码，并要求任何衍生作品在公有领域下使用时同样遵守该许可证。<br/><br/>此文档旨在为TrendRadar项目提供全面的指导和介绍，帮助潜在用户快速了解并开始使用这个工具进行个性化的内容监控与推送服务。 |
| [harvard-edge/cs249r_book](https://github.com/harvard-edge/cs249r_book) | 这是一份ML系统书的作者名单和贡献者列表。它详细列出了所有参与编写、编辑或提供重要贡献的个人，这些贡献包括代码实现、示例、评论、设计决策等。这份名单以表格形式展示，并分为两部分：前42行是根据作者名字排序的基本作者名单，接下来的部分则是按贡献大小排序的贡献者列表。<br/><br/>每位作者的名字后面还附有相应的GitHub链接和邮箱地址，方便读者进一步了解他们的工作或与他们交流。此外，有一个特别强调支持此工作的呼吁，鼓励大家在GitHub上给项目Star，并订阅相关的邮件列表、加入讨论组或者访问网站了解更多内容。<br/><br/>总的来说，这份名单不仅记录了ML系统书背后团队的集体智慧和努力，也提供了一个联系和参与这个社区的方式。通过这样的公开透明的方法，作者们增强了书籍与读者之间的联系，并促进了学术交流和技术分享的精神。 |
| [timescale/pg-aiguide](https://github.com/timescale/pg-aiguide) | pg-aiguide是一个用于PostgreSQL数据库的AI助手，旨在帮助开发者和数据库管理员解答问题、设计模式和优化性能。主要特点包括：<br/><br/>1. **语义搜索（MCP工具）**：它提供了对官方PostgreSQL文档的语义搜索功能，并且可以聚焦于特定版本的Postgres，同时也搜索Tiger Data的数据库相关文档，包括TimescaleDB及其扩展。<br/><br/>2. **技能模块**：pg-aiguide包含了预设的AI优化的最佳实践技能，用于指导schema设计、索引策略、数据类型选择、完整性约束、命名规范和性能调优。这些技能有助于开发者遵循最佳实践来编写更有效的PostgreSQL代码。<br/><br/>3. **生态系统文档支持**：目前支持TimescaleDB，并且将添加对pgvector和PostGIS等其他PostgreSQL生态组件的文档支持，以提供全面的技术资源和服务。<br/><br/>4. **开发与贡献**：开发者可以使用DEVELOPMENT.md指南进行本地MCP服务器运行、新增技能和文档。同时鼓励社区参与，包括提交新的最佳实践技能、更多相关文档、改进搜索功能以及报告问题或提出新功能建议。<br/><br/>pg-aiguide遵循Apache 2.0许可协议，这允许用户自由地获取、修改、复制、分发、展示或者在此基础上进行衍生作品的创作和分享。通过这些特性，pg-aiguide旨在为PostgreSQL生态系统提供一个强大的AI辅助工具集。 |
# eess.AS updates on arXiv.org
---
| Title | Summary |
| --- | --- |
| [MiMo-Audio: Audio Language Models are Few-Shot Learners](https://arxiv.org/abs/2512.23808) | 该论文的主要贡献可以总结为以下几个方面：<br/><br/>1. **跨模态预训练模型的音频领域应用**：论文提出将当前广泛应用于文本领域的大型预训练模型（如GPT-3）的预训练范式扩展到音频领域，通过大量数据集的预训练增强了模型的一般化能力。<br/><br/>2. **MiMo-Audio模型的提升**：论文通过增加预训练的数据量，使得MiMo-Audio模型能够在多种多样的音频任务中展现出少量样本学习的能力。该模型在开放源代码环境中，在语音智能和音频理解基准测试中均达到了SOTA（状态最优）性能。<br/><br/>3. **跨任务适应性**：MiMo-Audio-7B-Base模型不仅能在其训练数据包含的任务上表现出色，还能在未出现在训练集中的新任务如声线转换、风格转移和语音编辑等任务上进行泛化。<br/><br/>4. **生成能力的提升**：该模型展示出强大的语音连续性生成能力，能够生成高度逼真的广播节目、朗诵、直播和辩论等音频内容。<br/><br/>5. **后训练阶段的改进**：通过收集多样化的指令调优语料库，并将推理机制融入到音频理解和生成过程中，在后训练阶段对MiMo-Audio进行了改进。MiMo-Audio-7B-Instruct模型在音频理解基准测试、口语对话基准测试和指令-TTS评估中均达到了SOTA水平。<br/><br/>6. **开放资源与可用性**：论文提供了MiMo-Audio模型的检查点和完整的评估套件，通过GitHub仓库（https://github.com/XiaomiMiMo/MiMo-Audio）公开分享了所有资源。 |
| [Regularized autoregressive modeling and its application to audio signal reconstruction](https://arxiv.org/abs/2410.17790) | ### 贡献点：<br/><br/>1. **提出新的模型框架**：论文提供了一个全面且通用的自回归（AR）模型框架，旨在同时对时域信号值和AR系数进行正则化或约束。这个框架整合了对先验信息的需求或数值稳定性考量。<br/><br/>2. **阐述优化问题与算法**：详细描述了所提出框架的相关优化问题及解决方法，并提供了一种算法实现方式。<br/><br/>3. **探讨计算要求与改进效果**：分析了该算法的计算需求，评估并讨论了不同改进措施对算法收敛速度的影响。<br/><br/>4. **实验验证**：通过在音频去剪切和去量化两个具体应用领域进行实验，展示方法的有效性。比较了新方法与当前最先进的技术性能，并着重证明其在音乐信号去剪切方面的能力以及在语音去剪切时的优越性。<br/><br/>5. **引入GLP算法的竞争分析**：将一个名为通用线性预测（GLP）的启发式算法作为强大的竞争对手进行对比，这个算法此前仅以专利形式存在，在科学界是新的发现。通过性能比较，展示了所提出方法在与GLP的对战中展现出的优势。<br/><br/>6. **理论与实际应用结合**：提供了一个将理论研究转化为实际应用的实例，证明了所提出的AR模型框架和优化策略在音频领域特定问题上的适用性和有效性。 |
| [Structuring Concept Space with the Musical Circle of Fifths by Utilizing Music Grammar Based Activations](https://arxiv.org/abs/2403.00790) | 贡献点:<br/><br/>1. **提出神经编码框架**：论文提出了“谐波托罗尼代码”这一神经编码框架，这是一种将抽象的认知操作通过音乐理论结构所衍生的动力学活动在流形上实现的方法。<br/><br/>2. **音乐理论与认知操作的结合**：该研究创新地将音乐理论的概念融入到认知科学中，探索如何利用音乐中的和谐、调性和旋律等元素来表达和实施认知过程。<br/><br/>3. **神经网络与数学结构的融合**：论文展示了如何通过将神经网络与数学流形（manifolds）相结合，实现对抽象概念的操作编码。这是一种跨学科的方法论尝试，结合了神经科学、数学和音乐理论。<br/><br/>4. **动态活动在流形上的应用**：重点讨论了动态活动如何在由音乐理论结构衍生的流形上进行，这为理解认知过程提供了新的视角，同时也可能为开发更高级的人工智能系统提供灵感。 |
| [Automatic Stage Lighting Control: Is it a Rule-Driven Process or Generative Task?](https://arxiv.org/abs/2506.01482) | 贡献点如下：<br/><br/>1. **提出Skip-BART模型**：开发了一个端到端的模型，名为Skip-BART，该模型能够直接从经验丰富的舞台灯光工程师处学习，并预测出鲜艳、具有人性化的舞台灯光效果。这是第一个将自动舞台照明控制（ASLC）任务视为生成性任务而非仅仅分类问题的工作。<br/><br/>2. **BART模型的适应与创新**：采用了BART模型并进行了调整，使其能够接受音频音乐作为输入，并以光色调和亮度（强度）作为输出。此外，引入了一种新颖的跳过连接机制来增强音乐和光线之间的关系在帧网格内的联系。<br/><br/>3. **数据集建设**：由于缺乏可用的数据集，创建了第一个舞台灯光数据集，并提出了多种预训练和迁移学习技术，以在数据有限的情况下改进模型训练的效果。<br/><br/>4. **全面验证方法的有效性**：通过定量分析和人机评估对方法进行了验证，证明Skip-BART在所有评估指标上都优于传统基于规则的方法，在某些方面甚至与真正的灯光工程师相比只有轻微的差距。<br/><br/>5. **开放资源支持进一步研究**：提供了自收集的数据集、代码以及训练模型参数的访问链接（https://github.com/RS2002/Skip-BART），为后续的研究工作提供了一个基础。 |
| [Mamba2 Meets Silence: Robust Vocal Source Separation for Sparse Regions](https://arxiv.org/abs/2508.14556) | 贡献点如下：<br/><br/>1. **提出了一种针对准确人声隔离的新音乐源分离模型**：该模型能够提供更为精确的人声提取，相较于基于Transformer的方法，在处理间歇性出现的人声方面表现更优。<br/><br/>2. **利用Mamba2状态空间模型**：通过引入Mamba2，改进了对长期时间依赖关系的捕捉能力，以解决先前方法在长时序输入中难以有效处理的问题。<br/><br/>3. **结合分带策略与双路径架构**：该方法有效地管理了长时间序列输入，提高了模型处理大量数据的能力，同时保证了计算效率和性能。<br/><br/>4. **实验结果显著优于最新技术**：通过实际应用和对比测试，该模型在多项评估指标上（如cSDR）取得了令人瞩目的成绩，且在uSDR等其他关键指标上也表现出了显著优势。<br/><br/>5. **模型具有跨输入长度与声部出现模式的稳定性能**：实验结果显示，无论输入音频的长度或人声出现的模式如何变化，该模型都能保持稳定的性能和一致性。<br/><br/>6. **证实了Mamba基模型在高分辨率音频处理领域的有效性**：通过实证研究证明了基于Mamba2的状态空间模型在精确度方面高于传统方法，对音频处理领域有着重要的贡献，并为进一步的研究提供了新的方向。 |
| [STSR: High-Fidelity Speech Super-Resolution via Spectral-Transient Context Modeling](https://arxiv.org/abs/2509.03913) | ### 贡献点:<br/><br/>1. **提出STSR框架**: 引入了一种在MDCT域内形成的统一端到端框架来解决语音超分辨率问题。该框架旨在平衡全局谐波一致性和局部瞬态锐度。<br/><br/>2. **Spectral-Contextual Attention机制**:<br/>   - 通过层次化窗口技术，该方法能够适应性地聚合非局部谱上下文信息。<br/>   - 这种机制支持在MDCT域内对高至48 kHz的和谐重构提供一致性和连续性。<br/><br/>3. **稀疏感知正则策略**:<br/>   - 引入了一种针对压缩谱表示中固有瞬态抑制问题的策略来减轻其影响，确保精确的谐波对齐。<br/>   <br/>4. **性能表现**:<br/>   - STSR在感知保真度和无假设（zero-shot）泛化能力上均优于当前最先进的基线模型。<br/><br/>5. **实时高质量语音恢复**:<br/>   - 提供了一种稳健且能实时应用的方法，用于高质音响重建，满足了实际应用场景的需求。 |
| [Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization](https://arxiv.org/abs/2509.15579) | ### 贡献点:<br/><br/>1. **提出了基于块的自监督学习算法(Chunk SSL)**: 该论文提出了一种统一解决流式和离线语音预训练问题的方法，适用于低延迟语音人机通信场景。这种方法在自监督学习领域引入了块的概念。<br/><br/>2. **优化使用掩码预测损失**: Chunk SSL 算法通过优化以掩码预测损失为核心，并鼓励声学编码器利用同一块内的未遮蔽帧和前一块的未遮蔽帧来恢复被遮蔽语音帧的索引，以此提升模型性能。<br/><br/>3. **引入了复制与附加数据增强方法**: 为提高块级预训练效率，论文提出了一个复制与附加的数据增强策略。该方法有助于增加训练数据的多样性和丰富性，同时保持计算和存储的需求在可控范围内。<br/><br/>4. **使用有限标量量化(FSQ)模块对输入语音特征进行离散化处理**: FQ模块能够有效地将输入的连续信号转换为离散码本表示，研究表明高分辨率FSQ码本（词汇表大小达到数百万级）对于从预训练任务向下游任务转移知识非常有益。<br/><br/>5. **采用了组掩码预测损失来减轻大码本书提供带来的内存和计算成本**: 在预训练过程中采用组掩码预测损失可以有效缓解由于大量码本书提供的数据处理带来的高内存和计算负担，提高了算法的实用性和效率。<br/><br/>6. **在语音到文本任务上验证了方法的有效性**: 论文通过在 Librispeech 和 Must-C 数据集上的实验结果表明，提出的 Chunk SSL 方法在流式和离线模式下的语音识别和语音翻译任务中都能达到非常有竞争力的结果。 |
| [Hear: Hierarchically Enhanced Aesthetic Representations For Multidimensional Music Evaluation](https://arxiv.org/abs/2511.18869) | ### 贡献点:<br/><br/>1. **多源多层次表示模块**: HEAR框架采用一种结合了多种来源和多个尺度的表示方式, 旨在从片段级和轨道级层面获取互补特征。这一方法能够提供更全面且相互补充的音乐感知特征，为评价歌曲美学提供了坚实的基础。<br/><br/>2. **分层增强策略**: 该策略被用于减轻过拟合问题。通过有层次地对数据进行增强，HEAR能够在保持模型泛化能力的同时避免过度依赖特定的数据集或样本。<br/><br/>3. **混合训练目标**: HEAR结合了回归和排名损失两种损失函数作为其训练目标。这种设计旨在实现精确的评分并识别出可靠的顶级歌曲，能够同时关注于分数预测的准确性与排序结果的可靠性。<br/><br/>4. **全面性能评估**: 实验结果显示, HEAR在整个ICASSP 2026 SongEval基准测试的所有指标上均表现出优于基线的性能。这证明了其在音乐审美评价领域的有效性和优越性。<br/><br/>5. **开源代码和模型权重**: 提供了一个易于访问的公开资源，即GitHub仓库([https://github.com/Eps-Acoustic-Revolution-Lab/EAR_HEAR](https://github.com/Eps-Acoustic-Revolution-Lab/EAR\_HEAR))。这不仅允许其他研究者和开发者验证结果、复现实验或进行进一步的研究，还促进了领域内的知识共享与合作。<br/><br/>6. **创新方法的实践应用**: HEAR通过集成上述关键技术点，提供了一种全新的音乐审美评估框架。其实际效果表明了结合多源信息处理、增强策略及混合损失函数在该领域中的有效性和潜力。 |
| [Fun-Audio-Chat Technical Report](https://arxiv.org/abs/2512.20156) | 贡献点如下：<br/><br/>1. **双分辨率语音表示（DRSR）**：<br/>   - 通过共享大型语言模型（LLM）在高效5Hz下处理音频（通过分组令牌），同时语音细化头部生成高质量的25Hz令牌，平衡了效率（约降低GPU使用量50%）和质量。<br/>   - 这种方法解决了时间分辨率不匹配的问题，提高了模型的计算效率，并减少了对文本LLM知识的灾难性遗忘。<br/><br/>2. **核心鸡尾酒训练**：<br/>   - 采用两阶段微调并有中间合并步骤的方法，有效地缓解了灾难性遗忘问题。该策略通过在不同的训练阶段逐步融合信息来优化模型性能。<br/><br/>3. **多任务DPO训练**：<br/>   - 这种训练方法增强了模型的鲁棒性、音频理解能力、指令遵循能力和语音同理心。<br/>   - 多阶段后训练使得Fun-Audio-Chat能够保留文本LLM知识的同时，获得了强大的音频理解和生成能力。<br/><br/>4. **性能与应用**：<br/>   - Fun-Audio-Chat和其扩展版本Fun-Audio-Chat-Duplex在语音到文本、语音到语音任务上实现了竞争性的表现，并在口语问答基准测试中排名同类模型之首。<br/>   - 这些模型在音频理解、语音功能调用、指令遵循和语音同理心方面也表现出了竞争力甚至超越，尤其是在全双工交互场景下。<br/><br/>5. **开源与互动**：<br/>   - 提供了Fun-Audio-Chat-8B的训练和推理代码，并开放了一个互动演示，在GitHub上（https://github.com/FunAudioLLM/Fun-Audio-Chat）供公众访问和使用。 |
