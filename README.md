# 五里墩茶社
---
| Title | Date | Summary |
| --- | --- | --- |
# 小工蚁创始人
---
| Title | Date | Summary |
| --- | --- | --- |
# AIGCLINK
---
| Title | Date | Summary |
| --- | --- | --- |
# 技术爬爬虾
---
| Title | Date | Summary |
| --- | --- | --- |
# 秋芝2046
---
| Title | Date | Summary |
| --- | --- | --- |
# GitHub All Languages Daily Trending
---
| Title | Summary |
| --- | --- |
| [Mebus/cupp](https://github.com/Mebus/cupp) | CUPP（Common User Passwords Profiler）是一款用于评估密码强度和提供个性化密码生成建议的工具。它考虑了密码的常见弱点，如生日、名字等，并支持通过命令行或配置文件使用。该工具要求Python 3环境运行，并提供了多种功能选项来适应不同场景需求，如互动式用户评测、基于字典的密码分析和解析预定义用户名/密码数据。CUPP遵循GNU通用公共许可证，提供免费且开放源代码访问。 |
| [tobi/try](https://github.com/tobi/try) | `try`是一个命令行工具，专为快速管理和查找项目实验或代码尝试设计。以下是其主要特性：<br/><br/>- **实时搜索与导航**：在数千个目录中轻松搜索和切换。<br/>- **自动创建**：无需手动输入路径名；根据时间顺序智能命名并创建新目录。<br/>- **时间感知**：通过将文件或目录创建时间作为命名的一部分，快速定位特定时间段的实验或项目。<br/>- **配置灵活**：支持多种环境安装（如Homebrew、Nix），允许用户自定义存储位置。<br/><br/>该工具使用Ruby实现，单个可执行文件即可运行，依赖性少且适用于大部分系统。它简洁易用，适合开发者在多个项目间快速切换和组织自己的实验与尝试。<br/><br/>`try`的哲学理念是尊重开发者在编程过程中频繁的想法跳跃和代码尝试需求。每个“尝试”或实验都分配一个专属的家，并确保其易于查找。无论是深夜紧急的代码片段还是日常的小实验，`try`提供了一个高效的方法来管理和追踪这些资源，避免遗忘或混乱。<br/><br/>总之，`try`是一个为了解决开发者在处理多个项目和快速迭代时遇到的问题而设计的工具，通过自动化一些常规操作，并提供直观的时间感知搜索机制，帮助提高开发效率和组织性。 |
| [google/langextract](https://github.com/google/langextract) | LangExtract是一个基于自然语言处理的库，主要用于从文本中提取结构化信息。以下是其主要特点和功能：<br/><br/>1. **自动结构化抽取**：LangExtract可以帮助您从非结构化的文本数据（如文档、报告或社交媒体帖子）中识别并提取关键信息。<br/><br/>2. **支持多种模型**：它提供了一个灵活的框架，可以整合不同的NLP模型来处理不同类型的文本数据和任务。这包括预训练模型以及用户自定义的模型。<br/><br/>3. **结构化输出**：LangExtract将从文本中抽取的信息组织成可读性高的JSON格式或表，便于后续分析、存储或可视化。<br/><br/>4. **健康医疗领域增强**（可选）：对于需要处理医疗数据的应用，LangExtract提供了额外的功能和优化以适应医疗领域的特定需求，如识别医学术语、剂量说明等。<br/><br/>5. **社区贡献**：您可以通过在GitHub上的Community Providers页面上添加或分享自定义模型提供者来扩展LangExtract的能力。<br/><br/>6. **开发与贡献**：项目支持自动代码格式化、预提交检查和Linting等现代开发实践，鼓励开发者参与并改进代码质量。对于商业或关键应用的使用，请遵循相关的许可协议和条款。<br/><br/>7. **测试和部署指南**：提供了详细的指导文档来帮助新用户安装、测试以及在生产环境中部署LangExtract。<br/><br/>8. **兼容性和环境设置**：确保项目与多种Python版本（如3.10和3.11）兼容，并提供自动化脚本来简化代码格式化、运行预提交检查及执行linting过程。<br/><br/>总之，LangExtract是一个强大的工具，适用于处理文本数据的结构化提取任务。它为开发者提供了丰富的功能集、灵活的模型整合选项以及与健康医疗领域特定需求相关的增强功能。通过社区贡献和开发指南的支持，使得它可以持续地扩展和优化以满足不断变化的需求。 |
| [OpenBMB/VoxCPM](https://github.com/OpenBMB/VoxCPM) | ### 中文总结：<br/><br/>VoxCPM是一个基于先进模型技术的语音生成和个性化声音克隆工具。以下是关于该模型的一些关键点概述：<br/><br/>#### 特性与功能：<br/>- **无分词TTS（Text-to-Speech）**：无需预先进行文本分词即可直接转换文本为语音。<br/>- **上下文感知**：能够根据输入内容的语境生成合适的音调和情感表达，更贴近人类自然交流。<br/>- **真感声音克隆**：能实现与特定个体声音极为相似的模仿。<br/><br/>#### 技术背景：<br/>VoxCPM采用了多种创新技术，包括：<br/>1. **扩散自回归架构**（DiTAR）**为语音生成提供基础框架**。<br/>2. **语言模型**（MiniCPM-4）作为核心**，提升模型的理解和表达能力**。<br/>3. **基于流匹配的局部距离校正（LocDiT）**改进了声音风格的迁移。<br/>4. **音频向量化技术**（DAC）用于处理和生成高质量的声音。<br/><br/>#### 适应性和局限性：<br/>- **多语言支持**：当前版本主要针对中文和英文，其他语言的支持可能有限或效果不佳。<br/>- **表达控制**：对于情感、语速等细节的控制相对原始输入而言较为直接，高级定制仍有改进空间。<br/>- **技术稳定性**：模型在大多数场景下表现稳定，但对长文本或复杂表达存在一定的处理限制。<br/><br/>#### 发展计划与未来展望：<br/>- 计划提升声音质量、扩展多语言支持，并探索通过人类指令实现更精细的语音控制功能。<br/><br/>#### 使用和贡献：<br/>VoxCPM遵循Apache 2.0开源许可证，鼓励社区参与并提供了详细的指导文档。用户可在实现研究或开发项目时使用，但需遵守相关法律框架，确保合法合规地应用模型生成的内容。<br/><br/>### 致谢与支持机构：<br/>该项目由多家学术和研究机构共同推动，包括ModelBest、THUHCSI等，在技术创新和资源贡献方面给予支持。<br/><br/>VoxCPM的源代码已公开，用户可以探索其内部机制并进行定制开发。项目也鼓励用户提供反馈和建议，同时欢迎新的功能和技术改进提议。<br/><br/>#### 引用指南：<br/>如您在研究或项目中使用了VoxCPM，并发现其对您的工作具有帮助，请考虑引用该论文，并给予项目GitHub页面关注和支持以促进社区发展。 |
| [Flowseal/zapret-discord-youtube](https://github.com/Flowseal/zapret-discord-youtube) | 这是一个关于如何在受限制的网络环境中访问特定资源和使用相关工具的方法。以下是几个关键步骤：<br/><br/>1. **解锁网络访问**：<br/>   - **初始化环境**：安装所需的软件（如`ipset`），并了解其工作原理。<br/>   - **设置IP过滤规则**：通过编辑`ipset-all.txt`和`ipset-exclude.txt`文件，添加或排除特定的IP地址或子网来解锁访问。<br/><br/>2. **自定义域名列表**：<br/>   - 添加需要访问的域名到`list-general.txt`文件中（支持自动解析子域）。<br/>   - 通过`list-exclude.txt`文件排除不需要过滤的特定域名。<br/><br/>3. **配置工具**：<br/>   - 根据需求在脚本或配置文件中引用上述列表。<br/>   - 集成`zapret`工具以实现策略和规则应用。<br/><br/>4. **执行访问**：<br/>   - 启动服务并确认资源访问是否正常，可能需要调整策略或排除项。<br/><br/>5. **优化与扩展**：<br/>   - 根据需要调整过滤规则、域名列表或IP设置。<br/>   - 利用`README.md`和相关文档获取进一步的帮助或贡献反馈。<br/><br/>6. **技术与社区支持**：<br/>   - 考虑项目的星标以支持开发，了解项目贡献者，并寻找原创开发者`bol-van`的额外帮助途径（可能在GitHub页面）。<br/><br/>7. **遵守许可**：<br/>   - 遵循MIT开源许可证使用和分发资源。<br/>   <br/>8. **致谢参与者**：<br/>   - 通过项目图查看贡献者，特别感谢`bol-van`作为原始开发者的贡献。<br/><br/>总结来说，这是一个涉及网络策略管理、域名与IP列表配置以及社区参与的流程，旨在绕过访问限制。关键是合理配置规则以确保安全和高效地访问所需资源。 |
| [iOfficeAI/AionUi](https://github.com/iOfficeAI/AionUi) | 以下是关于AionUI的详细信息和介绍：<br/><br/>**项目概览**<br/><br/>AionUI是一款基于现代AI技术的聊天界面应用，旨在为用户提供个性化的交流体验。其核心功能包括智能问答、个性化推荐、实时翻译等。<br/><br/>**主要特性**<br/><br/>1. **AI助手**：提供智能对话，帮助用户解决问题或获取所需信息。<br/>2. **个性化服务**：根据用户偏好和历史交互数据，提供定制化内容和服务建议。<br/>3. **多语言支持**：具备多种语言功能，方便国际用户交流和使用。<br/><br/>**安装与配置**<br/><br/>- **下载与安装**：项目提供了GitHub上的最新发布版本供用户下载并安装。<br/>- **启动与使用**：安装完成后，即可直接体验AionUI的AI聊天界面。初始设置可能包括登录Google账户或通过API密钥进行认证。<br/><br/>**社区与贡献**<br/><br/>- **社区参与**：鼓励用户在GitHub讨论区分享想法、提出建议和交流经验。也提供了一个官方Discord频道供英文使用者交流。<br/>- **微信社群**：项目还提供了一个微信群（中文）供中国用户使用，可以通过扫描提供的二维码加入。<br/>- **提交反馈与贡献**：对于发现的错误或希望增加的功能，可以报告问题或直接在GitHub上提交Pull Request。<br/><br/>**技术支持**<br/><br/>- **文档和教程**：提供详细的安装指南和使用说明。需要详细配置指导时，可查看“完整的安装教程”部分。<br/><br/>**许可协议**<br/><br/>项目遵循Apache-2.0开源许可证，允许用户自由地修改、分发并用于商业或非商业项目中。<br/><br/>**贡献者与支持者**<br/><br/>感谢所有为AionUI贡献代码的开发者，并特别邀请用户根据文档指引提交问题报告和功能请求。为了表示对项目的支持，请考虑在GitHub上给项目打星。<br/><br/>**星历史统计**<br/><br/>项目还提供了GitHub上的Star变化趋势图表，用于追踪用户对其受欢迎程度的关注度。<br/><br/>**获取更多帮助与反馈**<br/><br/>如果遇到任何问题或有改进意见，可以直接在GitHub的Issues页面报告问题或提出功能请求。 |
| [nautechsystems/nautilus_trader](https://github.com/nautechsystems/nautilus_trader) | ### 中文总结：<br/><br/>这篇文档概述了NautilusTrader项目的主要方面，包括其用途、特性、贡献方式以及社区参与等。以下是几个关键点的总结：<br/><br/>**使用与功能：**<br/>- NautilusTrader是一个高效率的交易系统，旨在满足金融市场的复杂需求。<br/>- 提供全面的API和工具集，支持订单管理、风险管理、策略开发等。<br/><br/>**贡献途径：**<br/>1. **报告问题/提出建议**：通过GitHub上的问题追踪器提交对现有功能或新功能的需求反馈。<br/>2. **签署CLA**：在做出贡献前需要完成Contributor License Agreement（CLA），确保您的贡献能合法地纳入项目中。<br/>3. **代码贡献**：目标是向`develop`分支提交代码。开发和改进集中在这一分支，最终通过此流程整合进官方发布版本。<br/><br/>**社区与交流：**<br/>- NautilusTrader用户和开发者可通过Discord平台进行沟通和技术讨论。<br/>- 官方通知、更新及活动发布在NautilusTrader的官方网站、Discord服务器以及X（Twitter）账号上。确保从这些渠道获取官方信息，警惕未经授权的信息。<br/><br/>**法律与免责声明：**<br/>- 遵循GNU Lesser General Public License v3.0条款贡献代码，并了解项目不支持或推广任何加密货币代币。<br/>- 对于可疑活动应立即报告并联系项目方。<br/><br/>### 中文提示：<br/>- 使用中文时，请注意语法和拼写的准确性，避免使用非标准的表达方式。例如，“感谢您的贡献”在正式文件中应译为“感谢您对项目的贡献”，而非简化版本。<br/>- 尽管文档中的内容涵盖了详细信息，但在实际应用或翻译过程中还需考虑语境、专业术语和文化差异，以确保信息准确传达。 |
| [yt-dlp/yt-dlp](https://github.com/yt-dlp/yt-dlp) | 这是一个关于`yt-dlp`（一个用于下载视频、音频和其他媒体文件的命令行工具）的详细用户指南。该文档概述了各种功能和选项，帮助用户理解如何有效地使用这个强大的工具进行媒体内容的下载和处理。<br/><br/>- **基本选项**涵盖了诸如输出格式、存储位置等基础配置。<br/>- **高级选项**包括了视频编码选择、音频格式化、多文件下载控制等内容，允许用户精细调整下载过程以满足特定需求。<br/>- **注释与日志**部分描述了如何启用详细的下载日志和使用注释进行记录跟踪，以便于监控下载过程。<br/>- **HTTP代理**提供了配置下载请求时使用的代理服务器的方法。<br/>- **错误处理和警告**说明了如何识别、理解并解决下载过程中可能出现的常见问题。<br/><br/>该指南还包括了关于文件命名规则、自定义输出格式、跳过广告选项等具体应用实例。对于用户关心的安全性，文档也包含了如禁用HTTPS验证和优先选择不安全连接的相关设置。<br/><br/>最后，文档提供了代码贡献者指导以及访问项目wiki的链接，鼓励社区参与项目的改进和完善。<br/><br/>总的来说，这份指南是`yt-dlp`用户和潜在开发者不可或缺的资源，涵盖了从基本使用到高级定制的一系列信息。 |
| [yichuan-w/LEANN](https://github.com/yichuan-w/LEANN) | LEANN是一个低存储向量索引系统，旨在解决大规模数据检索时的内存和计算资源限制。它通过构建紧凑的数据结构来实现高效的大规模文本检索。关键特性包括：<br/><br/>1. **压缩表示**：使用编码技术对文本进行压缩处理，减少存储需求。<br/>2. **近似搜索**：支持快速但可能不完全准确的查询结果，以换取更快的响应时间和更低的内存占用。<br/>3. **索引管理**：自动管理和维护索引结构，包括添加、删除和更新文档的能力。<br/><br/>为了使用LEANN，你通常会首先创建一个`IndexBuilder`实例，并调用其相关方法来构建或调整索引。例如：<br/><br/>```python<br/>from leann import IndexBuilder<br/><br/># 创建一个索引构建器实例<br/>index_builder = IndexBuilder()<br/><br/># 添加新文档至索引<br/>index_builder.add(documents)<br/><br/># 构建最终的索引<br/>index_builder.build(index_name="my_index")<br/>```<br/><br/>LEANN提供了丰富的API来处理查询、添加和删除文档，以及评估系统性能。此外，项目还包括了详细的教程、FAQ和贡献指南。<br/><br/>###获取更多信息：<br/><br/>- **论文**：阅读关于LEANN的技术报告或在[arXiv](https://arxiv.org/abs/2506.08276)上查找。<br/>- **API文档**：查看GitHub上的项目文档，了解如何使用不同功能和类方法。<br/>- **社区参与**：在GitHub项目页面上提出问题、提出改进或直接贡献代码。<br/><br/>项目的主页和GitHub仓库都提供了丰富的资源来帮助你了解并开始利用LEANN。 |
# eess.AS updates on arXiv.org
---
| Title | Summary |
| --- | --- |
| [DSA-Tokenizer: Disentangled Semantic-Acoustic Tokenization via Flow Matching-based Hierarchical Fusion](https://arxiv.org/abs/2601.09239) | ### 贡献点:<br/><br/>1. **提出DSA-Tokenizer:** 引入了一种全新的分词器设计,称为“动态语义与声学令牌化器(Dynamic Semantic and Acoustic Tokenizer, DSA-Tokenizer)”,专门用于离散语音大语言模型(Speech Large Language Models, Speech LLMs)。该分词器旨在实现更优的语义和声学特征分离。<br/><br/>2. **明确分离语义与声学令牌:** 通过设置特定的优化约束,DAS-Tokenizer能够将语音信号明确划分为独立的离散语义令牌和声学令牌,分别关注语言内容的捕捉和风格信息的编码。<br/><br/>3. **引入Hierarchical Flow-Matching解码器:** 为了解决两个序列之间固定长度限制的问题,引入了一种分层流匹配解码器。该设计进一步提高了语音生成的质量。<br/><br/>4. **采用联合重建与重组训练策略:** 实施了一种策略,通过强制执行分离语义和声学令牌的目标,使得DSA-Tokenizer能够实现高保真度的重构和灵活的重组,从而在言语LLMs中实现可控生成能力。<br/><br/>5. **突出分层化令牌化的重要性:** 分析表明,这种分离化的令牌化方法是未来语音建模的关键范式。论文提供了音频样本展示(DSA_Tokenizer_demo)以供参考。<br/><br/>6. **开源代码与模型:** 论文接受后,DAS-Tokenizer的代码和模型将向公众开放,促进学术研究和应用开发。 |
| [Unifying Speech Recognition, Synthesis and Conversion with Autoregressive Transformers](https://arxiv.org/abs/2601.10770) | ### 贡献点:<br/><br/>1. **提出统一的音频基础模型（General-Purpose Audio，GPA）**: GPA是一个整合了多个核心语音任务于单一大型语言模型架构中的统一音频框架。这打破了传统的、针对特定任务的分离式模型体系结构，如文本转语音(TTS)、自动语音识别(ASR)和声音转换(VC)，从而解决了规模化、效率低下以及跨任务泛化能力不足的问题。<br/><br/>2. **共享离散音频令牌空间**: GPA在其内部使用了一个共享的离散音频令牌空间，这使得模型能够在一个共同的表示上执行多个任务，支持指令驱动的任务诱导。这意味着单一的自回归模型能够在没有架构修改的情况下灵活地执行TTS、ASR和VC。<br/><br/>3. **结合完全自回归形式、跨语音领域联合训练以及可扩展推理管道**: 这个统一的设计包含了一个完全自回归的表述方式，用于离散语音令牌上，同时在多个语音领域内进行了联合多任务训练，并具有一个可扩展的推理管道，实现了高并发性和吞吐量。<br/><br/>4. **支持多尺度部署和轻量化模型**: GPA模型家族提供了高效的大规模部署能力，包括一个经过优化以适应边缘环境和资源受限场景的0.3B参数的轻量化变体。这表明统一的自回归架构在保留低延迟、实用部署的同时，能够提供跨不同语音任务的竞争性性能。<br/><br/>5. **展示统一的自回归架构在多任务中的竞争力**: 通过比较GPA与其他多任务模型的表现，论文展示了其在各种语音任务上的性能，证明了即使在低延迟和实际部署场景下，单一架构也能实现与专门针对单个任务的模型相匹敌的效果。<br/><br/>### 总结：<br/>这篇论文提出了General-Purpose Audio（GPA）框架，这是一种新的统一音频基础模型。通过融合文本转语音、自动语音识别和声音转换等核心语音任务于一个大型语言模型中，并设计了可扩展性高、性能强大的推理管道，该模型实现了跨多任务的有效处理能力。此外，论文还展示了如何通过多尺度部署和轻量化设计来优化资源使用，确保模型在低延迟场景下的实际可用性，以及如何维持与专门针对单个任务的模型相媲美的性能水平。 |
| [FlashLabs Chroma 1.0: A Real-Time End-to-End Spoken Dialogue Model with Personalized Voice Cloning](https://arxiv.org/abs/2601.11141) | 贡献点如下：<br/><br/>1. **首次公开的实时端到端口语对话模型** - Chroma 1.0 是首个将实时性和高保真个性化语音克隆能力结合在一起的开放源代码端到端口语对话模型。<br/><br/>2. **低延迟的交互体验** - 通过交错的文字-音频令牌调度策略（每生成文本1个单位，同时生成音频2个单位），Chroma实现了次秒级别的端到端延迟，支持流式生成，提供了流畅、实时的互动体验。<br/><br/>3. **高质量个性化语音合成** - 在多轮对话中，Chroma维持了高品质的个性化声音合成能力，并在与人类基线相比时显示出了10.96%的相对改进，在语音相似性方面。<br/><br/>4. **卓越的表现和推理能力** - 实验结果表明，Chroma不仅在语音相似度上表现优异，而且在整个对话过程中保持了强大的推理和对话能力。<br/><br/>5. **公开的可访问资源** - 所有代码和模型都已开放源码，并且可以从官方 GitHub 页面（[链接](https://github.com/FlashLabs-AI-Corp/FlashLabs-Chroma)）及 Hugging Face 平台（[链接](https://huggingface.co/FlashLabs/Chroma-4B)）获取。 |
| [SuperEar: Eavesdropping on Mobile Voice Calls via Stealthy Acoustic Metamaterials](https://arxiv.org/abs/2501.15032) | 以下是该论文的主要贡献点：<br/><br/>1. **首次提出并实现SuperEar系统**：SuperEar是首个使用声学超材料的便携式系统，用于可靠地在移动通话场景中捕获对话。该系统的应用能够解决现有的音频窃听攻击在实际户外环境中的局限性。<br/><br/>2. **实际原型验证威胁存在**：研究通过构建一个实用原型来增强微弱信号、采用紧凑设计覆盖全范围语音，并减少噪音和失真，从而产生清晰的音频输出，以证实声学窃听威胁的真实性。<br/><br/>3. **低成本组件实现系统**：SuperEar可以使用低成本的3D打印部件和现成的硬件进行实施，这表明了系统的可负担性和实际应用性。<br/><br/>4. **显著提升捕获距离与成功率**：实验结果显示，SuperEar在4.6米的距离上能够恢复电话通话音频的成功率达到80%以上——这是之前方法范围的两倍多。这一结果强调了超材料技术所导致的新类别的隐私威胁，并要求得到关注。 |
| [Data Standards in Audiology: A Mixed-Methods Exploration of Community Perspectives and Implementation Considerations](https://arxiv.org/abs/2505.04728) | 贡献点如下：<br/><br/>1. **研究目标**：该论文关注音频领域中数据标准化的概念性问题，并提出了实现标准化的步骤。研究通过调查计算听觉社区，了解他们对当前数据标准的理解、需求和偏好。<br/><br/>2. **设计方法**：采用了混合方法的研究设计，包括：<br/>   - 对现有标准化努力的回顾<br/>   - 调查全球计算听觉社群成员的意见<br/>   - 在2024年计算听觉虚拟会议上进行专家讨论专题讲座<br/><br/>3. **样本与参与者**：<br/>   - **调查**：共有82名全球社区成员参与了问卷调查。<br/>   - **专家小组讨论**：有五位专家参加了讨论会。<br/><br/>4. **研究结果**：<br/>   - 认为在任何全球听觉数据库中，都需要达成一致的数据标准。虽然许多参与者熟悉标准化的普遍概念，但对现有项目知之甚少或没有亲自参与过。<br/>   - 90%的受访者表示愿意跟随或贡献于标准化工作。<br/><br/>5. **专家讨论**：<br/>   - 讨论了相关倡议（如OMOP、openEHR和Noah等）以及它们面临的挑战（如一致性问题）和机遇（与其他医疗领域对齐及方法转换）。<br/><br/>6. **结论与指导建议**：该研究结合了概念性讨论与社区视角，为在听觉领域实施可互操作的数据标准提供了指导。它强调了社区的支持、需要解决的关键问题，并提出了未来工作的可能路径。 |
| [What Makes a Good Speech Tokenizer for LLM-Centric Speech Generation? A Systematic Study](https://arxiv.org/abs/2506.12537) | ### 贡献点:<br/><br/>1. **系统性研究SLS中的语音分词器设计作用**: 通过在以大型语言模型（LLM）为中心的SLS框架下，对联结、半脱耦和完全脱耦的语音分词器进行了系统性的研究。发现完全脱耦的分词方式显著提高了模态间的有效对齐以及语音生成的质量。<br/><br/>2. **引入多令牌预测（MTP）机制**: 将MTP引入到SLS中，使每个隐藏状态能够解码多个语音令牌。这一方法使得解码速度提高至12倍以上，并极大地降低了词错误率，从6.07降至3.01。<br/><br/>3. **提出基于说话者感知的生成范式**: 提出了一种以说话者感知为中心的生成框架，以及引入了RoleTriviaQA，这是一个大型的角色扮演知识问答基准数据集，具有多样的说话者身份。实验结果显示，这种方法在增强知识理解与提高说话者一致性方面表现出优势。<br/><br/>4. **构建大规模角色扮演知识问答基准（RoleTriviaQA）**: 这一贡献提供了一个新的、大规模的数据集，用于评估和改进SLS在处理多种说话者身份方面的性能，特别适用于验证方法在不同场景下的适应性和有效性。 |
| [POWSM: A Phonetic Open Whisper-Style Speech Foundation Model](https://arxiv.org/abs/2510.24992) | ### 贡献点:<br/><br/>1. **多任务统一框架** - 介绍了一种名为POWSM（Phonetic Open Whisper-style Speech Model）的新型模型，该模型旨在联合完成多个与语音相关的任务。这是首个能够同时处理多种发音任务的统一框架。<br/><br/>2. **跨模态转换能力** - POWSM允许无缝地在音频、文本（字形）和语音之间进行转换，这为通用和低资源语音处理开辟了新的可能性。<br/><br/>3. **性能优化** - POWSM模型在大小相似的情况下，相对于专门用于发音识别的Wav2Vec2Phoneme和ZIPA模型，在多任务支持方面表现出更好的性能或与之相匹配。<br/><br/>4. **开放科学倡议** - 为促进科学界的开放性，提供了训练数据、代码和模型的公开访问，以推动更多研究者在相关领域进行探索和应用。 |
