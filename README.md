# 五里墩茶社
---
| Title | Date | Summary |
| --- | --- | --- |
# 小工蚁创始人
---
| Title | Date | Summary |
| --- | --- | --- |
# AIGCLINK
---
| Title | Date | Summary |
| --- | --- | --- |
# 技术爬爬虾
---
| Title | Date | Summary |
| --- | --- | --- |
# 秋芝2046
---
| Title | Date | Summary |
| --- | --- | --- |
# GitHub All Languages Daily Trending
---
| Title | Summary |
| --- | --- |
| [AI4Finance-Foundation/FinRobot](https://github.com/AI4Finance-Foundation/FinRobot) | 本文概述了AI和机器学习领域的最新进展，特别关注金融领域中的应用。主要包含了以下方面：<br/><br/>1. **AI在金融研究中的使用**：<br/>   - 引入FinRobot，这是一个基于大型语言模型的AI代理平台，用于金融应用。<br/>   - 提到AI与人类专家的合作方法，以优化投资分析。<br/><br/>2. **AI驱动的研究工具和技术**：<br/>   - 介绍增强投资分析的方法，通过优化AI代理在财务研究中的协作来提升效能。<br/><br/>3. **开源框架和工具**：<br/>   - 列举多个用于构建大型语言模型、多模态AI代理和其他金融相关任务的开源框架和工具。<br/>   - 包括FinRobot在内的平台，旨在实现多语种理解和生成能力的融合，提供全面的语言融合框架等。<br/><br/>4. **学术引用与研究参考**：<br/>   - 提供了用于引用的研究论文示例，强调了在财务分析和估值中的AI应用。<br/><br/>总结来说，这篇文档提供了金融领域内利用先进AI技术进行投资分析、增强决策制定过程的重要概述。通过集成大型语言模型和多模态代理的工具和框架，研究者和实践者可以更有效地处理复杂数据，提升效率并优化策略决策。同时强调在实际应用中需要谨慎行事，并寻求专业建议，确保风险控制和合规性。 |
| [Blaizzy/mlx-audio](https://github.com/Blaizzy/mlx-audio) | MLX Audio是一个专为Apple Silicon平台设计的音频处理库，它提供了文本转语音（TTS）、语音转文字（STT）和语音识别与合成（STS）的功能。主要特点包括：<br/><br/>1. **平台支持**：该库针对M1、M2、M3、M4等基于ARM架构的Apple Silicon进行优化，旨在提供高性能的音频处理解决方案。<br/><br/>2. **功能组合**：MLX Audio集成了TTS、STT和STS能力，能够处理从文本到语音的转换，以及从语音到文本的信息提取。这使其在多领域应用中具有广泛的应用价值，如智能助手、游戏、虚拟助理等。<br/><br/>3. **兼容性和扩展性**：该库提供了用于量化模型以减少内存使用并提高运行时性能的工具。同时，用户可以根据需要自定义和上传优化后的模型到Hugging Face Hub等平台。<br/><br/>4. **依赖与要求**：<br/>   - 需要Python 3.10及以上版本。<br/>   - 应用在MacOS环境下，并且支持Apple M系列芯片。<br/>   - 必需安装ffmpeg，用于MP3或FLAC格式的音频编码。WAV格式无需ffmpeg。<br/><br/>5. **许可与引用**：MLX Audio遵循MIT License协议，用户需要正确引用库以遵循版权和使用条款。同时提供了一个BibTeX格式的引用，方便在学术发表中引用。<br/><br/>6. **扩展性资源**：提及了对于Swift（用于iOS应用开发）的兴趣，建议用户参考mlx-audio-swift项目，这是一个针对macOS和iOS平台上的TTS功能进行优化的版本。<br/><br/>综上所述，MLX Audio为开发者提供了一个强大且易于集成的音频处理解决方案，特别是在Apple Silicon设备上的性能表现尤为突出。 |
| [block/goose](https://github.com/block/goose) | goose是一款开源的可扩展AI助手，能自动化工程任务，提供从头到尾的开发流程支持，包括代码生成、执行、调试和外部API交互。它适应各种LLM模型配置以优化性能与成本，并支持多模型，适合快速迭代及复杂工程流程管理。goose以桌面应用和命令行界面形式提供，是开发者提升效率、聚焦创新的理想AI伙伴。 |
| [remotion-dev/remotion](https://github.com/remotion-dev/remotion) | Remotion是一个利用React框架进行编程式视频制作的工具。它允许开发者使用Web技术如CSS、Canvas、SVG和WebGL，通过代码中的变量、函数和算法创建新的视觉效果，同时还能享受到React带来的可复用组件、强大组合、快速刷新等功能。其作品丰富多样，包括个性化年度回顾等项目，并提供了入门指南和官方文档供参考学习。 |
| [business-science/ai-data-science-team](https://github.com/business-science/ai-data-science-team) | 这是一个利用AI驱动的数据科学团队库，包括专门的代理和旗舰应用“AI Pipeline Studio”，旨在帮助用户以10倍的速度完成常见数据科学任务。该工具提供从数据加载、清洗、可视化到模型构建的一站式服务，并支持项目保存、多集合并工作流等高级功能。 |
| [VectifyAI/PageIndex](https://github.com/VectifyAI/PageIndex) | 这是一个关于一个名为“PageIndex”的AI项目的概述和介绍。主要包含以下几个要点：<br/><br/>1. **项目背景**：<br/>   - PageIndex旨在通过提供无向量的索引系统，来增强对复杂金融文档的理解和检索能力。<br/>   - 它特别适用于处理证券报告、收益披露等财务文件。<br/><br/>2. **技术亮点**：<br/>   - 采用层级化的索引化方法与基于推理的检索机制，实现精准导航和相关上下文提取。<br/>   - 能够显著提升传统的向量数据库RAG（阅读理解增强）系统在金融领域任务上的表现。<br/><br/>3. **实际应用案例**：<br/>   - Mafin2.5是使用PageIndex技术驱动的财务文档分析系统，它在FinanceBench基准测试中取得了98.7%的高准确率。<br/>   - 与传统基于向量的方法相比，Mafin2.5能更精确地定位和提取关键信息。<br/><br/>4. **资源和服务**：<br/>   - 提供了详细的Cookbooks、教程、博客文章等学习资料。<br/>   - API文档和整合指南帮助开发者集成使用PageIndex。<br/>   <br/>5. **社区与支持**：<br/>   - 用户可以通过Twitter、LinkedIn和Discord等渠道与项目团队互动。<br/>   - 通过在线表单或电子邮件联系开发团队获取技术支持。<br/><br/>6. **支持与合作**：<br/>   - 鼓励用户给项目star，表达对项目的认可和支持。<br/><br/>整体来看，PageIndex旨在通过创新的技术方案解决复杂文档的检索和理解问题，并在金融领域展示了显著的优势。项目提供了丰富的资源和社区支持，以及开放的合作机会。 |
| [k4yt3x/video2x](https://github.com/k4yt3x/video2x) | 这段文档是关于一个开源项目Video2X的介绍。该文档详细描述了项目的使用方法、许可信息、提供的素材样例以及对重要贡献者的致谢。以下是主要内容摘要：<br/><br/>1. **使用方法**：<br/>   - 文档提供了如何使用Video2X进行视频超分辨率处理的方法，包括支持的API和命令行参数。<br/>   - 包括FFmpeg在内的多个项目被提及，表明这些工具在实现过程中发挥了作用。<br/><br/>2. **许可信息**：<br/>   - 该项目采用GNU AGPLv3许可证，意味着它是一个开源项目，并且遵循了该协议下的使用、修改和分发条款。<br/>   - 提到了多个依赖的开源项目的许可证（如LGPLv2.1、GNU GPLv2、BSD 3-Clause、MIT License等），这些为Video2X提供了技术基础。<br/><br/>3. **素材样例**：<br/>   - 文档提供了一些样本视频文件用于参考，包括原始视频、经过不同方法处理后的结果以及真实数据的超分辨率版本。<br/><br/>4. **感谢声明**：<br/>   - 特别感谢了对项目做出重要贡献的个人，如ArchieMeng、BrianPetkovsek等，按照字母顺序列出了他们的名字。<br/><br/>总体而言，这份文档旨在为用户提供如何使用Video2X进行视频处理的方法指引，并公开了项目的许可详情和致谢声明。 |
| [supermemoryai/supermemory](https://github.com/supermemoryai/supermemory) | Supermemory是一个用于存储和检索信息的工具。以下是对其功能和服务的详细中文概述：<br/><br/>1. **使用方法**：<br/>   - **添加记忆**：您可以直接在平台上创建并存储信息，比如从网页、社交媒体（如Twitter）或者通过其他集成的服务（如Notion、Google Drive等）导入内容。<br/>   - **聊天模式**：通过点击“打开聊天”功能，可以与Supermemory进行对话来检索之前存储的信息。<br/>   - **AI工具集成**：通过连接您喜爱的AI工具（例如ChatGPT和Claude），用户可以将Supermemory整合到其工作流程中以获取智能辅助。<br/><br/>2. **浏览器扩展**：<br/>   - **Chrome/Edge插件**：允许用户在浏览网页时直接保存信息，并与聊天机器人（如ChatGPT）互动。<br/>   - **Raycast插件**：提供键盘快捷键的支持，以便用户在Raycast中轻松添加和搜索记忆。<br/><br/>3. **支持资源**：<br/>   - **邮件支持**：使用邮箱联系支持团队进行问题查询或反馈。<br/>   - **Discord群组**：加入在线社区交流，获取帮助和支持。<br/>   - **文档中心**：查阅官方文档以了解更详细的指南、API接口和教程。<br/><br/>4. **贡献方式**：<br/>   - 开发者可以报告错误（🐛）、提出新功能建议（✨）、优化用户体验（🎨）或提高性能（⚡）。通过查看Issues页面找到适合您技能级别的项目开始参与。<br/>   <br/>5. **更新与路线图**：<br/>   - 查看官方发布的变更日志以了解最新改进和修复。<br/>   - 关注Supermemory的X账号获取动态信息和新功能预告。<br/><br/>总结，Supermemory是一个集成了AI技术的信息管理工具，通过其网站、浏览器扩展和社区支持为用户提供高效的信息检索与存储服务。它鼓励开发者社区参与贡献，共同提升平台的功能和服务质量。 |
# eess.AS updates on arXiv.org
---
| Title | Summary |
| --- | --- |
| [The Voice of Equity: A Systematic Evaluation of Bias Mitigation Techniques for Speech-Based Cognitive Impairment Detection Across Architectures and Demographics](https://arxiv.org/abs/2601.16989) | ### 贡献点:<br/><br/>1. **提出首个全面的公平性分析框架**：研究团队开发了一套针对基于语音的多类认知障碍检测的完整公平性评估体系，系统地评估了不同架构和人口子群体中的偏见缓解方法。<br/><br/>2. **构建两个基于转换器的架构**：研究中使用了两种基于转换器的模型——SpeechCARE-AGF 和 Whisper-LWF-LoRA，并在多语言NIA PREPARE Challenge数据集上进行了训练。这两种模型都具有强大的性能（F1分数分别为70.87和71.46）。<br/><br/>3. **全面评估偏见缓解方法**：与以往工作通常只关注单一的偏见缓解技术不同，本研究比较了预处理、在处理和后处理三种方法，通过平等机会（Equality of Opportunity）和等化概率（Equalized Odds）对性别、年龄、教育程度和语言等因素进行了公平性的评估。<br/><br/>4. **揭示了不同群体间的偏差**：研究发现，对于80岁及以上的成年人，敏感性较低，相比于年轻群体；西班牙语使用者在真阳性率（TPR）方面较英语使用者低。这表明，模型的性能存在明显的群体差异。<br/><br/>5. **展示不同的缓解效果**：不同架构对偏见的缓解效果不同。针对年龄较大的人群进行过采样能显著提高SpeechCARE-AGF的敏感性（80+人群的TPR从46.19%提升至49.97%），但对Whisper-LWF-LoRA的效果较小。<br/><br/>6. **强调架构设计的关键作用**：研究结果表明，模型架构在决定偏见模式和缓解策略的有效性方面起着基础性的作用。这揭示了在开发公平的语音筛查工具时必须考虑模型架构和人口统计特征的重要性。<br/><br/>7. **提出灵活的数据干预机制**：研究中提到的自适应融合机制允许对数据进行灵活的调整，以应对不同的数据干预需求。频率重加权被证明能够提供跨架构的稳健改进。<br/><br/>8. **建立系统性的公平性开发框架**：本研究为开发旨在减少认知健康领域诊断差异的公平语音筛查工具提供了系统的方法论基础。这有助于减少基于言语的检测技术中的不平等现象，对于提高医疗保健的公平性和效率至关重要。 |
| [BickGraphing: Web-Based Application for Visual Inspection of Audio Recordings](https://arxiv.org/abs/2601.17014) | ### 贡献点:<br/><br/>1. **开发了一款名为BickGraphing的浏览器基于研究工具**，用于可视化音频记录，帮助用户进行视觉检查。<br/><br/>2. **广泛应用于研究中的所有音频可视化**，不仅局限于农业害虫的声音分析，而是为生物声学和相关领域提供通用的工具。<br/><br/>3. **支持大文件上传与处理能力**，能够批量处理大量的.wav格式音频文件，并在本地计算波形和频谱图。<br/><br/>4. **具有互动性的时间和频率探索功能**，用户可以互动地浏览和分析音频事件。<br/><br/>5. **采用SvelteKit和TypeScript构建的Web应用**，使用编译到WebAssembly的FFmpeg和自定义FFT工具，实现客户端侧信号处理管道。<br/><br/>6. **开源且有标准MIT许可**，通过GitHub仓库（<https://github.com/bicklabuw/BickGraphing>）发布，并可以用于快速评估.wav录音的质量。<br/><br/>7. **可能成为研究中音频数据可视化的本地化、易于使用、无需编码的平台**。 |
| [PC-MCL: Patient-Consistent Multi-Cycle Learning with multi-label bias correction for respiratory sound classification](https://arxiv.org/abs/2601.17080) | 贡献点:<br/><br/>1. **提出PC-MCL（Patient-Consistent Multi-Cycle Learning）方法**: 该方法旨在解决自动化呼吸声分类中出现的周期级分析依赖性和患者特异性过拟合问题。通过采用多周期串联、三标签表述和患者匹配辅助任务三个关键组件，提升模型性能。<br/><br/>2. **解决多标签分布偏见**：针对传统二标签描述（爆裂音与喘鸣）在应用多周期串联时存在的一种内在问题——即当正常和异常周期混合后系统性地丢失了正常信号信息。提出三标签表述（正常、爆裂、喘鸣），通过这种方式在混合作品中保留所有组成部分的信息，解决了这一偏见。<br/><br/>3. **引入患者匹配辅助任务**：作为多任务正则化器，该辅助任务有助于模型学习更稳健的特征，从而提高泛化能力。这种设计促进了对异常呼吸事件的检测效果提升。<br/><br/>4. **IHCBI 2017基准测试表现**：PC-MCL在ICBHI（国际呼吸声挑战）2017的基准上取得了65.37%的得分，超越了现有基线模型。这表明该方法具有实际应用潜力和较高的分类准确度。<br/><br/>5. **组件重要性确认**：通过消融实验验证，所有三个关键组件——多周期串联、三标签表述和患者匹配辅助任务——在协同作用下对异常呼吸事件的检测有显著提升效果，证明了它们各自不可或缺的作用。 |
| [Recovering Performance in Speech Emotion Recognition from Discrete Tokens via Multi-Layer Fusion and Paralinguistic Feature Integration](https://arxiv.org/abs/2601.17085) | 贡献点如下：<br/><br/>1. **全面探索离散语音标记在情感识别中的应用**：论文对离散语音标记在声学情绪识别（SER）领域进行了全面研究。通过使用微调后的WavLM-Large模型，系统地量化了不同层配置和k-means量化粒度下的性能退化。<br/><br/>2. **量化信息损失问题**：指出在离散化过程中由于量化而导致的伴随语言信息丢失是限制离散语音标记应用于SER的关键障碍，并对此进行了深入探讨。<br/><br/>3. **提出解决策略**：提出了两种关键策略来恢复信息损失：<br/>   - （1）基于注意力的多层融合，用于捕获不同层间的互补信息。<br/>   - （2）将开放SMILE特征集成到模型中，以显式地重新引入伴随语言线索。<br/><br/>4. **神经编码器-解码器（Neural Codec Tokenizers）比较**：对主流的神经编码器-解码器标记器（SpeechTokenizer、DAC、EnCodec）进行了对比分析，并研究了它们在融合声学特征时的行为表现。<br/><br/>5. **研究成果验证**：通过多层融合和声学特征集成方法，论文表明离散标记可以通过缩小与连续表示之间的性能差距，在SER任务中实现性能的提高。 |
| [Spoofing-Aware Speaker Verification via Wavelet Prompt Tuning and Multi-Model Ensembles](https://arxiv.org/abs/2601.17557) | ### 贡献点:<br/><br/>1. **提出了一种集成化的防御策略** - 系统设计了基于波变换增强的XLSR-AASIST对策与多模型集成的级联式防生成伪造攻击说话人验证框架，旨在同时对讲话者身份和音频真实性进行验证。<br/><br/>2. **采用多种架构融合** - ASV组件整合了ResNet34、ResNet293和WavLM-ECAPA-TDNN等多种深度学习模型，采用了Z-score规范化后分数平均的方法来提升验证效果。<br/><br/>3. **基于真实数据的训练与评估** - 系统在VoxCeleb2和SpoofCeleb数据集上进行了训练，并据此评估了性能，取得了0.2017的Macro a-DCF值以及SASV EER为2.08%的结果。<br/><br/>4. **强调跨域泛化挑战** - 通过在ASVspoof5等未见过的数据集上的测试结果（EER为0.16%），凸显了对于不同领域数据的一致性验证的困难，这显示出系统在跨域应用时面临的局限性和挑战。 |
| [ToS: A Team of Specialists ensemble framework for Stereo Sound Event Localization and Detection with distance estimation in Video](https://arxiv.org/abs/2601.17611) | 贡献点:<br/><br/>1. 提出了一种新型多模式框架Team of Specialists (ToS)，用于三维声事件定位与检测（3D SELD），该框架旨在同时处理音频和视觉信息，以识别视频中的活跃声事件及其空间坐标。<br/><br/>2. 为解决单一模型在联合推理（跨语义、空间和时间维度）时的局限性问题，ToS将三个互补子网络整合在一起：一个时空语言模型、一个空间-时间模型和一个节奏语言模型。每个子网络专注于不同维度对之间的关系，分别擅长处理独特的跨维度关联。<br/><br/>3. ToS在DCASE2025 Task 3 Stereo SELD的开发集上与最先进的音频视觉模型进行基准测试，并表现出了一致性的高表现，在关键指标上显著优于现有的方法。<br/><br/>4. 提出了一个扩展ToS框架未来研究方向的想法，即通过为每个专业团队成员（specialist）提供适合的任务、训练和预训练课程来强化他们。这将有助于进一步优化性能，使其在3D SELD任务中更加高效和准确。 |
| [End-to-End Joint ASR and Speaker Role Diarization with Child-Adult Interactions](https://arxiv.org/abs/2601.17640) | 贡献点:<br/><br/>1. **统一的端到端框架**: 该论文提出了一种结合自动语音识别(ASR)和儿童-成人说话者角色的联合模型化方法，采用了一种整体端到端的架构，旨在同时提高转录和演讲者会话的准确性。<br/><br/>2. **序列输出训练方案**: 引入了序列化的输出训练机制，该机制能够生成演讲者标签及起始/结束时间戳，这种方案有助于在单一框架中联合优化ASR与演讲者识别任务。<br/><br/>3. **轻量级帧级聚言头**: 通过增强演讲者区分的编码器表示，提出了一种轻量化帧级聚言头部，提升模型对不同讲话者的分辨能力，并提高了时间上的精确度。<br/><br/>4. **基于迪纳机的强迫解码过程**: 实施了基于状态机的过程来确保结构上有效的输出，保证生成的内容在逻辑上是合理和可验证的。<br/><br/>5. **全面性能评估**: 论文通过在两个数据集上进行详尽的评估，展示了与现有的级联方法相比显著且一致的改进，特别是多谈者词错误率较低，并且在Whisper小模型与大模型中均展现出竞争性的聚言准确性。<br/><br/>6. **公开源代码和模型权重**: 提供了该框架的公共可访问代码及模型权重，为研究人员和开发人员提供了实用的应用工具和技术资源。 |
| [Speech Emotion Recognition with ASR Integration](https://arxiv.org/abs/2601.17901) | 贡献点:<br/><br/>1. **研究重点**: 该论文将自动语音识别(ASR)技术与语音情绪识别(SER)相结合，旨在提高从口头语言中识别情绪的鲁棒性、可扩展性和实用性。<br/><br/>2. **现实世界应用**: 论文重点关注在真实世界的、自发场景以及资源有限的情况下部署SER的技术挑战，并寻求解决方案来克服这些问题。<br/><br/>3. **理解人类沟通**: 通过改进SER技术，该研究有助于更深入地理解人类之间的沟通方式，促进具有情感智能的系统发展。<br/><br/>4. **人工智能领域应用**: 论文探索了SER在开发人工通用智能(AGI)中的基础作用和重要性，旨在推动这一领域的进展。<br/><br/>5. **复杂情绪表达处理**: 鉴于情感表达的复杂性，论文针对当前语音和语言技术的局限性，提出整合ASR到SER中以提升情感识别能力的方法。 |
| [AmbER$^2$: Dual Ambiguity-Aware Emotion Recognition Applied to Speech and Text](https://arxiv.org/abs/2601.18010) | ### 贡献点:<br/><br/>1. **提出AmbER$^2**: 首次引入了针对多模态情绪识别任务的双层模糊模型, 即同时关注评价者和模态级别的模糊性。<br/><br/>2. **教师-学生架构与分布式训练目标**：使用一种名为教师-学生(teacher-student)的架构来同时处理评价者和模态层面的不确定性，并通过分布式的训练目标进行优化，以此提升模型性能。<br/><br/>3. **解决多模态冲突**：该框架旨在明确解决跨模态之间的冲突，而不仅仅是简单地融合特征，为情绪识别提供了更深层次的理解。<br/><br/>4. **性能与基准比较**：在IEMOCAP和MSP-Podcast等数据集上的评估表明，AmbER$^2$相较于传统的交叉熵基线模型，在分布一致性方面显著提高，并且其性能与最近的最优系统相匹配或优于它们。例如，在IEMOCAP数据集上，AmbER$^2在Bhattacharyya系数、R$^2$、准确率和F1分数上的相对改进分别为20.3%（从0.69提高到0.83）、13.6%（从0.59提升至0.67）、3.8%（从0.658增加到0.683）以及4.5%（从0.646升至0.675）。<br/><br/>5. **高不确定性样本特别受益**：通过进一步分析发现，明确地建模模糊性对于高度不确定的样本尤为重要，这突出了在构建稳健的情绪识别系统时联合处理评价者和模态层面模糊性的重大意义。 |
| [SpatialEmb: Extract and Encode Spatial Information for 1-Stage Multi-channel Multi-speaker ASR on Arbitrary Microphone Arrays](https://arxiv.org/abs/2601.18037) | 贡献点如下：<br/><br/>1. **提出了解决方案**：针对目前多声道多说话人目标语音识别中空间信息利用不足、处理流程效率低下以及受限于特定设置和设备适应性的问题，引入了一个名为SpatialEmb的轻量级嵌入模块。<br/><br/>2. **SpatialEmb模块特点**：该模块直接从原始数据中提取并编码空间信息用于ASR模型，支持固定或任意麦克风布局拓扑结构。<br/><br/>3. **实验设计与结果评估**：在实际会议场景下使用AliMeeting数据集进行了一系列全面的实验，以确定最佳的SpatialEmb模型设计方案，在性能和效率方面均进行了优化。训练105小时的数据后，最优模型在Eval和Test集上的字符错误率（CER）分别为17.04%和20.32%，达到了使用相同训练数据的新前沿水平。<br/><br/>通过以上贡献，该论文提供了一种改善多声道多说话人语音识别系统性能、提高处理效率以及增强设备适应性的方法。 |
| [OneVoice: One Model, Triple Scenarios-Towards Unified Zero-shot Voice Conversion](https://arxiv.org/abs/2601.18094) | ### 贡献点：<br/><br/>1. **统一框架OneVoice**：提出了一种名为“OneVoice”的统一的零启动框架，旨在解决语音转换（VC）领域中现有的模型专门针对语言保持、表达和歌唱场景的问题。这一框架能够在一个单一模型内处理所有三种情况。<br/><br/>2. **基于VAE-free next-patch扩散的语言连续模型**：该框架建立在使用无VAE的next-patch扩散训练的语言连续模型之上，确保了高度保真度和有效的序列建模能力。<br/><br/>3. **统一的核心设计**：OneVoice的核心设计包含了一个混合专家（MoE）结构，用于明确地建模共享转换知识和场景特定的表达性。该设计通过共享专家隔离和全局-局部线索指导的场景感知域专家分配来协调专家选择过程。<br/><br/>4. **精确条件融合机制**：采用门控机制将场景特异性韵律特征融入每层中，实现了对韵律信息的适应性使用，以提高模型在各种情况下的表现。<br/><br/>5. **双阶段渐进式训练方法**：采取了两阶段训练策略，包括基础预训练和通过LoRA（低秩调整）域专家增强场景特定性能。这种方法有助于解决数据不平衡问题（言语数据丰富而歌唱数据稀缺），并促进核心概念的实现。<br/><br/>6. **灵活性与高效性**：实验结果表明，“OneVoice”在所有三个场景中均能匹配或超越专门化的模型，并且提供了快速解码版本，仅需2步即可。这强调了其对不同场景的灵活控制能力。<br/><br/>7. **代码和模型公开**：计划不久后发布相关的代码和模型供公众使用，使得其他研究者能够基于此框架进行进一步的研究与应用。 |
| [Efficient Rehearsal for Continual Learning in ASR via Singular Value Tuning](https://arxiv.org/abs/2601.18266) | 该论文的主要贡献点如下：<br/><br/>1. **解决了连续学习中的问题**：针对自动语音识别（ASR）领域中，当进行新任务、域或演讲者适应时，可能会出现灾难性遗忘的问题。这强调了在连续学习框架下改进ASR系统的重要性。<br/><br/>2. **提出了一种低成本的存储策略**：提出了在内存中存储过去数据集子集以进行温习的方法来缓解问题。这种方法旨在降低资源成本，并克服预训练模型应用和隐私法规的限制。<br/><br/>3. **解决数据存储与成本、隐私之间的平衡**：讨论了现有基于重演方法在有限内存大小下运行时面临的挑战，即通常会导致性能下降。提出了一个策略以更经济地处理这一问题。<br/><br/>4. **引入了一种高效的方法来减少记忆需求**：提出了一种新的连续学习方法，即使是在最低限度的内存中也能保持有效。该方法分为两个阶段，并通过应用奇异值分解（SVD）对线性层的变化进行操作。<br/><br/>5. **采用参数效率的策略**：在第一阶段后，通过仅重新训练单个向量（控制从第一阶段接受更新的程度），使用重演来优化第二阶段中对主成分（由SVD产生的）的再训练过程。这种方法提高了模型在有限资源下的适应性和学习能力。<br/><br/>6. **全面评估和分析**：进行了充分的测试和分析，包括在双语和多语种基准上，以展示该方法在减小遗忘和超越现有ASR连续学习最佳实践方面的效果，即使仅使用每个先前任务的一次陈述。这表明了方法的有效性和适应性。<br/><br/>7. **实现性能提升**：论文证明了即使是在资源受限的场景下（如限于一个任务中的单个语句），该方法也能显著减少遗忘，并且在ASR连续学习领域中具有竞争力，甚至超过了当前最先进的连续学习方法。 |
| [Noise-Robust Contrastive Learning with an MFCC-Conformer For Coronary Artery Disease Detection](https://arxiv.org/abs/2601.18295) | 贡献点如下：<br/><br/>1. **创新算法应用**：提出了一种基于能量的新型多通道噪声段拒绝算法，该算法结合了心脏和噪音参考麦克风，用于在训练深度学习分类器前排除音频中非平稳噪声较大的部分。<br/><br/>2. **改进的数据处理技术**：通过在算法中引入多通道信息，提高了模型对噪声的鲁棒性。使用多种信道的梅尔频率倒谱系数（MFCCs），进一步增强了模型性能。<br/><br/>3. **临床价值提升**：方法显著提升了CAD检测任务的准确率和平衡准确率至78.4%和78.2%，相比不进行噪声段拒绝处理时分别提高了4.1%和4.3%，这在实际应用中显示出了重要的进步。<br/><br/>这些贡献展示了通过结合多通道音频信号分析、创新的数据预处理策略以及深度学习模型的优化，能够有效提升心血管疾病尤其是冠状动脉疾病的诊断准确性。 |
| [Residual Learning for Neural Ambisonics Encoders](https://arxiv.org/abs/2601.18322) | ### 贡献点:<br/><br/>1. **提出了一种结合线性编码器和神经网络的残差学习框架**，用于改进空间音频的压缩方式。这一框架旨在利用两者的优势, 线性编码器提供对信号独立且鲁棒性的优势，而神经网络可以提供更优的性能。<br/><br/>2. **使用了智能眼镜的阵列传输函数数据**，对比了一种基于UNet的传统编码模型和一个新设计的递归注意力模型在Ambisonics空间音频表示中的表现。<br/><br/>3. **发现当将神经编码器集成到残差学习框架中时，这两种神经网络模型都能以一致的方式显著超过线性基线。**这意味着通过这种方式训练的模型，在处理域内数据时表现出更优性能，并对域外数据也有所改善。<br/><br/>4. **然而，一致性分析显示，尽管使用了残差配置，所有神经编码器配置仍然在高频率方向准确编码方面存在挑战**。这表明即使是在优化后，神经网络在高频音频的方向性表示上仍需改进。<br/><br/>5. **论文为智能穿戴设备中的空间音频压缩提供了一种有效和可能的先进解决方案**，通过结合传统信号处理技术和机器学习方法来提升用户体验。<br/><br/>6. **这一研究结果对于进一步开发高保真、小型化的头戴式麦克风阵列有重要意义**。它不仅提高了对现有技术的理解，也为未来的设计提供了新的思路。 |
| [Noise-Robust AV-ASR Using Visual Features Both in the Whisper Encoder and Decoder](https://arxiv.org/abs/2601.18396) | 贡献点如下：<br/><br/>1. **提出了一种新的视觉融合方法**：该论文引入了在预训练的自动语音识别（ASR）中同时利用视觉特征于编码器和解码器的策略（dual-use），以学习音频视觉间的互动，并在解码器阶段对模态进行加权。这种简单而有效的方法旨在改进噪声鲁棒性。<br/><br/>2. **对比分析不同大小的Whisper模型**：研究了基于Whisper ASR的各种大小模型中的视觉融合方法，特别是双功能方法（dual-use），发现与典型的中等融合方法相比，在0dB信噪比下的babble噪声环境中，分别在Whisper小、中等规模模型上实现了35%和57%的相对性能改进。<br/><br/>3. **进行了模块设计和融合选项的影响评估**：通过Ablation研究（即剥离分析），探究了不同模块设计以及融合策略对系统性能的影响。结果显示，在利用1929小时的音频视觉数据进行微调后，使用Whisper中等规模模型的双功能方法，在不同的信噪比下，分别在LRS3 AV-ASR基准测试中的MUSAN babble噪声和NoiseX babble噪声环境下，实现了4.08%和4.43%的平均词错误率（WER），从而确立了新的嘈杂条件下的最先进水平。<br/><br/>总结：该论文提出的双功能视觉融合方法在音频可视自动语音识别系统中表现出色，在Whisper ASR模型上实现实质性性能提升，并为噪声环境中的ASR性能设置新标准，同时提供了对关键设计和融合策略的深入分析。 |
| [Audio Inpainting in Time-Frequency Domain with Phase-Aware Prior](https://arxiv.org/abs/2601.18535) | ### 贡献点:<br/><br/>1. **时间-频率音频填充问题的提出与解决**:<br/>   - 针对时间域中的音频修复（即填补缺失的信号样本段）和时间-频率域中缺失频谱列的重构问题，论文提出了新的方法。这种类型的问题在文献中已有发展，而本研究关注的是时间和频率双重维度的情况。<br/><br/>2. **利用瞬时频率的相位感知信号先验**:<br/>   - 引入并使用了基于相位感知的信号先验方法，该方法通过估计瞬时频率来处理时间-频率音频填充问题。这种方法在估计和修复缺失数据方面更为有效。<br/><br/>3. **优化算法设计与应用**:<br/>   - 为了解决提出的优化问题，论文采用了广义Chambolle-Pock算法。这种算法能够高效地求解所构建的优化模型，从而实现对时间-频率域音频的填充。<br/><br/>4. **方法性能评估**:<br/>   - 在客观和主观测试中，该方法被与其他时间-频率填充方法（包括深度先验神经网络和基于自回归的方法）进行了比较。结果显示，在所有评估标准下，所提出的方法都优于这些现有技术，并且在处理速度上具有显著优势。<br/><br/>5. **贡献与创新**:<br/>   - 通过提供一种新的时间和频率域的音频修复方法，该论文为音频信号处理领域带来了新颖的视角和解决方案。<br/>   - 实现了对缺失数据更准确、快速的重构，同时减少计算资源的需求，这在实际应用中具有重要意义。<br/><br/>总之，这篇论文主要贡献在于提出了一个高效且准确的时间-频率音频填充方法，通过创新的方法论和算法优化，提高了音频修复的质量，并显著减少了处理时间和计算成本。 |
| [Learning to Discover: A Generalized Framework for Raga Identification without Forgetting](https://arxiv.org/abs/2601.18766) | 贡献点:<br/><br/>1. **解决Raga识别挑战**：论文提出的方法解决了印度艺术音乐中存在众多鲜少演奏的 Ragas 无法在可用训练数据集上进行代表的问题。传统分类模型在此场景下表现不佳，因为它们假设了已知类别的封闭集合，并因此难以识别或对未见过的新 Raga 进行有意义的分组。<br/><br/>2. **应对遗忘问题**：以前的工作尝试对未见Ragas进行分类，但遇到了灾难性遗忘问题，即已学习到的Ragas知识逐渐减弱。为解决这一问题，该论文采用了一种集成学习框架，同时利用有标签和无标签音频数据，从而让模型能够发现与未见过Ragas相匹配的连贯类别，并保留对已知Ragas的知识。<br/><br/>3. **超越现有方法**：提出的模型在基准 Raga 识别数据集上的测试结果表明，它不仅优于基于NCD（核距离）的传统管道，在发现未见Ragas分类方面表现出色，而且能够同时处理见过、未见和所有Ragas类别。这为印度艺术音乐任务中的表示学习提供了新的见解。<br/><br/>4. **提供新洞察**：该方法在Ragas识别上展现出的性能超越了现有技术，并提供了对印度艺术音乐领域更深入的理解，特别是对于Raga的分类、识别和潜在的新Ragas类别的发现。 |
| [SonoEdit: Null-Space Constrained Knowledge Editing for Pronunciation Correction in LLM-Based TTS](https://arxiv.org/abs/2601.17086) | ### 贡献点:<br/><br/>1. **针对低资源语言实体的语音合成问题**: 鉴于神经文本转语音系统在处理低资源的语言实体（如非英语名称、品牌和地理地点）时普遍存在系统性发音错误的问题，SonoEdit提供了一种解决方案来纠正这些问题。<br/><br/>2. **无需额外数据或昂贵的手动标注**: 与现有方法依赖多语言数据收集、监督微调或人工音素注释不同,SonoEdit技术无需重新训练模型即可进行精确的参数更新，简化了在多元语言环境下部署TTS系统的复杂性。<br/><br/>3. **基于Null-Space Pronunciation Editing的模型编辑方法**: SonoEdit采用了一种名为Null-Space Pronunciation Editing的方法来局部地调整发音错误，这种方法通过单次参数更新修改特定单词的发音方式，并且可以证明不会影响到其他模型行为。<br/><br/>4. **适应性因果追踪（Acoustic Causal Tracing）**: 使用该方法识别了Transformer层以了解其在将文本映射为发音时的功能和作用。这有助于定位对最终发音具有直接影响的部分，从而更精确地进行调整。<br/><br/>5. **Null-Space Constrained Editing的数学优化策略**: SonoEdit中使用了Null-Space Constrained Editing技术来计算闭式形式的权重更新方法。这种方法确保在保持一般语音生成空间不变的同时纠正目标发音。通过这一约束，模型的声学输出被引导至期望的发音范例，并保证对保存的语音语料库的第一阶变化为零。<br/><br/>6. **综合解决方案**: 综上所述，SonoEdit提供了一种无需昂贵数据收集或繁琐的手动标注过程即可有效纠正低资源语言实体在神经文本转语音系统中的发音错误的方法，这对于多元化语言环境的应用特别有价值。 |
| [Sink or SWIM: Tackling Real-Time ASR at Scale](https://arxiv.org/abs/2601.17097) | ###贡献点:<br/><br/>1. **SWIM系统设计**: 提出了一种名为SWIM的实时自动语音识别系统，该系统基于OpenAI的Whisper模型构建，旨在实现模型级并行化以支持可扩展、多语言转录。SWIM特别注重在不修改底层模型的情况下同时支持多个并发音频流。<br/><br/>2. **并行处理能力**: SWIM实现了真正的模型级别并行处理，能够有效支撑多个用户的交互应用需求，并同时保持低延迟和高准确性。这一特性对于实时自动语音识别系统至关重要。<br/><br/>3. **多语言及跨客户端兼容性**: 系统支持包括英语、意大利语和西班牙语在内的多种语言转录，且在不同客户并发使用的情况下依然能提供高质量的服务。这使得SWIM具备更广泛的适用场景。<br/><br/>4. **延迟和吞吐量优化**：相较于单客户端环境下Whisper-Streaming的平均延迟大约为3.4秒及10个会话时的延迟降低至大约2.4秒，表明SWIM在多用户、多语言环境中实现了有效性能提升。在支持高达20个并发客户的情况下，SWIM依然保持了良好的准确性和较高的吞吐量。<br/><br/>5. **动态环境下的稳定性**：该系统证明了其在多用户和动态环境下对于实时语音转录的稳定性和效率均有显著提升，这标志着在扩展自动语音识别能力方面的一次重要进步。 |
| [Window Size Versus Accuracy Experiments in Voice Activity Detectors](https://arxiv.org/abs/2601.17270) | ### 贡献点:<br/><br/>1. **窗口大小对VAD算法的影响分析**: 论文通过研究在不同窗口大小下，Silero、WebRTC和基于均方根(RMS)的语音活动检测(VAD)算法的准确性。这一研究提供了优化VAD系统时考虑窗口尺寸的重要参考。<br/><br/>2. **Hysteresis技术应用于VAD输出**：论文探讨了在每种VAD输出上添加“迟滞”(hysteresis)的效果，对于WebRTC而言，引入“迟滞”的方法被证明是有益的。这为VAD系统的实际应用提供了优化策略。<br/><br/>3. **基于真实世界音频流的数据集**：通过使用一系列多样的实时数字音频流作为研究对象，论文提供了一个更贴近实际情况的研究框架，使得实验结果更具普遍适用性。<br/><br/>4. **比较分析VAD算法性能**: 详细对比了Silero、WebRTC和RMS这三种VAD算法的性能差异。发现Silero在准确性上显著优于其他两种方法，为选择合适的VAD技术提供了理论依据。<br/><br/>5. **提供优化建议**：论文不仅对现有VAD技术进行了评估，还提出了具体的优化建议，特别是针对使用“迟滞”技术提升WebRTC性能方面，为实际应用和后续研究提供了一定的指导。 |
| [EuleroDec: A Complex-Valued RVQ-VAE for Efficient and Robust Audio Coding](https://arxiv.org/abs/2601.17517) | 贡献点如下：<br/><br/>1. **复杂值RVQ-VAE音频编解码器的引入**：论文提出了一种全链路保持幅度相位耦合的复杂数值RVQ-VAE音频编码器，该方法在分析、量化和合成管道中全面保留了幅度与相位之间的关联性。<br/><br/>2. **去除了对抗式鉴别器和扩散后滤波器**：通过去除需要引入对抗式鉴别器补偿音频信号表示不足的问题，以及后续的扩散处理步骤，提高了模型训练的收敛速度和稳定性。<br/><br/>3. **无GAN或扩散过程的性能**：在不依赖于生成对抗网络（GAN）或扩散流程的情况下，与长时间训练的基线相比，本方法可以实现领域内匹配甚至超越的表现，并且在离域条件下实现了最佳的相位一致性和平面波形保真度。<br/><br/>4. **显著减少训练预算和提高计算效率**：相比于需要数十万步训练的传统基准模型，本文提出的方法将训练预算减少了至少一个数量级，同时保持了高质量的感知质量，展现出更高的计算效率。 |
| [Home Health System Deployment Experience for Geriatric Care Remote Monitoring](https://arxiv.org/abs/2601.17608) | 论文的贡献点主要包括以下几个方面：<br/><br/>1. **老龄化社会背景下的远程照护需求**：<br/>   - 描述了在支持老年人在家安全、健康地生活（即“老龄化的居家护理”）过程中，成年子女作为非正式看护者面临的挑战和需求。<br/>   - 强调了隐私保护的连续监测系统对于提供实时活动监控及直观、实用信息的重要性。<br/><br/>2. **远程监控系统的部署经验**：<br/>   - 通过三个迭代阶段展示了远程监控系统的实际部署过程，包括硬件、模型和用户界面等方面的改进。<br/>   - 指出了Geriatric 4Ms框架（关注事物对老年人最重要、认知能力、移动性及用药情况）在指导系统优化中的作用。<br/><br/>3. **LLM辅助解决方案的开发**：<br/>   - 展示了一种利用大型语言模型（LLM）来平衡用户体验（隐私保护和易用性）与系统性能的方法。<br/>   - 表明了技术，尤其是人工智能和机器学习技术，在设计适用于非正式看护者需求的远程监控系统中的应用潜力。<br/><br/>4. **多方面的改进与提升**：<br/>   - 突出了在硬件、模型和用户界面等方面进行的迭代优化策略。<br/>   - 说明了通过反馈循环和实际使用经验，可以持续改善远程护理技术的有效性和适用性。 |
| [AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs' Contextual and Cultural Knowledge and Thinking](https://arxiv.org/abs/2601.17645) | ### 贡献点：<br/><br/>1. **定义新基准**：作者引入了“AVMeme Exam”，这是一个由人类编目的音频和视频内容的大型数据库，包括超过一千个标志性互联网声音和视频片段。这些内容涵盖了语音、歌曲、音乐和音效，并附有独特的问答集，评估理解范围从表面信息到上下文、情感以及使用方式和世界知识。<br/><br/>2. **全面的元数据**：每个Meme都配有详细的元数据，包括原始发布年份、文字脚本、摘要和敏感度等级等，为研究提供了丰富的背景信息。<br/><br/>3. **多模态大语言模型（MLLMs）评估**：使用“AVMeme Exam”基准对当前最先进的多模态大型语言模型进行系统性评估，并将结果与人类参与者的测试成绩进行了对比。<br/><br/>4. **识别理解局限性**：研究发现，现有的AI模型在处理无文本音乐和音效方面表现不佳，且在理解和适应文化上下文中遇到困难。这表明了当前AI模型在感知深度和文化层面的能力存在缺陷。<br/><br/>5. **启发与未来展望**：研究结果揭示了一种关键差距——人类对齐的多模态智能，并呼吁发展能够超越听觉和视觉表面理解情境和文化的模型，以解决这一问题。 |
| [BanglaRobustNet: A Hybrid Denoising-Attention Architecture for Robust Bangla Speech Recognition](https://arxiv.org/abs/2601.17679) | 贡献点如下：<br/><br/>1. **针对语言代表性问题** - 该论文关注了孟加拉语（Bangla）在先进自动语音识别（ASR）研究中的不足，特别是对于噪声和演讲者多样性条件下的应用。<br/><br/>2. **提出BanglaRobustNet框架** - 开发了一种结合去噪和注意力机制的混合型模型架构，该架构基于Wav2Vec-BERT构建，旨在解决上述问题。此框架通过集成基于扩散的降噪模块来抑制环境噪声并保留孟加拉语特有的音素提示。<br/><br/>3. **增强鲁棒性** - 框架中包含了上下文交叉注意力模块，可以对演讲者嵌入进行条件处理，以提高在性别、年龄和方言方面的鲁棒性。<br/><br/>4. **集成多目标损失函数训练** - BanglaRobustNet通过组合CTC（连接词图）损失、音素一致性与演讲者对齐等复合目标来实现端到端的训练，以优化模型性能。<br/><br/>5. **显著性能提升** - 相对于Wav2Vec-BERT和Whisper基线模型，在语音错误率（WER）和字符错误率（CER）上取得了显著改进。<br/><br/>6. **实证评估与应用验证** - 通过在Mozilla Common Voice Bangla数据集以及噪声增强的数据上进行的评估，证明了该方法的有效性，并将其确立为适用于资源有限、噪音密集语言环境的鲁棒ASR系统。 |
| [Segment Length Matters: A Study of Segment Lengths on Audio Fingerprinting Performance](https://arxiv.org/abs/2601.17690) | ### 贡献点:<br/><br/>1. **探讨音频指纹性能与段落长度的关系**: 通过研究不同段落长度对音频指纹性能的影响, 研究发现了较短的段落长度（0.5秒）在实践中通常能获得更好的识别效果。<br/><br/>2. **神经网络在音频指纹中的应用**: 针对现代神经方法通常操作于短、固定时长的音频片段这一特征，研究扩展了现有的神经指纹架构，并评估了不同段落长度和查询持续时间下的检索准确率。<br/><br/>3. **利用大语言模型推荐最佳段落长度**: 通过对三个研究的大语言模型（LLM）进行实验，研究发现GPT-5-mini在考虑五个因素时，能够提供最优的建议选择，用于确定最佳段落长度，显示了使用LLM来优化音频指纹性能和系统设计的有效性。<br/><br/>4. **为大规模神经音频检索系统提供实用指导**: 基于上述发现, 研究提供了关于如何在大型神经网络驱动的音频检索系统中选择合适的段落时长的实践指南。 |
| [CaSNet: Compress-and-Send Network Based Multi-Device Speech Enhancement Model for Distributed Microphone Arrays](https://arxiv.org/abs/2601.17711) | 该论文的主要贡献包括以下几个方面：<br/><br/>1. **提出了一种适应资源受限分布式麦克风阵列（DMA）的压缩和发送网络（Compress-and-Send Network，CaSNet）**。此方法在语音增强领域提供了一个创新性解决方案。<br/><br/>2. **设计了一种适用于DMA系统的新型工作流**。在此系统中，一个麦克风作为融合中心（FC）与参考点，并负责处理信息。其他设备则各自对测量的原始数据进行编码并生成特征矩阵，再通过奇异值分解（SVD）来压缩这些数据，形成更为紧凑的数据表示。<br/><br/>3. **通过跨窗口查询实现了在FC处接收特征的对齐**。这一过程使FC能够根据参考点的数据对收到的数据进行校准和整合。<br/><br/>4. **采用神经解码方法生成空间上协调一致的增强语音**。这确保了经过CaSNet处理后的语音质量能够与未压缩情况下的效果相比，仅在性能上具有微小影响甚至几乎无差别。<br/><br/>5. **通过实验验证了CaSNet的有效性和效率**。论文利用多个数据集进行了实验，结果显示CaSNet能够在降低数据量的同时保持语音增强的性能水平。<br/><br/>6. **提供了可复现代码的链接**。论文作者在GitHub上分享了用于实现CaSNet的可复现实验代码（https://github.com/Jokejiangv/CaSNet），这为其他研究者和开发者提供了实际操作和验证此方法的可能性。 |
| [dLLM-ASR: A Faster Diffusion LLM-based Framework for Speech Recognition](https://arxiv.org/abs/2601.17902) | 贡献点:<br/><br/>1. **提出dLLM-ASR框架** - 引入了一种基于离散扩散大型语言模型（dLLMs）的自动语音识别（ASR）新方法，旨在通过将dLLM用于解码过程，同时克服其逐令牌生成机制导致的线性增长推理延迟问题。<br/><br/>2. **解决文本生成与ASR需求之间的不匹配** - 解决了直接应用面向文本的dLLMs到ASR时遇到的根本性不匹配问题。即，在开放式的文本生成与需要根据语音信号条件化的转录框架之间存在差异，这引入了不必要的复杂性和计算冗余。<br/><br/>3. **高效解决难点** - 通过将dLLM的解码过程视为一个由先验引导和自适应去噪的过程来处理上述问题。利用ASR的先验信息初始化去噪过程，并提供序列长度的锚点。<br/><br/>4. **引入动态修剪机制** - 实现了基于长度适配的修剪动态去除冗余令牌，增强了模型在不同任务场景下的效率。<br/><br/>5. **采用信心驱动的去噪策略** - 使合并（即已收敛）的令牌可以早期退出去噪循环，允许逐个令牌级别的自适应计算，从而优化计算资源分配。<br/><br/>6. **实验验证有效性与效率** - 实验结果表明dLLM-ASR在识别准确性上与基于自回归大型语言模型的传统ASR系统相当，并提供了4.44倍的推理速度提升。这确立了适用于ASR的实用和高效方法，显著提升了语音识别系统的实际操作性能。<br/><br/>###中文总结：<br/>该论文主要贡献在于提出了一种名为dLLM-ASR的新框架，将离散扩散大型语言模型应用于自动语音识别中，并通过一系列策略有效解决了文本生成与ASR需求之间的不匹配问题。实验验证了此方法在保持高识别精度的同时显著提高了推理速度，实现了对语音识别领域的技术进步和实际应用价值的提升。 |
| [From Human Speech to Ocean Signals: Transferring Speech Large Models for Underwater Acoustic Target Recognition](https://arxiv.org/abs/2601.18086) | 贡献点如下：<br/><br/>1. **研究领域创新**：论文探讨了将用于大规模人类语音数据集训练的语音大模型（SLMs）应用于水下声学目标识别（UATR），这是一个具有挑战性的领域，因为它受限于标注数据不足和海洋环境的复杂性。<br/><br/>2. **提出新框架**：作者提出了一个名为UATR-SLM的新框架，该框架利用了现有的语音特征管道、将大模型作为声学编码器，并附加了一个轻量级分类器。这种设计使得模型能够适应水下声学环境的需求。<br/><br/>3. **实验验证有效性**：通过在DeepShip和ShipsEar基准测试中进行的实验证明，UATR-SLM能够在领域内达到超过99%的准确率，在不同信号长度下具有强大的鲁棒性，并且在跨域评估中的准确率达到96.67%。这些结果强调了大模型（SLMs）在水下声学目标识别任务上的高可移植性。<br/><br/>4. **建立新范式**：论文表明，通过利用语音基础模型的可移植性，在水下声学领域中使用预先训练的大型语言模型具有极大的潜力和可能性。这为该领域的研究者提供了一种新的、有前景的研究方向或方法论。 |
| [VIBEVOICE-ASR Technical Report](https://arxiv.org/abs/2601.18184) | ### 贡献点:<br/><br/>1. **VibeVoice-ASR框架**: 提出了一种名为VibeVoice-ASR的通用语音理解框架，旨在解决长音频（如会议、播客等）中的上下文碎片化和多说话者复杂性问题。该框架相较于传统的流水线方法，不依赖于音频分段，能够实现长达60分钟的音频单次通过处理。<br/><br/>2. **一体化任务设计**: VibeVoice-ASR将自动语音识别(Automatic Speech Recognition, ASR)、发言人对话语者归属分析(Speaker Diarization)和时间戳生成整合为一个端到端的生成任务，实现了一个多模块统一化的工作流程。<br/><br/>3. **多语言支持**: 支持超过50种语言，并且无需显式设置语言即可使用。该框架能够原生处理句子内的代码切换以及跨句的代码切换。<br/><br/>4. **上下文注入机制**: 引入了一种基于提示的上下文注入机制，允许用户提供自定义的上下文信息，这极大地提高了在特定领域术语和多音节角色歧义上的准确性。<br/><br/>### 总结:<br/>VibeVoice-ASR框架不仅解决了一些长期存在的语音理解挑战，如长音频中的上下文问题和多说话者复杂性，并且通过其一体化设计、跨语言支持以及创新的上下文注入机制，在自动语音识别领域迈出了重要一步。这一框架有望为各种应用提供更准确、高效的语音处理能力，尤其是在需要处理多语言和多说话者对话的场景中。 |
| [LLM-ForcedAligner: A Non-Autoregressive and Accurate LLM-Based Forced Aligner for Multilingual and Long-Form Speech](https://arxiv.org/abs/2601.18220) | 贡献点如下：<br/><br/>1. **跨语言适应性（Cross Language）**：LLM-ForcedAligner方法能够应用于多语言、跨语言和长序列的语音场景，利用了大型语言模型在多语言理解和长时间序列处理方面的优势。<br/><br/>2. **改进的语言独立性**：现有的对齐方法通常针对特定的语言，而LLM-ForcedAligner则通过将时间戳作为离散索引，并在转录中插入特殊的时间戳令牌进行改革，减少了语言的依赖性和累积的时间偏移问题。<br/><br/>3. **避免直接应用下一词预测模式带来的问题**：通过将FA重新定义为填充槽的范式，即把时间戳视为离散索引来处理，并对SLLM进行了调整以直接预测包含在转录中的槽位的时间索引，从而解决了直接使用SLLM进行下一句预测时产生的幻觉现象。<br/><br/>4. **自适应时间戳插入**：动态插槽插入技术允许LLM-ForcedAligner在任意位置执行对齐操作，提高了其灵活性和适用性。<br/><br/>5. **非自回归推理支持**：通过支持非自回归推理，方法避免了产生错误预测（幻觉现象）并显著提高了推断速度。<br/><br/>6. **性能提升**：实验结果表明LLM-ForcedAligner相比先前的方法在累积平均偏移量上实现了69%至78%的相对减少。<br/><br/>7. **公开资源**：作者表示将在后续阶段发布该方法的检查点和推理代码，以供社区使用和进一步研究。 |
| [OCR-Enhanced Multimodal ASR Can Read While Listening](https://arxiv.org/abs/2601.18393) | 论文的贡献点如下：<br/><br/>1. **提出Donut-Whisper模型**：一种结合了视觉和听觉信息的自动语音识别（ASR）模型，它采用双编码器结构来利用视觉信息以提升在英语和汉语两种语言下的ASR性能。<br/><br/>2. **交叉注意力模块融合优势**：通过将线性结构与基于Q-Former的方法相结合，Donut-Whisper使用跨注意力机制生成更强大的音频-视觉特征。这种设计使模型能够充分利用听觉和视觉数据的互补优势。<br/><br/>3. **轻量级知识蒸馏方案**：提出一种新的、针对ASR任务的知识蒸馏方法，展示出使用音频-视觉模型来训练仅依赖于音频信息的模型可以获得更好的性能结果的可能性。<br/><br/>4. **开发多语言音频-视觉语音识别数据集**：基于包含中英双语部分的电影片段创建了一个新的人工智能数据集。这个数据集为评估和改进语音识别系统的跨语言能力提供了标准化的方法。<br/><br/>5. **显著提升ASR性能**：Donut-Whisper在用于测试的数据集的英语和汉语分区上都达到了显著的性能提升，与Donut和Whisper大型V3基线模型相比分别实现了绝对减少5.75% WER（词错误率）和16.5% CER（字符错误率）。 |
| [Pisets: A Robust Speech Recognition System for Lectures and Interviews](https://arxiv.org/abs/2601.18415) | ### 贡献点:<br/><br/>1. **开发了一种名为"Pisets"的多组件语音转文本系统**，旨在提高语音识别的准确度，并减少与Whisper模型相关的错误和幻觉。<br/>   <br/>2. **采用了Wav2Vec2进行主要的语音识别**。通过这一方法提高了系统的整体性能。<br/><br/>3. **引入了Audio Spectrogram Transformer (AST)进行假阳性过滤**，以进一步增强系统的精确性，减少了误报。<br/><br/>4. **最终使用Whisper进行语音识别**作为系统架构的最后一环，确保高质量的文本转换。<br/><br/>5. **实施了课程学习方法**，通过这一技术优化了模型在不同情况下处理长音频数据的能力。<br/><br/>6. **利用多样的俄语语音语料库**，这不仅提高了系统的适应性，而且在特定语言环境下表现出了优势。<br/><br/>7. **引入了高级不确定性建模技巧**，提升了转录质量。这意味着系统能够更准确地理解和转换语音输入，并对不确定性有更合理的处理方式。<br/><br/>8. **相比WhisperX和常规的Whisper模型**，“Pisets”系统在不同声学条件下展现了更强的鲁棒性（robustness）。<br/><br/>9. **提供了一个开源代码库**，使“Pisets”系统的实现可以在GitHub上公开获取。这意味着研究人员和开发人员可以访问、研究、改进或部署该系统。<br/><br/>通过这些贡献，该论文不仅推动了语音识别技术的发展，还为科学研究和新闻报道等实际应用提供了高效、准确的工具。 |
| [Geneses: Unified Generative Speech Enhancement and Separation](https://arxiv.org/abs/2601.18456) | 贡献点如下：<br/><br/>1. **提出了一种统合的、高质量的语音增强（SE）与语音分离（SS）方法**：“Geneses”，这是一种生成框架，旨在实现统一且具有高度质量的SE-SS。它通过利用多模式扩散转换器来估计每个说话者纯净语音特征，并以此为基础进行条件化学习，从而在受到噪声混合影响的情况下也能有效处理。<br/><br/>2. **使用了基于自监督学习的方法**：Geneses方法采用了自监督学习代表，这是从嘈杂混合中获取信息的一种高效方式。这有助于模型更好地理解并针对实际世界中的复杂降级情况做出反应。<br/><br/>3. **实验评估和性能比较**：进行了在LibriTTS-R数据集下的二元说话人混合物条件下的实验评估，分别在仅噪声干扰和复杂降级的情况下进行。结果显示，“Geneses”方法在各种客观指标上均表现出显著的优越性，并且具有很强的鲁棒性，能够很好地应对复杂的降级情况。<br/><br/>4. **提供音频样本**：研究团队提供了可访问的示例音频文件，这为验证其性能和效果提供了实际证明，使得“Geneses”的应用和效用更加直观和具体。 |
| [MELA-TTS: Joint transformer-diffusion model with representation alignment for speech synthesis](https://arxiv.org/abs/2509.14784) | ### 贡献点：<br/><br/>1. **MELA-TTS框架的提出**：引入了一种名为MELA-TTS（联合转换器-扩散框架）的新型端到端文本转语音合成方法，该方法通过自回归方式生成连续mel频谱帧，并且不需要进行语音令牌化和多阶段处理流程。<br/><br/>2. **消除传统限制**：通过这种方式，MELA-TTS架构克服了传统的TTS系统需要依赖于复杂的多阶段处理管道的局限性，简化了文本转语音过程。<br/><br/>3. **提出代表对齐模块（Representation Alignment Module）**：为了应对连续特征建模中的固有困难，该论文提出了一个在训练过程中将转换器解码器的输出表示与预训练ASR（自动语音识别）编码器的语义嵌入对齐的机制。这个机制不仅加速了训练过程的收敛性，还增强了文本和声学域之间的跨模体一致性。<br/><br/>4. **提高性能与稳定性**：MELA-TTS在多个评估指标上均表现出卓越的性能，并且同时保持了强大的零样本语音克隆能力，在离线合成和流媒体合成模式下都表现出了较高的鲁棒性。<br/><br/>5. **建立新基准**：该工作为连续特征生成方法在TTS（文本转语音）领域设立了新的评价标准，提供了一种有吸引力的、对离散令牌基框架的替代方案。这表明MELA-TTS不仅在技术上具有创新性，而且对于评估和优化其他TTS系统有着重要的参考价值。<br/><br/>综上所述，该论文通过提出MELA-TTS模型，不仅推进了文本转语音合成领域的技术前沿，还为相关研究提供了新的方法论和性能指标基准。 |
| [VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency](https://arxiv.org/abs/2509.15969) | 1. **VoXtream模型的介绍**：本文介绍了一种全自回归、无需预训练的流式文本转语音（TTS）系统，名为VoXtream。该系统在实时场景中能够从第一词开始播放声音。<br/><br/>2. **直接映射机制**：VoXtream通过单调对齐方案和有限的向前查看机制将传入的音素直接映射为音频令牌，这一设计避免了启动延迟。<br/><br/>3. **模型架构**：该系统围绕三种核心组件构建，包括增量音位变换器、用于预测语义和持续时间令牌的时间变换器以及生成声学令牌的深度变换器。<br/><br/>4. **最低初始延迟**：据我们所知，VoXtream具有当前可用流式TTS系统中最低的初始化延迟，在GPU上仅为102毫秒。<br/><br/>5. **性能与规模对比**：虽然在中等规模、9k小时的数据集上进行训练，但VoXtream在多个指标上与其他较大基线相匹配或超越，并且在输出和全流式设置下提供了竞争力的品质。<br/><br/>6. **可访问性**：项目提供了演示和代码供公众访问，相关链接为https://herimor.github.io/voxtream。 |
| [ARTI-6: Towards Six-dimensional Articulatory Speech Encoding](https://arxiv.org/abs/2509.21447) | ### 贡献点:<br/><br/>1. **ARTI-6框架提出**: 提出了一种名为ARTI-6的紧凑型六维语音编码架构，从实时MRI数据中提取关键发声腔区域信息，包括软腭、舌根和声带。<br/><br/>2. **核心特征集构建**: 构建了一个包含六个维度的语音活动体特征集，代表了发声腔的关键部位。<br/><br/>3. ** articulatory inversion模型开发**: 开发了一种articulatory inversion模型，能够利用语音基础模型从语音声学预测语音活动体特性，预测相关性达到0.87。<br/><br/>4. **articulatory synthesis模型创建**: 创立了一个articulatory synthesis模型，能够直接从语音活动体特征重建可理解的言语信息，表明即使是低维表示也可以生成自然音质的声音。<br/><br/>5. **框架的全面性**: ARTI-6提供了一种具有可解释性、计算效率高且生理基础坚实的框架，推进了语音反转、合成以及更广泛的语音技术应用领域的发展。<br/><br/>6. **开源资源支持**: 公开提供了该研究的源代码和语音样本，便于其他研究人员复现结果或进一步开发。 |
| [TASU: Text-Only Alignment for Speech Understanding](https://arxiv.org/abs/2511.03310) | ###贡献点:<br/><br/>1. **创新的文本导向语音理解对齐方法(TASU)**: 提出了TASU这一新型的对齐范式，它能够仅通过未配对的文字数据来指导跨模态对齐。这是针对当前主要依赖大量音频-文本配对数据和计算密集型训练的传统对齐方式的一种改进。<br/><br/>2. **零样本语音识别竞争力**: 实验结果显示，TASU在零样本语音识别任务中表现出竞争性性能，这表明它能够有效地利用未见的领域或任务进行学习与应用。<br/><br/>3. **作为课程学习预训练阶段的应用**: TASU被证明可以作为一个有效的预训练阶段应用于课程学习过程中，特别有助于增强语音识别任务中的域泛化能力。<br/><br/>4. **广泛的语音理解任务零样本推广**: 通过TASU, 零样本的泛化能力能够扩展到多种不同的语音理解任务，显示出在广泛的应用场景下的高效和可扩展性。<br/><br/>5. **对当前主流语音大型语言模型的超越**: 在MMSU基准测试中，TASU显著优于GLM-4-Voice和Step-Audio等主流语音大型语言模型，确立了其作为语音大型语言模型高效且具有扩展性的对齐范式的地位。 |
| [How Far Do SSL Speech Models Listen for Tone? Temporal Focus of Tone Representation under Low-resource Transfer](https://arxiv.org/abs/2511.12285) | 论文的贡献点主要集中在以下几个方面：<br/><br/>1. **研究对象**：论文将目光聚焦于在自监督学习（SSL）语音模型中对词汇声调的研究，尤其是探索除普通话以外的语言。具体涉及了四种具有复杂和多样化声调系统的语言，包括缅甸语、泰语、老挝语和越南语。<br/><br/>2. **初步估计**：提出了对声调线索持续时间的初步估算，即大约100毫秒（缅甸语/泰语）和约180毫秒（老挝语/越南语），作为研究的基础参考。<br/><br/>3. **自监督模型性能分析**：通过探针和梯度分析在微调后的SSL模型上，发现声调的迁移因下游任务的不同而有所变化。自动语音识别的微调与语言特定的声调线索相匹配，而涉及语音和语音相关性任务则倾向于过于延长持续时间。<br/><br/>4. **任务对音高聚焦的影响**：研究结果表明，声调的转移受下游任务的影响较大，并指出在声调建模中任务特性会影响时间聚焦。这些发现强调了任务效应对时间关注点的影响，揭示了自监督学习模型在处理不同语言和任务时对于声调特征识别与处理的不同表现。<br/><br/>通过这一系列研究，论文不仅丰富了我们对自监督学习方法在非普通话语境下的理解，同时也为后续的研究提供了重要的方向和参考。 |
| [XLSR-MamBo: Scaling the Hybrid Mamba-Attention Backbone for Audio Deepfake Detection](https://arxiv.org/abs/2601.02944) | 以下是该论文的贡献点：<br/><br/>1. **提出XLSR-MamBo框架**：引入了一种模块化架构，将XLSR前端与协同Mamba-Attention后端相结合，旨在提高音频伪造检测（ADD）的技术。<br/><br/>2. **探索混合架构的可扩展性**：通过实验评估了四种高级状态空间模型变体（Mamba、Mamba2、Hydra和Gated DeltaNet），以此来研究如何提升音频合成系统的性能。<br/><br/>3. **系统性比较设计**：对上述四种设计进行了全面比较，以找到最优的设计组合。<br/><br/>4. **关键性能结果**：使用ASVspoof 2021 LA、DF和In-the-Wild基准测试了XLSR-MamBo的配置，并且显示配置MamBo-3-Hydra-N3能够与现有最先进的系统相媲美，特别是在对抗性合成方法上。<br/><br/>5. **高效时间依赖捕获**：Hydra的原生双向建模能力有助于更有效地捕捉整体时域相关性，相比于先前工作中使用的手动双支路策略。<br/><br/>6. **跨领域泛化能力**：XLSR-MamBo框架在DFADD数据集上的评估表明其能够有效处理未见过的扩散和流匹配合成方法。<br/><br/>7. **深度骨干的有效作用**：分析揭示了增加架构深度能有效地减少浅层模型中性能波动和不稳定性的现象，增强了系统的鲁棒性与稳定性。<br/><br/>8. **综合贡献**：该框架通过捕捉音频伪造中的特征，提供了有效的ADD方法，为音频安全领域带来了潜在的应用价值。 |
| [Sound event localization and classification using WASN in Outdoor Environment](https://arxiv.org/abs/2403.20130) | 论文的贡献点如下：<br/><br/>1. **多麦克风阵列融合** - 提出了一种基于深度学习的声音事件定位与分类方法，使用多个麦克风阵列来改善信号衰减和环境噪声的影响，扩展了监测范围。<br/><br/>2. **引入Soundmap特性** - 利用该特性能捕捉跨多个频率带的声学空间信息，为理解声音源的空间分布提供了一种更全面的方式。<br/><br/>3. **采用Gammatone滤波器** - 生成更适合户外环境的声学特征，增强在自然或复杂环境中定位和分类声音事件的能力。<br/><br/>4. **集成注意力机制** - 学习音频特征中的通道间关系以及时间依赖性，增强了对复杂声场的理解和响应能力。<br/><br/>5. **实验设计与评估** - 使用包含不同噪声级别、监测区域大小、麦克风阵列类型及源位置的模拟数据集进行实证研究，验证了方法的有效性和优越性。<br/><br/>6. **结果对比与分析** - 详细比较了提出的方法与当前最先进的方法在声事件分类和声音源定位任务上的性能，并提供了对观测误差原因的深入探讨。 |
| [Adaptable Symbolic Music Infilling with MIDI-RWKV](https://arxiv.org/abs/2506.13001) | 贡献点如下：<br/><br/>1. **自动音乐生成研究聚焦**：现有工作主要集中在端到端系统上，这些系统用于生成完整的乐曲或作品的延续部分。这类方法对作曲家来说迭代性较差。<br/><br/>2. **计算机辅助作曲领域**：与之相对的是，在生成模型融入现有创作流程（即计算机辅助作曲）这一领域相对较少探索和开发。<br/><br/>3. **研究目标**：这项研究旨在通过处理音乐风格调整、多轨长上下文以及符号音乐填充的可控性问题，来提升计算机辅助作曲过程。目的是使自动化系统与人类协同工作更有效率，并且保持音乐创作的一致性。<br/><br/>4. **MIDI-RWKV模型**：研究中使用了一种基于RWKV-7线性架构的小型基础模型（MIDI-RWKV），旨在实现边缘设备上的高效和连贯的音乐协作生成。这个模型特别设计用于在边缘设备上处理实时的音乐创作任务。<br/><br/>5. **风格调整方法**：展示并验证了MIDI-RWKV的一种有效策略，即在样本数量非常低的情况下，通过微调其初始状态来实现音乐风格的快速适应和改变。<br/><br/>6. **性能评估与开源共享**：研究结果包括对现有模型进行多方面的定量和定性评估，并在GitHub上公开发布MIDI-RWKV的模型权重和代码，以促进学术和行业内的进一步研究和应用。 |
| [From Contrast to Commonality: Audio Commonality Captioning for Enhanced Audio-Text Cross-modal Understanding in Multimodal LLMs](https://arxiv.org/abs/2508.01659) | 贡献点:<br/>1. **提出Audio Commonality Captioning (ACC)模型**: 作者引入了 ACC 模型作为增强音频与文本跨模态理解的手段，特别是在预训练和微调多模态语言模型（MLLMs）时。相比 Audio Difference Captioning (ADC)，ACC 更注重于捕捉音频剪辑间的共享语义，而不仅仅是细微差别，从而提供了一种更为平衡的方法来处理通用性和任务特定性能之间的关系。<br/><br/>2. **解决AC与ADC间的问题**: 文章指出 ADC 在促进模型对输入音频（可能包含多种事件）差异的识别时存在一个语义鸿沟。这种偏差使得 ACC 作为一种更温和且具有挑战性的替代方案成为可能，它专注于捕捉剪辑间的共享语义而非详细差别，从而提高了模型对不同语音和音乐任务的一般能力。<br/><br/>3. **增强音频-文本理解**: 实验结果显示，ACC 不仅在captioning基准测试中提升了音频与文本的跨模态理解能力，而且能够更好地保持在多样化语音和音乐任务上的通用能力。这证明了 ACC 在多模态理解方面的能力，并且能实现更稳健的跨模态理解以及更好的泛化性和任务特定性能之间的平衡。<br/><br/>4. **改善预训练过程**: 通过引入 ACC，文章指出可以解决传统 ADC 方法中导致“灾难性遗忘”的问题。即，在预训练过程中，模型能够更好地保存和利用共享语义，而不是仅依赖于详细的音频差异描述，从而提高了 MLLMs 的跨模态理解能力。 |
| [How Does a Deep Neural Network Look at Lexical Stress in English Words?](https://arxiv.org/abs/2508.07229) | ### 贡献点：<br/><br/>1. **构建数据集**：研究团队从朗读和自发性口语中自动构建了一个英文字节词的数据集，用于预测音节中的重音位置。<br/><br/>2. **CNN模型应用**：采用了几种卷积神经网络（CNN）架构来训练模型，用于仅从缺少最小重音对的二元词的光谱图表示中预测重音的位置。此模型在保留测试数据上实现了高达92%的准确率。<br/><br/>3. **可解释性分析**：利用层间相关传播（LRP）技术对CNN进行可解释性分析，揭示了用于预测保留最小配对（如PROtest vs. proTEST）时，预测主要受到重音与非重音音节中的信息影响，特别是重音元音的频谱特性。<br/><br/>4. **深入特征分析**：提出了一个针对特定特征的相关性分析，并得出结论，表现最佳的分类器强烈地依赖于重音元音的第一和第二形态因子，还有证据表明其音高以及第三形态因子也有所贡献。这揭示了深度学习从自然发生的数据中获取分布式的重音线索的能力。<br/><br/>5. **理论与实践结合**：研究结果扩展了基于高度控制刺激的传统语音学工作，通过使用来自自然数据的实例来理解并解释神经网络决策背后的机制。 |
| [DISPATCH: Distilling Selective Patches for Speech Enhancement](https://arxiv.org/abs/2509.15922) | 贡献点如下：<br/><br/>1. **Distilling Selective Patches (DISPatch)**: 提出了一个新的知识蒸馏（KD）框架，用于语音增强领域。该框架通过比较教师模型和学生模型在不同区域的性能差异来指导优化过程。仅对那些教师优于学生的、确定为“知识差距分”高的Spectrogram小块应用知识蒸馏损失。<br/><br/>2. **最小化错误模仿与最大化潜在改进**：该方法避免了传统KD方法中学生完全模仿教师输出的问题，而是有选择性地关注和优化那些学生可以显著提高性能的区域。这样做的目的是减少对教师在不擅长或提供不可靠指导区域的影响。<br/><br/>3. **Multi-Scale Selective Patches (MSSP)**: 引入了一种频率依赖的方法（MSSP），通过使用不同大小的Spectrogram小块来考虑低频和高频带的谱差异，这更好地适应了语音增强过程中的频率异质性问题。<br/><br/>4. **整合与性能提升**：将DISPatch方法集成到传统的KD方法中，观察到了在紧凑型学生模型上的一致性增益。通过将MSSP和DISPatch结合到最先进的频率依赖的KD方法中，整体性能得到了显著提升，并涵盖了所有评估指标。<br/><br/>5. **优化方向明确与效率提升**：通过DISPatch策略，KD框架更加高效地指导了对关键改进区域的关注，避免了低效模仿和减少误导性的指导，从而提升了语音增强模型的整体性能和效率。 |
| [Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing](https://arxiv.org/abs/2509.17052) | ### 贡献点:<br/><br/>1. **Sidon模型引入**:<br/>   - 开发出一个快速的、开源的语音恢复模型，名为Sidon。<br/>   - Sidon能将野外录制的嘈杂语音转换为专业级别的清晰语音，并且可以扩展到多种语言。<br/><br/>2. **双模型架构**:<br/>   - 该模型由两部分组成：w2v-BERT 2.0进行特性的净化处理，从嘈杂语音中提取出干净的特性；使用合成器对经过清理的特征生成恢复后的语音。<br/><br/>3. **与Miipher性能媲美**:<br/>   - Sidon在语音恢复性能上达到了与Google内部的语音恢复模型Miipher相匹敌的表现。<br/>   - 旨在通过数据集清理来优化语音合成过程中的效果。<br/><br/>4. **计算效率高**:<br/>   - Sidon运行非常高效，相较于实际时间，在单个GPU上的速度可提升至500倍。<br/><br/>5. **零样本训练TTS模型的改进**:<br/>   - 提出使用Sidon清理的自动语音识别数据集来训练文本转语音（TTS）模型，即使是在完全未接触过新数据的情况下，也能提高合成语音的质量。<br/><br/>6. **开源与可复现性**:<br/>   - Sidon项目代码和模型向研究社区开放，促进数据集清理过程的可复现性。 |
| [SingMOS-Pro: An Comprehensive Benchmark for Singing Quality Assessment](https://arxiv.org/abs/2510.01812) | 贡献点如下：<br/><br/>1. **引入SingMOS-Pro数据集**：为自动评估歌唱质量提供了新的工具。此数据集对Singing MOS（Singing MOS）的前期版本进行了扩展，不仅提供总体评分，还包含歌词、旋律和整体质量的额外标注部分，从而增加了覆盖范围和多样性。<br/><br/>2. **数据集内容丰富**：包含7,981个由41个模型生成的歌唱片段，这些模型分布在从早期系统到最新进展的不同数据集中。每个片段至少有5位专业注释者的评分，以确保评估的一致性和可靠性。<br/><br/>3. **标准一致性**：探讨了如何有效利用在不同标准下标注的MOS（主观满意度评分）数据，并对SingMOS-Pro进行了广泛使用的评估方法基准测试，为未来研究建立了坚实的基线和实用参考。<br/><br/>4. **可访问性**：SingMOS-Pro数据集可以在线获取，通过链接https://huggingface.co/datasets/TangRain/SingMOS-Pro提供给研究人员。 |
| [Probing the Hidden Talent of ASR Foundation Models for L2 English Oral Assessment](https://arxiv.org/abs/2510.16387) | 贡献点如下：<br/><br/>1. **探索Whisper模型在第二语言口语评估（SLA）领域的潜力**：论文研究了已广泛使用的自动语音识别（ASR）基础模型Whisper，将其应用于L2口语评估任务。这一应用不同于先前的研究通过外在分析由Whisper生成的转录结果，而是深入探究其潜在能力。<br/><br/>2. **从隐藏表示中提取声学和语言特征**：研究团队采用了一种方法来挖掘Whisper的潜在能力，通过从其中间输出和最终输出中抽取声学和语言特性。仅需在Whisper的基础上进行轻量级分类器训练即可达到性能优势。<br/><br/>3. **超越现有先进基准**：该方法在通用英语水平测试（GEPT）图片描述数据集上表现优异，并超过现有的尖端基础，包括多模态方法。<br/><br/>4. **整合图像和文本提示信息作为辅助相关线索**：通过将额外的辅助相关信息（如图像与文本提示）融入模型中，进一步提升了评估性能。<br/><br/>5. **深入分析Whisper的嵌入表示**：研究揭示了即使未经具体任务微调，Whisper模型内部也能够编码口语的能力等级模式和语义特征。这表明其潜在能力作为SLA和其他口语理解任务的强大基础。 |
| [RRPO: Robust Reward Policy Optimization for LLM-based Emotional TTS](https://arxiv.org/abs/2512.04552) | 贡献点如下：<br/><br/>1. **提出Robust Reward Policy Optimization（RRPO）框架**：为了应对不同iable强化学习（RL）框架如DiffRO在情感控制等微妙任务中的奖励作弊问题，引入了RRPO框架。该框架通过混合正则化方案来提高鲁棒性。<br/><br/>2. **混合正则化方案**：该方案开发出一种鲁棒的奖励模型（RM），其奖励信号与人类感知更加可靠地对齐。这促使策略模型放弃有害的捷径，转而学习真正情感的复杂特征。<br/><br/>3. **增强的跨语言泛化能力**：通过针对不同语言的一系列实验，验证了我们的RM在跨语言场景下的鲁棒性和普遍性表现更强。<br/><br/>4. **主观评估结果**：实验证明，这种鲁棒的奖励模型有效地减少了奖励作弊现象，不仅在情感表达上取得了显著提升，在自然度方面也优于所有基线方法。<br/><br/>5. **展示页面**：提供了一个用于演示该框架效果的网页链接（https://lrwinr.github.io/RRPO-CosyVoice），供用户直接体验和验证其改进后的性能。 |
| [Mathematical Foundations of Polyphonic Music Generation via Structural Inductive Bias](https://arxiv.org/abs/2601.03612) | ### 贡献点：<br/><br/>1. **解决“缺失的中间”问题**：论文提出了一种针对多声部音乐生成的新方法，通过结构诱导偏置来处理此问题。这为音乐创作提供了一个具有创新性的途径。<br/><br/>2. **贝多芬钢琴奏鸣曲案例研究**：以作曲家贝多芬的钢琴奏鸣曲作为样本进行实证分析，验证了音高和手部属性之间的独立性。<br/><br/>3. **引入Smart Embedding架构**：提出了一个名为Smart Embedding的新型架构，通过这一方法在参数数量上实现了48.30%的减少，从而提高了模型的效率。<br/><br/>4. **数学证明与理论支撑**：<br/>   - 使用信息论进行严谨的数学证明，证明了模型中存在微小的信息损失（不超过0.153比特）。<br/>   - 利用Rademacher复杂性提供了一种更紧的泛化误差估计（比现有方法紧28.09%），进一步强化了模型的泛化能力。<br/><br/>5. **稳定性与一般性的提升**：通过数学理论的证明和实证研究，论文展示了改进的模型在稳定性、预测准确性和泛化能力上的优势。<br/><br/>6. **实验结果的验证**：通过SVD分析和专家听觉测试（N=53），论文确认了模型在验证集上9.47%的损失减少是显著且可靠的结果。<br/><br/>7. **理论与应用框架的结合**：构建了一个集理论探索与实际应用为一体的双层框架，旨在弥补人工智能音乐生成领域内的知识和实践缺口。该框架为数学基础深厚的深度学习提供了可验证的见解。<br/><br/>### 总结：<br/>论文通过创新方法解决“缺失中间”的问题，在贝多芬钢琴奏鸣曲上实证其独立性，并引入Smart Embedding架构显著减少参数数量，提升了模型的理论支撑（如信息损失、泛化能力）和实际应用效果（如减少验证集损失、通过专家测试）。该研究不仅为AI音乐生成领域提供了技术上的突破，还推动了数学与深度学习的交叉融合，具有重要的学术和实用价值。 |
| [Sound2Hap: Learning Audio-to-Vibrotactile Haptic Generation from Human Ratings](https://arxiv.org/abs/2601.12245) | ### 贡献点:<br/><br/>1. **用户感知研究**：通过邀请34名参与者对四种现有音频至触觉算法生成的振动进行评估，发现没有明显的算法偏好。这项研究为后续的数据驱动模型开发提供了基础。<br/><br/>2. **数据驱动模型创建**：利用收集到的数据集训练了一个基于卷积神经网络（CNN）的自动编码器——Sound2Hap。该模型能够从多种声音中生成感知上具有意义的振动，并且具有低延迟特性。<br/><br/>3. **性能评估与比较**：在后续的研究中，15名参与者对Sound2Hap的输出进行了评估，结果显示其在音频至触觉匹配度和Haptic体验指数（HXI）方面均优于基于信号处理的基线方法。特别之处在于，Sound2Hap能够更好地与多种声音和谐共存。<br/><br/>4. **感知验证的方法**：这项工作表明了一种以感知为基础的音频至触觉转换方式的有效性，这为音源驱动的触感设计开辟了新的可能性和范围。<br/><br/>5. **拓展声驱动触感的应用**：研究结果展示了一个基于感知的音频到触觉翻译方法，这扩大了声音驱动触感在用户应用中的使用范畴。 |
