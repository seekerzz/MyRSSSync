# 五里墩茶社
---
| Title | Date | Summary |
| --- | --- | --- |
# 小工蚁创始人
---
| Title | Date | Summary |
| --- | --- | --- |
# AIGCLINK
---
| Title | Date | Summary |
| --- | --- | --- |
# 技术爬爬虾
---
| Title | Date | Summary |
| --- | --- | --- |
# 秋芝2046
---
| Title | Date | Summary |
| --- | --- | --- |
# eess.AS updates on arXiv.org
---
| Title | Summary |
| --- | --- |
| [Quality-Controlled Multimodal Emotion Recognition in Conversations with Identity-Based Transfer Learning and MAMBA Fusion](https://arxiv.org/abs/2511.14969) | 贡献点如下：<br/><br/>1. **数据质量控制管道**：提出了一套针对MELD和IEMOCAP数据集的多模态情绪识别（MERC）的数据质量控制流程，包括验证讲者身份、音频与文本对齐以及面部检测。<br/><br/>2. **转移学习应用**：利用了从讲者识别和面部识别中获取的身份差异性嵌入，并假设这些嵌入不仅捕获了稳定的听觉和面部特征，还包含了个体特有的情绪表达模式。通过RecoMadeEasy(R)引擎提取512维的讲者和面部嵌入。<br/><br/>3. **多阶段转移学习**：使用MPNet-v2对情绪感知文本表示进行微调，并将这些特征通过情绪特定的MLP（多层感知机）训练，针对单模态数据集进行了调整。这一过程在控制质量的数据子集上实现了一致的竞争性能。<br/><br/>4. **多模态融合方法**：基于MAMBA模型进行跨模态融合，在MELD数据集上达到了64.8%的准确率，在IEMOCAP数据集上达到了74.3%的准确率。这些结果表明，将基于身份的声音和视觉嵌入与情绪调整的文字表示相结合，并应用于高质量的数据子集中，可以为多模态情绪识别在对话中提供一致的竞争性能。<br/><br/>5. **对低频复杂情感类别的改善基础**：研究成果提供了改善挑战性的、频率较低的情绪类别的一般性基础，暗示着进一步的改进空间。 |
| [CASTELLA: Long Audio Dataset with Captions and Temporal Boundaries](https://arxiv.org/abs/2511.15131) | 贡献点:<br/><br/>1. **提出了一项全新的音频基准(CASTELLA)**:作者引入了CASTELLA，这是一个用于音频瞬间检索任务的人工注释数据集。这项工作填补了当前领域缺乏实用世界真实数据标准的空白。<br/><br/>2. **大规模人工标注的数据集**:与之前的研究不同，该数据集规模大（包含109、213和640个训练、验证和测试片段），是现有数据集规模的24倍。这为模型评估提供了更可靠的依据，并使得应用在现实世界环境中时性能更为可靠。<br/><br/>3. **建立基准模型**:论文中，作者使用CASTELLA建立了AMR任务的基本模型。通过对比实验结果，他们表明，在预训练于合成数据后，对CASTELLA进行微调的模型比仅在合成数据上进行训练的模型在召回率（Recall1@0.7）上的表现提高了10.4点。<br/><br/>4. **公开发布和可用性**:CASTELLA数据集及相关的实验代码已经开源，并可以通过以下链接访问：https://h-munakata.github.io/CASTELLA-demo。这促进了社区对数据集的使用、研究和改进，对于进一步推动音频瞬间检索任务的研究具有重要意义。<br/><br/>综上所述，这项工作主要贡献在于提供了一个实用、大规模的注释音频基准和相关的模型测试案例，以及为该领域提供了开放的数据集访问途径，加速了音频瞬间检索技术的发展。 |
| [Auden-Voice: General-Purpose Voice Encoder for Speech and Language Understanding](https://arxiv.org/abs/2511.15145) | 贡献点:<br/><br/>1. **研究目标明确** - 研究旨在构建一种能捕捉微妙语音线索的通用音频编码器，以平衡身份和语伴信息的编码。<br/><br/>2. **方法比较分析** - 通过全面评估发现，多任务训练能提供最平衡的表现，而对比语言-音频预训练（CLAP）主要提升了检索能力，但在增强语伴理解方面效果不明显。<br/><br/>3. **新型语音编码器开发** - 最终推出了名为Auden-Voice的编码器，在与大型语言模型（LLMs）集成时表现出强大性能。<br/><br/>4. **开放资源贡献** - 提供了用于音频理解工具包Auden中的代码和训练食谱，促进了社区的学习和进一步研究。 |
| [OBHS: An Optimized Block Huffman Scheme for Real-Time Audio Compression](https://arxiv.org/abs/2511.14793) | ### 贡献点：<br/><br/>1. **提出新型音频压缩算法**：OBHS（优化块霍夫曼方案）是一种专为实时流媒体应用设计的无损音频压缩算法，旨在实现高压缩比的同时保持低计算复杂度。<br/><br/>2. **采用分块霍夫曼编码及元码表示**：通过将音频数据分割成固定大小的区块，并为每个区块构建最优霍夫曼树，利用元码进行高效存储和传输，OBHS实现了优化的数据压缩策略。<br/><br/>3. **高效实时性能**：算法的时间复杂度为O(n)，意味着对于n个音频样本，其处理时间与样本数量线性相关，适用于资源受限的实时音频流媒体场景。<br/><br/>4. **广泛适用性**：实验结果显示，在各种类型的音频（包括粉噪声、音调和实际录音）中，OBHS均能保持良好的性能，并在沉默丰富的音频文件中实现了高达93.6%的压缩比，说明了其广泛的适应性和高效率。 |
| [Fine-tuning Pre-trained Audio Models for COVID-19 Detection: A Technical Report](https://arxiv.org/abs/2511.14939) | 贡献点如下：<br/><br/>1. **探索预训练音频模型在COVID-19检测任务中的性能**：研究聚焦于使用标准基准数据集评估COVID-19检测的预训练音频模型，具体考察了音频-MAE和三个PANN架构（CNN6、CNN10、CNN14）在Coswara和COUGHVID数据集上的性能。<br/><br/>2. **年龄与性别严格分层**：为了防止模型利用与COVID-19状态相关的统计数据之间的伪相关性，研究实施了年龄和性别的严格分层。这有助于评估模型检测COVID-19的真实能力，消除潜在的混淆因素。<br/><br/>3. **结果分析**：<br/>   - 内部数据集结果显示中等性能，其中音频-MAE在Coswara数据集上表现最佳（AUC：0.82，F1分数：0.76），但在COUGHVID上的所有模型都显示出有限的性能（AUC：0.58-0.63）。<br/>   - 跨数据集评估揭示了所有模型在一般化方面存在严重问题（AUC：0.43-0.68），音频-MAE的F1分数大幅下降至0.00-0.08。<br/><br/>4. **平衡对模型性能的影响**：研究表明，虽然平衡处理降低了模型表现，但它提供了更现实的COVID-19检测能力评估，通过消除统计数据间的混淆因素。这有助于识别模型在不同人群中的偏差和局限性。<br/><br/>5. **数据集大小与模型需求**：平衡后的数据集大小（1219至2160个样本）证明过小，不足以支持通常需要大量训练集的深度学习模型。这一发现强调了开发泛化性强的音频基COVID-19检测系统的根本挑战，并突出了对严格人口统计控制进行临床稳健模型评估的重要性。<br/><br/>这些贡献点揭示了在COVID-19检测领域使用预训练音频模型时面临的关键问题和改进方向，特别是在数据集选择、模型性能评估和潜在混淆因素识别方面。 |
| [Aligning Generative Music AI with Human Preferences: Methods and Challenges](https://arxiv.org/abs/2511.15038) | 贡献点:<br/><br/>1. **偏好对齐技术在音乐生成中的系统应用**：论文强调了使用偏好的对齐技术来改善音乐生成的质量，解决当前计算优化与人类音乐欣赏之间的根本差距。这表明，传统的损失函数往往无法充分考虑到复杂的用户偏好。<br/><br/>2. **结合近期突破性成果**：文章综合讨论了如MusicRL的大规模偏好学习、基于扩散的DiffRhythm+中的多偏好对齐框架以及Text2midi-InferAlign等在音乐生成中应用的最新技术。这些方法分别针对时间一致性、和声一致性和主观质量评估的独特挑战。<br/><br/>3. **音乐生成的关键研究挑战**：论文识别了几个关键的研究挑战，如长篇幅作品的可扩展性问题，以及其他偏好模型的可靠性等，这些问题对于推进音乐生成技术至关重要。<br/><br/>4. **未来愿景与应用展望**：作者对未来提出了乐观的展望，期望偏好对齐的音乐生成能够为互动作曲工具和个性化音乐服务带来革命性的变化。这表明了一个整合机器学习、音乐理论来创造真正服务于人类创意和体验需求的音乐AI系统的前景。<br/><br/>5. **跨学科合作的重要性**：论文呼吁在多学科研究领域中持续合作，汇集机器学习领域的最新进展与音乐理论的知识，以期构建出能够满足人类对音乐创作和体验个性化需求的先进系统。 |
| [Scene-wide Acoustic Parameter Estimation](https://arxiv.org/abs/2410.23523) | ### 贡献点:<br/><br/>1. **场景音频特性估计方法的创新**：论文提出了一种通过利用AR/VR环境中的轻量级信息，从场景几何直接推断出整个场景的空间分布性声学参数（如C50、T60等）的方法。这一方法简化了从场景中直接估算Room-impulse Responses (RIRs)的复杂性和数据需求。<br/><br/>2. **图像到图像翻译任务**：作者使用了一个图像是图像的转换任务，通过条件化在AR/VR上下文中的2D地面地图，将之转化为表示声学参数的二维热图。这为利用现有场景信息进行音频特性的估计提供了一种高效且直观的方式。<br/><br/>3. **方向依赖性参数预测**：论文展示了所提出的方法不仅适用于基于几何信息的参数预测，还能够适应方向依赖的情况（即被束成束后的参数预测），拓宽了方法的应用范围和实用性。<br/><br/>4. **数据集贡献与实证验证**：为研究该任务，作者创建并公开了一个包含1000个房间、复杂场景的数据集。通过使用这一数据集来评估和比较方法的性能，论文提供了有力的实证支持，并展示了相较于统计基线的改进情况。<br/><br/>### 总结：<br/><br/>这篇论文贡献了一种用于AR/VR应用中的创新方法，能够从轻量级信息中推断出整个场景的空间分布性声学参数。通过引入一个专门的数据集和详细的评估策略，该研究不仅推动了这一领域的技术发展，还为后续的研究提供了基准和资源。 |
| [Bridging the Modality Gap: Softly Discretizing Audio Representation for LLM-based Automatic Speech Recognition](https://arxiv.org/abs/2506.05706) | 贡献点:<br/><br/>1. 解决了将连续的音频数据与离散令牌化的大语言模型（LLMs）集成时存在的问题。通过引入向量量化(VQ)方法，研究提出了一种在基于LLM的自动语音识别（ASR）中整合VQ的方法。<br/><br/>2. 利用LLM嵌入表作为VQ码本，VQ模块将音频编码器产生的连续表示与离散LLM输入对齐，使LLM能够操作更符合语言结构的离散化音频表示。<br/><br/>3. 创建了音频表示的“软离散化”方法，通过更新码本并进行代码本嵌入的加权求和来进行。这种方法在保留音频数据连续性的同时，实现了对其的离散处理。<br/><br/>4. 提出了的集成VQ到LLM ASR中的方法，在跨域条件下显著优于基于LLM的ASR基准线。这表明了软离散化在LLM ASR中的潜在应用价值作为模态桥梁。<br/><br/>5. 研究结果强调了软离散化在通过LLM进行语音识别中作为一种跨模态连接方式的可能性和潜力。 |
| [Efficient and Generalizable Speaker Diarization via Structured Pruning of Self-Supervised Models](https://arxiv.org/abs/2506.18623) | 贡献点如下：<br/><br/>1. **提出了一种基于知识蒸馏的结构化剪枝方法**：这项工作系统地研究了通过知识蒸馏指导的结构化剪枝，用于压缩基于自监督学习（SSL）模型的说话者分段。<br/><br/>2. **研究了结合模型参数和计算复杂度的多种裁剪目标**：探讨了旨在同时优化模型参数和减少计算成本的不同剪枝策略，并分析了不同的方法。结果显示，整体简单化的剪枝方式在效率与准确性之间提供了最佳平衡。<br/><br/>3. **实现了高达80%的模型大小压缩**：通过此方法，在不牺牲性能的情况下，获得了4倍更快的推理速度。<br/><br/>4. **跨八种公共说话者分段数据集进行了全面实验**：实验显示，修剪后的模型在准确度方面与未压缩模型相当甚至更好，证明了方法的有效性。<br/><br/>5. **展示了强大的域外泛化能力**：在CHiME-6数据集上进行的实验结果表明，在CHiME-7挑战中达到顶级系统性能而无需任何领域适应的情况下，修剪后的模型能够展现出良好的跨域泛化能力。<br/><br/>6. **强调了结构化剪枝和知识蒸馏相结合的重要性**：该研究揭示了通过知识蒸馏指导的结构化剪枝可以产生高效且具有泛化性的说话者分段系统，适用于实际应用场景。 |
| [UniAV: Unified Audio-Visual Perception for Multi-Task Video Event Localization](https://arxiv.org/abs/2404.03179) | 贡献点如下：<br/><br/>1. **统一框架的提出**：引入了一种一体化的方法，用于解决包括时间动作定位（TAL）、声音事件检测（SED）和音频视觉事件定位（ABEL）在内的视频事件定位任务。旨在实现对视频内容全面理解的支持。<br/><br/>2. **挑战与问题**：现有方法往往过于专门化于个别任务，忽视了不同事件在完整理解视频内容时的等价重要性。这使得将单一任务策略直接应用于多任务环境存在不足之处。<br/><br/>3. **UniAV框架的创新**：提出了“UniAV”（统一音频视觉感知网络），一个能够跨任务和模态有效学习并共享互惠知识的统一框架。通过设计一种联合音频-视频编码器，从多个时间尺度为所有任务的视频提取通用表示。<br/><br/>4. **专家与特定知识融合**：在每个任务中引入了专门的任务专家来捕获与其特定相关的唯一知识，同时保持对不同任务实例的有效识别能力。<br/><br/>5. **多任务预测头替代方案**：通过使用语义对齐的任务提示开发了一个新颖的统一语言感知分类器，该方法无需单独的预测头部，从而能够灵活地在各个任务之间定位各种实例，并具有出色的处理新类别开放集的能力。<br/><br/>6. **实验验证**：全面的实验证明了“UniAV”的优越性能，与单一任务模型和原始多任务基准相比，在所有三个任务上均显著超越。尤其是在ActivityNet 1.3、DESED和UnAV-100评估指标上，其性能优于专门针对各个任务的最佳方法。 |
| [MelodySim: Measuring Melody-aware Music Similarity for Plagiarism Detection](https://arxiv.org/abs/2505.20979) | ### 贡献点:<br/><br/>1. **数据集构建与改进**:<br/>   - 作者提出了一种新的方法来创建专注于旋律相似性的数据集。他们通过修改现有的MIDI数据集Slakh2100，如通过分割音符、变奏、小调轨道降级和重新配器等手段，生成每首作品的变异版本，同时保留了核心旋律。<br/>   - 通过用户研究验证了正样本确实包含相似的旋律，并且与之相比的其他音乐片段显著地发生了变化。这表明数据集能够准确地区分相似性和非相似性。<br/><br/>2. **模型开发**:<br/>   - 开发了一种基于段落级别的旋律相似度检测模型，该模型利用MERT（Melody Encoder with Relative Transform）编码器，并应用了三元组神经网络来捕获旋律的相似性。<br/>   - 这一设计使得能够产生一个决策矩阵，用于高亮显示可能存在的抄袭片段的位置。<br/><br/>3. **性能评估与对比**:<br/>   - 实验结果显示，该模型在检测MelodySim测试集中的相似旋律片段时，能够超越基线模型。这表明所开发的模型具有较好的识别能力，特别是在旋律模仿和抄袭检测方面表现出了优势。<br/>   <br/>总之，此研究贡献了旋律感知音乐相似度的数据集构建方法、基于MERT编码器和三元组网络的模型以及在旋律抄袭检测任务上的应用实例，为音乐领域中的版权保护提供了新的技术手段。 |
| [Retrieval Augmented Generation based context discovery for ASR](https://arxiv.org/abs/2509.19567) | ### 贡献点:<br/><br/>1. **研究方向**: 探索检索增强生成(retrieval augmented generation)作为提升情境感知自动语音识别(ASR)系统中上下文发现效率的策略。特别是在处理罕见或不在词汇表中的术语时，以提高转录准确性。<br/><br/>2. **自动上下文发现**: 针对自动上下文识别中存在的挑战，提出了一个基于嵌入式的检索方法来有效地在ASR系统中发现上下文。<br/><br/>3. **性能对比分析**: 对比了两种基于大型语言模型(LLMs)的替代方案以评估其有效性和可行性。具体包括：<br/><br/>   - 通过提示生成具有上下文的大型语言模型(LLM)-基于的方法。<br/>   <br/>   - 基于LLM的转录后文本修正方法。<br/><br/>4. **实验验证**: 在TED-LIUMv3、Earnings21和SPGISpeech数据集上进行了实证研究，结果表明所提出的方法相较于无上下文的情况，能够将错误率(WER)降低最多17%，而理想情况下(oracle context)，WER的减少达到最大24.1%。<br/><br/>这些贡献强调了通过检索增强生成策略改善ASR系统中的上下文发现能力的有效性，并且提供了在实际应用中提高准确性的方法。 |
| [Step-Audio-EditX Technical Report](https://arxiv.org/abs/2511.03601) | 贡献点:<br/>1. **首个人开源LLM（大型语言模型）驱动的音频编辑模型**："Step-Audio-EditX" 是第一款基于大型语言模型的开源音频模型，能够实现表达力强、迭代式的音频编辑。它不仅涵盖了情感、说话风格等语言特性，还拥有强大的零样本文本到语音（TTS）能力。<br/><br/>2. **独特的学习方法**：该模型的核心创新在于仅依赖于大间隔合成数据进行学习，这一步骤跳过了需要嵌入式先验或辅助模块的步骤。通过大间隔学习方法，"Step-Audio-EditX" 既实现了迭代控制也保证了音色的高表达力。<br/><br/>3. **彻底改变传统的关注点**：这种模型代表了一个根本性的转变，从常规的关注于表示级的分解转向关注于在声音编辑和情感处理上的多维度控制能力。<br/><br/>4. **卓越的表现**："Step-Audio-EditX" 在情感编辑等精细粒度的控制任务上超越了 "MiniMax-2.6-hd" 和 "Doubao-Seed-TTS-2.0"，证明其在复杂音频处理任务中的领先地位和性能优势。 |
